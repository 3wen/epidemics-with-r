[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modelling epidemics with R",
    "section": "",
    "text": "As Michel likes to emphasize, our offices are located in a triangle within the Aix Marseille School of Economics (AMSE) department of Aix Marseille Université, so we couldn’t be closer. However, it was not until the lockdown brought about by the COVID-19 pandemic that we actually started working together. The COVID-19 pandemic, despite all the disasters it has brought upon a lot of people, has had at least one positive externality for us –it has prompted remote discussions. We have spent a lot of time exploring the use of entirely new methods for each of us. Through this ebook, we aim to share the knowledge we have gained during this period. There may be errors in the codes provided, so if you spot any, please don’t hesitate to point them out to us."
  },
  {
    "objectID": "index.html#outline",
    "href": "index.html#outline",
    "title": "Modelling epidemics with R",
    "section": "Outline",
    "text": "Outline\nThe first part of this ebook relates to the SIR model. In Chapter 1, the SIR model is presented. In Chapter 2, R codes to simulate an epidemics with the SIR model, with and without lockdown are provided. Chapter 3 shows the dynamics of the epidemics with animated graphs. The second part of this ebook is devoted to statistical models. Chapter 4 presents the Covid-19 data from Oxford University. Chapter 5 shows how to estimate the reproduction number. Chapter 6 presents the phenomenological models and Chapter 7 shows how to use R to estimate those phenomenological models."
  },
  {
    "objectID": "SIR-background.html#reproduction-numbers",
    "href": "SIR-background.html#reproduction-numbers",
    "title": "1  Theoretical background",
    "section": "1.1 Reproduction Numbers",
    "text": "1.1 Reproduction Numbers\nSince \\[\\frac{\\text{d}S}{\\text{d}t} + \\frac{\\text{d}I}{\\text{d}t} + \\frac{\\text{d}R}{\\text{d}t} = 0,\\] and that by integration we find \\(S+I+R = N\\), \\(N\\) can be seen as an arbitrary integration constant. Consequently, \\(S\\), \\(I\\), and \\(R\\) are usually considered to be proportions that add up to 1 with: \\[\nS+I+R=1\n\\tag{1.2}\\]\nleading to a simpler presentation of the model: \\[\n\\begin{aligned}\n\\frac{\\text{d}S}{\\text{d}t} &= -\\beta I \\times S\\\\\n\\frac{\\text{d}I}{\\text{d}t} &= -\\beta I \\times S - \\gamma I\\\\\n\\frac{\\text{d}R}{\\text{d}t} &= \\gamma I\n\\end{aligned}\n\\tag{1.3}\\]\nThe basic reproduction number \\(\\mathcal{R}_0\\), i.e., the average number that an infected person manages to contaminate during the period of contagion is given by \\(T_r / T_c = \\beta / \\gamma\\). This number is fixed at the beginning of the epidemic and is its main characteristics. For COVID-19, the first values taken in the model of Imperial College were between 2 and 2.6, later updated to an interval between 2.4 and 3.3 for the UK. In European countries, values as high as between 3 to 4.7 were found as reported in Adam (2020).\nBecause the epidemic evolves over time and finally stops, it is necessary to introduce a complementary notion, the effective reproduction number defined as: \\[\n\\mathcal{R}_t^e = \\frac{\\beta}{\\gamma} \\times S_t = \\mathcal{R}_0 \\times S_t.\n\\tag{1.4}\\]\nThis effective reproduction number decreases with the number of susceptibles \\(S_t\\).\n\nIf \\(\\beta > \\gamma\\) so that \\(\\mathcal{R}_0 > 1\\), then the epidemic grows exponentially.\nIf \\(\\beta < \\gamma\\) so that \\(\\mathcal{R}_0 < 1\\), then the epidemic dies out exponentially.\n\nThe major goal of a health policy is to obtain a \\(\\mathcal{R}_0\\) lower than 1.0, using a lockdown policy that will lead to a decrease in the value of \\(\\beta\\).\nThe model assumes that when a person has been infected, they recover (or die), but can never be re-infected. Because of the conservation identity Equation 1.2, the number of susceptible decreases while the number of recovered increases. But if in the long run \\(I\\) tends to 0, the number of susceptible does not decreases to zero, because of herd immunity. Herd immunity is reached when a sufficient proportion of individuals have been infected and have become immune to the virus. This proportion of immune people depends on the contagiousness of the disease and is equal to: \\[R^\\star = 1- 1/\\mathcal{R}_0.\\]\nTo this proportion corresponds the equilibrium proportion of infected people: \\[S^\\star = 1/\\mathcal{R}_0.\\] This proportion is reached at the peak of the epidemic and is usually lower than the limiting value \\(S_{\\infty}\\) when \\(t \\rightarrow \\infty\\). So the model is overshooting by a non-negligible percentage as will be detailed below. With a plausible value of \\(\\mathcal{R}_0 = 2.5\\) for the COVID-19, the herd immunity threshold is \\(S^\\star = 0.4\\), meaning that herd immunity is reached when 60% of the population has recovered or is protected by a vaccine.\nThe probability of dying is a constant proportion \\(\\pi\\) of the infected, completing thus the model by a fourth equation: \\[\n\\frac{\\text{d}D}{\\text{d}t} = \\pi \\gamma I,\n\\tag{1.5}\\]\nwhich simply means that the proportion of deaths is a fraction of \\(R\\) with \\(D = \\pi R\\). This variable has no action on the dynamics of the model, but its prediction is of course of prime importance. As a matter of fact, most of the controversies reported in the literature (see, for instance Adam (2020)) concern the predicted number of deaths. The number of deaths at the end of the epidemic is computed as: \\[\nD = (1-S_{\\infty}) \\pi \\times N \\times S_0\n\\tag{1.6}\\]"
  },
  {
    "objectID": "SIR-background.html#phase-diagram",
    "href": "SIR-background.html#phase-diagram",
    "title": "1  Theoretical background",
    "section": "1.2 Phase diagram",
    "text": "1.2 Phase diagram\nThe dynamics of the model is best described using phase diagrams as advocated in Moll (2020). Phase diagrams plot \\(S\\) against \\(I\\), assuming \\(S + I < 1\\). After some algebraic manipulations, we can find the number of Infected as a function of the number of Susceptible, the \\(\\mathcal{R}_0\\) and the initial conditions. We get: \\[\nI_t = 1-R_0 - S_t + \\frac{1}{\\mathcal{R}_0} log(S_t  / S_0),\n\\tag{1.7}\\]\nwhich is convenient for analysing some properties of the model. Typical initial conditions are: \\[\n\\begin{aligned}\nS_0 &= 1 - I_0,\\\\\nI_0 &\\approx 0,\\\\\nR_0 &= 0.\\\\\n\\end{aligned}\n\\]\nwhere \\(I_0\\) can be set for instance to \\(1/N\\). With these elements in mind, a phase diagram can be drawn. For given initial conditions and a given grid of \\(S_t\\) , the corresponding proportion of infected persons is obtained."
  },
  {
    "objectID": "SIR-background.html#introducing-a-lockdown",
    "href": "SIR-background.html#introducing-a-lockdown",
    "title": "1  Theoretical background",
    "section": "1.3 Introducing a Lockdown",
    "text": "1.3 Introducing a Lockdown\nA lock-down is introduced in the SIR model by considering a time variable \\(\\beta_t\\) . If \\(\\ell_t\\) is the strength of the lock-down and \\(\\beta_0\\) the value of \\(\\beta\\) in the absence of lock-down, then: \\(\\beta_t = \\beta_0 \\times (1-\\ell_t)\\),\nso that a lock-down is a very efficient way of decreasing the value of \\(\\beta_t\\) . It implies that: \\[\\mathcal{R}_t = (1-\\ell_t)\\mathcal{R}_0 S_t,\\] which means that with a very strict lock-down the epidemic ceases to spread out. But that does not mean that the epidemic will cease, once the lock-down is removed.\nWith a very strict lock-down the epidemic ceases to expand at an exponential rate. But that does not mean that the epidemic will stop immediately. However, a lock-down is applied over a limited period, so we have to be able to provide a graph where time is the horizontal axis. So we have to find a numerical way to find the trajectory of the model in its three variables, and a simple phase diagram is no longer sufficient. For given values of the parameters, the trajectory of a SIR model can be found by discretizing the system with \\(\\Delta_t < 1\\) and use the Euler’s method to solve the system:\n\\[\n\\begin{aligned}\nS_i & = S_{i-1} - \\beta_0 (1-\\ell_i) S_{i-1} I_{i-1} \\Delta_t,\\\\\nI_i & = I_{i-1} + (\\beta_0 (1-\\ell_i) S_{i-1} I_{i-1} - \\gamma I_{i-1}) \\Delta_t,\\\\\nR_i & = I_{i-1} + \\gamma I_{i-1} \\Delta_t.\n\\end{aligned}\n\\tag{1.8}\\]\nWhen iterating this system, \\(1/\\Delta_t\\) iterations are needed to cover one period when the parameters are calibrated on a daily basis.\n\n\n\n\nAdam, David. 2020. “Special Report: The Simulations Driving the World’s Response to COVID-19.” Nature 580: 316–18.\n\n\nKermack, William Ogilvy, and A. G. McKendrick. 1927. “A Contribution to the Mathematical Theory of Epidemics.” Proceedings of the Royal Society A 115 (772): 700–721.\n\n\nMoll, Benjamin. 2020. “Lockdowns in SIR Models.” LSE.\n\n\nPark, M., A. R. Cook, J. T. Lim, Y. Sun, and B. L. Dickens. 2020. “A Systematic Review of COVID-19 Epidemiology Based on Current Evidence.” Journal of Clinical Medicine 9 (4). https://doi.org/10.3390/jcm9040967.\n\n\nToda, Alexis Akira. 2020. “Susceptible-Infected-Recovered (SIR) Dynamics of COVID-19 and Economic Impact.” arXiv:2003.11221v2.\n\n\nWang, Huwen, Zezhou Wang, Yinqiao Dong, Ruijie Chang, Chen Xu, Xiaoyue Yu, Shuxian Zhang, et al. 2020. “Phase-Adjusted Estimation of the Number of Coronavirus Disease 2019 Cases in Wuhan, China.” Cell Discovery 6 (1): 1–8. https://doi.org/10.1038/s41421-020-0148-0."
  },
  {
    "objectID": "SIR-simulations.html#without-lockdown",
    "href": "SIR-simulations.html#without-lockdown",
    "title": "2  Simulations with R",
    "section": "2.1 Without lockdown",
    "text": "2.1 Without lockdown\nWe can simulate a SIR without lockdown first. Let us adapt the The Matlab codes codes provided by Benjamin Moll.\nWe will need some functions to manipulate data from {tidyverse}.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n\nLet us set the reproduction number \\(\\mathcal{R}_0=2.5\\):\n\nreprod_number <- 2.5\nTinf <- 7\nbeta <- reprod_number / Tinf\ngamma <- 1 / Tinf\n\nThe choice of the number of periods can be made here:\n\nimplicit <- 0\nT <- 400    # Length of simulation\ndt <- 0.1   # Time step \nNt <- T / dt + 1\ntime <- seq(0, T, length.out = Nt)\n\nA matrix with 3 rows can be initiated. Each row corresponds to \\(S\\), \\(I\\), and \\(R\\), resp.\n\nmu <- matrix(rep(0, 3 * (Nt + 1)), nrow = 3)\nS <- I <- R <- N <- F <- Re <- rep(0, Nt)\n\nThe transition matrices at each time step (we initialize an empty list):\n\nA_t <- vector(mode = \"list\", length = Nt)\n\nSetting the initial conditions for \\(S\\), \\(I\\) et \\(R\\):\n\n# March 1st 2020 so that March 31st there are appx. 150,000 cases \n# (50% more than measured)\nI0 <- 0.35 * 10^(-4) \nS0 <- 1 - I0\nR0 <- 0\n\nLet the first column of mu contain the initial conditions for \\(S_0\\), \\(I_0\\), and \\(R_0\\)\n\nmu[,1] <- matrix(c(S0, I0, R0), nrow = 3)\n\nNow, a loop over the different periods can be made. Benjamin Moll uses the matrix form of the SIR model here.\n\nfor (n in 1:(Nt - 1)) {\n  S[n] <- mu[1, n]\n  I[n] <- mu[2, n]\n  R[n] <- mu[3, n]\n  Re[n] <- reprod_number * S[n]\n  A_t[[n]] <- matrix(\n    c(\n      -beta * I[n], beta * I[n], 0,\n      0, -gamma, gamma,\n      0, 0, 0\n    ),\n    byrow = TRUE,\n    ncol = 3)\n  \n  if (implicit == 0) {\n    mu[, n+1] <- dt * (t(A_t[[n]]) %*% mu[, n]) + mu[, n]\n  } else {\n    mu[n + 1, ] <- (I(3) - dt * (t(A_t[[n]])) %/% mu[, n])\n  }\n}\n\nmu <- mu[, 1:Nt]\n\nRecall that the rows of matrix mu contain the values for \\(S\\), \\(I\\), and \\(R\\), respectively. The columns correspond to the values at all the periods.\n\nS <- mu[1, ]\nI <- mu[2, ]\nR <- mu[3, ]\n\nN <- S + I + R\nD <- 100 - N\n\ndf_lockdown <- tibble(S = S, I = I, R = R, t = time, type = \"no-lockdown\")\ndf_lockdown\n\n# A tibble: 4,001 × 5\n       S         I          R     t type       \n   <dbl>     <dbl>      <dbl> <dbl> <chr>      \n 1  1.00 0.000035  0            0   no-lockdown\n 2  1.00 0.0000357 0.0000005    0.1 no-lockdown\n 3  1.00 0.0000365 0.00000101   0.2 no-lockdown\n 4  1.00 0.0000373 0.00000153   0.3 no-lockdown\n 5  1.00 0.0000381 0.00000207   0.4 no-lockdown\n 6  1.00 0.0000389 0.00000261   0.5 no-lockdown\n 7  1.00 0.0000397 0.00000317   0.6 no-lockdown\n 8  1.00 0.0000406 0.00000373   0.7 no-lockdown\n 9  1.00 0.0000415 0.00000431   0.8 no-lockdown\n10  1.00 0.0000424 0.00000491   0.9 no-lockdown\n# ℹ 3,991 more rows"
  },
  {
    "objectID": "SIR-simulations.html#with-lockdown",
    "href": "SIR-simulations.html#with-lockdown",
    "title": "2  Simulations with R",
    "section": "2.2 With lockdown",
    "text": "2.2 With lockdown\nNow, let us create a function that will run the SIR model with a lockdown. The code from the SIR model without lockdown is slightly modified, to introduce the lockdown and its severity. The following function allows us to easily make a simulation depending on the start and end of the lockdown, as well as its severity:\n\n#' Simulate the SIR model with a lockdown\n#' \n#' @param lock_start start of the lockdown (number of days from the outbreak)\n#' @param lock_end end of the lockdown (number of days from the outbreak)\n#' @param lock_severity severity of the lockdown (in [0,1], 0 for no lockdown)\nsimulate_sir_lockdown <- function(lock_start,\n                                  lock_end,\n                                  lock_severity,\n                                  type) {\n  # Matrix with 3 rows, each corresponding to S, I and R, resp.\n  mu <- matrix(rep(0, 3 * (Nt + 1)), nrow = 3)\n  S <- I <- R <- N <- F <- Re <- rep(0, Nt)\n  \n  # Transition matrix\n  A_t <- vector(mode=\"list\", length = Nt)\n  \n  # First column: initial conditions\n  mu[, 1] <- matrix(c(S0, I0, R0), nrow = 3)\n  \n  lockdown <- rep(0, Nt)\n  \n  for (n in 1:(Nt - 1)) {\n    S[n] <- mu[1, n]\n    I[n] <- mu[2, n]\n    R[n] <- mu[3, n]\n    \n    if (time[n] >= lock_start & time[n] <= lock_end) {\n      # Lockout\n      lockdown[n] <- lock_severity\n    }\n    \n    Re[n] <- reprod_number * (1 - lockdown[n]) * S[n]\n    \n    A_t[[n]] <- matrix(\n      c(\n        -beta * (1-lockdown[n]) * I[n], beta * (1 - lockdown[n]) * I[n], 0,\n        0, -gamma, gamma,\n        0, 0, 0\n      ),\n      byrow = TRUE,\n      ncol = 3)\n    \n    if (implicit == 0) {\n      mu[, n+1] <- dt * (t(A_t[[n]]) %*% mu[, n]) + mu[, n]\n    } else {\n      mu[n + 1, ] <- (I(3) - dt * (t(A_t[[n]])) %/% mu[, n])\n    }\n  }\n  \n  mu <- mu[, 1:Nt]\n  \n  S_tight <- mu[1, ]\n  I_tight <- mu[2, ]\n  R_tight <- mu[3, ]\n  \n  N_tight <- S_tight + I_tight + R_tight\n  D_tight <- 100 - N_tight\n  \n  tibble(S = S_tight, I = I_tight, R = R_tight, t = time)\n}\n\nNow let us set the values for a tight lockdown:\n\nlock_start_tight <- 37\nlock_end_tight <- lock_start_tight+30\nlock_severity_tight <- .7\n\nAn let us use those values in the modified SIR model:\n\ndf_lockdown_tight <- \n  simulate_sir_lockdown(lock_start = lock_start_tight,\n                        lock_end = lock_end_tight,\n                        lock_severity = .7) |> \n  mutate(type = \"tight-lockdown\")\n\n\nTight lockdownLoose LockdownModerate Lockdown\n\n\nNow let us set the values for a tight lockdown:\n\nlock_start_tight <- 37\nlock_end_tight <- lock_start_tight + 30\nlock_severity_tight <- .7\n\nAn let us use those values in the modified SIR model:\n\ndf_lockdown_tight <- simulate_sir_lockdown(\n  lock_start = lock_start_tight,\n  lock_end = lock_end_tight,\n  lock_severity = .7) |> \n  mutate(type = \"tight-lockdown\")\n\n\n\nThe following values for the lockdown lead to simulating a loose lockdown:\n\nlock_start_loose <- 37\nlock_end_loose <- lock_start_loose + 30\nlock_severity_loose <- .4\n\nThese values can be used to make a new simulation of the SIR model:\n\ndf_lockdown_loose <- simulate_sir_lockdown(\n  lock_start = lock_start_loose,\n  lock_end = lock_end_loose,\n  lock_severity = lock_severity_loose) |> \n  mutate(type = \"loose-lockdown\")\n\n\n\nA moderate lockdown can then be considered, picking the following values:\n\nlock_start_mix <- 37\nlock_end_mix <- lock_start_mix + 90\nlock_severity_mix <- .425\n\nAnd these values can be given to the simulation function:\n\ndf_lockdown_mix <- simulate_sir_lockdown(\n  lock_start = lock_start_mix,\n  lock_end = lock_end_mix,\n  lock_severity = lock_severity_mix) |> \n  mutate(type = \"mix-lockdown\")"
  },
  {
    "objectID": "SIR-simulations.html#graphs-of-the-evolution",
    "href": "SIR-simulations.html#graphs-of-the-evolution",
    "title": "2  Simulations with R",
    "section": "2.3 Graphs of the evolution",
    "text": "2.3 Graphs of the evolution\nThe datasets for each scenario need to be reshaped to be used in ggplot2().\n\n#' Reshape the data from two scenarios\n#' \n#' @param df_scenario_1 data for first scenario\n#' @param df_scenario_2 data for second scenario\n#' @param name_scenario_1 name of the scenario in `df_scenario_1`\n#' @param label_scenario_1 desired label for the name of the first scenario\n#' @param name_scenario_2 name of the scenario in `df_scenario_2`\n#' @param label_scenario_2 desired label for the name of the second scenario\nreshape_data_graph <- function(df_scenario_1,\n                               df_scenario_2,\n                               name_scenario_1,\n                               label_scenario_1,\n                               name_scenario_2,\n                               label_scenario_2) {\n  df_scenario_1 |> \n  bind_rows(df_scenario_2) |> \n  pivot_longer(cols = c(\"S\", \"I\", \"R\")) |> \n  filter(t <= Nt - 1) |> \n  mutate(name = factor(name, levels = c(\"S\", \"I\", \"R\")),\n         type = factor(\n           type, \n           levels = c(name_scenario_1, name_scenario_2), \n           labels = c(label_scenario_1, label_scenario_2)))\n}\n\nWe define a theme function to make the graphs pretty (at least for us!).\n\n\nCode\nlibrary(grid)\ntheme_paper <- function(..., size_text = 8)\n  theme(text = element_text(size = size_text),\n        plot.background = element_rect(fill=\"transparent\", color=NA),\n        panel.background = element_rect(fill = \"transparent\", color=NA),\n        panel.border = element_blank(),\n        axis.text = element_text(), \n        legend.text = element_text(size = rel(1.1)),\n        legend.title = element_text(size = rel(1.1)),\n        legend.background = element_rect(fill=\"transparent\", color=NULL),\n        legend.position = \"bottom\", \n        legend.direction = \"horizontal\", legend.box = \"vertical\",\n        legend.key = element_blank(),\n        panel.spacing = unit(1, \"lines\"),\n        panel.grid.major = element_line(colour = \"grey90\"), \n        panel.grid.minor = element_blank(),\n        plot.title = element_text(hjust = 0, size = rel(1.3), face = \"bold\"),\n        plot.title.position = \"plot\",\n        plot.margin = unit(c(1, 1, 1, 1), \"lines\"),\n        strip.background = element_rect(fill=NA, colour = NA),\n        strip.text = element_text(size = rel(1.1)))\n\n\nAnd we define a function to plot the evolution of the two scenarios with respect to time.\n\n#' Plots the evolution of two scenarios\n#' \n#' @param df_plot data with the two scenarios (obtained from \n#' `reshape_data_graph()`)\n#' @param lock_start start period of the lockdown\n#' @param lock_end end period of the lockdown\n#' @param lock_severity severity of the lockdown\nplot_scenario <- function(df_plot,\n                          lock_start,\n                          lock_end,\n                          lock_severity) {\n  ggplot(data = df_plot) +\n  geom_rect(\n    data = tibble(x1 = lock_start, \n                          x2 = lock_end,\n                          y1 = 0, y2 = lock_severity),\n    mapping = aes(xmin = x1, xmax = x2, ymin = y1, ymax = y2),\n    fill = \"grey\", alpha = .8\n  ) +\n  geom_line(aes(x = t, y = value, linetype = type, colour = name)) +\n  scale_linetype_discrete(NULL) +\n  scale_colour_manual(\n    NULL, \n    values = c(\"S\" = \"#1b9e77\", \"I\" = \"#d95f02\", \"R\" = \"#7570b3\")) +\n  labs(x = \"Days from the outbreak\", y = \"Proportion of cases\") +\n  theme_paper() +\n  coord_cartesian(xlim = c(0, 150))\n}\n\n\nTight lockdownLoose lockdownModerate lockdown\n\n\n\ndf_plot_lockdown_tight <- reshape_data_graph(\n  df_scenario_1 = df_lockdown,\n  df_scenario_2 = df_lockdown_tight,\n  name_scenario_1 = \"no-lockdown\",\n  label_scenario_1 = \"Laissez-faire\",\n  name_scenario_2 = \"tight-lockdown\",\n  label_scenario_2 = \"Tight lockdown\")\nplot_scenario(\n  df_plot = df_plot_lockdown_tight,\n  lock_start = lock_start_tight,\n  lock_end = lock_end_tight,\n  lock_severity = lock_severity_tight)\n\n\n\n\n\n\n\ndf_plot_lockdown_loose <- reshape_data_graph(\n  df_scenario_1 = df_lockdown,\n  df_scenario_2 = df_lockdown_loose,\n  name_scenario_1 = \"no-lockdown\",\n  label_scenario_1 = \"Laissez-faire\",\n  name_scenario_2 = \"loose-lockdown\",\n  label_scenario_2 = \"Loose lockdown\")\nplot_scenario(\n  df_plot = df_plot_lockdown_loose,\n  lock_start = lock_start_loose,\n  lock_end = lock_end_loose,\n  lock_severity = lock_severity_loose)\n\n\n\n\n\n\n\ndf_plot_lockdown_mix <- reshape_data_graph(\n  df_scenario_1 = df_lockdown,\n  df_scenario_2 = df_lockdown_mix,\n  name_scenario_1 = \"no-lockdown\",\n  label_scenario_1 = \"Laissez-faire\",\n  name_scenario_2 = \"mix-lockdown\",\n  label_scenario_2 = \"Long-tight lockdown\")\nplot_scenario(\n  df_plot = df_plot_lockdown_mix,\n  lock_start = lock_start_mix,\n  lock_end = lock_end_mix,\n  lock_severity = lock_severity_mix)"
  },
  {
    "objectID": "SIR-simulations.html#phase-diagrams",
    "href": "SIR-simulations.html#phase-diagrams",
    "title": "2  Simulations with R",
    "section": "2.4 Phase diagrams",
    "text": "2.4 Phase diagrams\nThe phase diagrams can also be plotted. Once again, the data need to be reshaped for use in ggplot2.\n\n#' @param df_scenario_1 data for first scenario\n#' @param df_scenario_2 data for second scenario\n#' @param name_scenario_1 name of the scenario in `df_scenario_1`\n#' @param label_scenario_1 desired label for the name of the first scenario\n#' @param name_scenario_2 name of the scenario in `df_scenario_2`\n#' @param label_scenario_2 desired label for the name of the second scenario\nreshape_data_phase <- function(df_scenario_1,\n                               df_scenario_2,\n                               name_scenario_1,\n                               label_scenario_1,\n                               name_scenario_2,\n                               label_scenario_2) {\n  df_scenario_1 |>\n    filter(I > 0.001) |> \n    group_by(S) |> \n    slice(1) |> \n    ungroup() |> \n    bind_rows(\n      df_scenario_2 |> \n        filter(I > 0.001) |> \n        group_by(S) |> \n        slice(1) |> \n        ungroup()\n    ) |> \n    mutate(\n      type = factor(\n        type, \n        levels = c(name_scenario_1, name_scenario_2), \n        labels = c(label_scenario_1, label_scenario_2))\n    )\n}\n\nIf we want to display some arrows to highlight the direction on the diagrams, we can create a function that will do so for a single scenario.\n\n#' Gives the coordinates to draw a small arrow on the phase diagram of one \n#' scenario\n#' \n#' @param df_plot table with the data of the diagram of one scenario ready for \n#' use in ggplot2\n#' @param val_S value for S\ncreate_df_arrow <- function(df_plot,\n                            val_S) {\n  df_arrows <- \n    df_plot |> \n    arrange(desc(S)) |> \n    group_by(type) |> \n    filter(S < !!val_S) |> \n    slice(1:2) |> \n    select(S, I, type) |>\n    ungroup() |> \n    mutate(toto = rep(c(\"beg\", \"end\"), 2))\n  \n  df_arrows |> \n    select(-I) |> \n    pivot_wider(names_from = toto, values_from = S, names_prefix = \"S_\") |> \n    left_join(\n      df_arrows |> \n        select(-S) |> \n        pivot_wider(names_from = toto, values_from = I, names_prefix = \"I_\"),\n      by =  \"type\"\n    )\n}\n\nThen, let us create a function that will plot the phase diagrams for two scenarios.\n\nplot_phase_diagram_scenarios <- function(df_plot,\n                                         x_arrows,\n                                         reprod_number = 2.5\n                                         ) {\n  # Ending points of the epidemic\n  df_dots <- df_plot |> \n    group_by(type) |> \n    filter(I == min(I))\n  \n  # Arrow\n  df_arrows <- \n    map_df(x_arrows, ~create_df_arrow(df_plot, val_S = .))\n  \n  # The coordinates of $S$ corresponding to herd immunity\n  S_herd <- 1./reprod_number\n  curve_scenario <- tibble(\n    x1 = S_herd + .1, \n    x2 = S_herd, \n    y1 = .30, \n    y2 = .27)\n  label_curve_scenario <- \n    tibble(\n      x = S_herd + .15,\n      y = .3,\n      label = \"Herd Immunity\"\n    )\n\n  # The graph\n  ggplot() +\n    geom_line(\n      data = df_plot,\n      mapping = aes(x = S, y = I, linetype = type)\n    ) +\n    geom_vline(xintercept = S_herd, linetype = \"dashed\") +\n    geom_curve(\n      data = df_arrows,\n      mapping = aes(x = S_beg, xend = S_end,\n                    y = I_beg, yend = I_end),\n      arrow = arrow(length = unit(0.04, \"npc\")),\n      curvature = 0\n    ) +\n    geom_curve(\n      data = curve_scenario, \n      mapping = aes(x = x1, y = y1, xend = x2, yend = y2),\n      arrow = arrow(length = unit(0.03, \"npc\")), curvature = -0.2)  +\n    geom_label(\n      data = label_curve_scenario,\n      mapping = aes(x = x, y = y, label = label),\n      size = rel(3)\n    ) +\n    geom_point(\n      data = df_dots, \n      mapping = aes(x = S, y = I), \n      colour = \"red\", size = 3\n      ) +\n    scale_linetype_discrete(NULL) +\n    scale_x_continuous(breaks = seq(0, 1, by = .1)) +\n    labs(x = \"Susceptible\", y = \"Infected\") +\n    theme_paper()\n    \n}\n\n\nTight lockdownLoose lockdownModerate lockdown\n\n\n\ndf_plot_tight <- \n  reshape_data_phase(\n  df_scenario_1    = df_lockdown,\n  df_scenario_2    = df_lockdown_tight,\n  name_scenario_1  = \"no-lockdown\",\n  label_scenario_1 = \"Laissez-faire\",\n  name_scenario_2  = \"tight-lockdown\",\n  label_scenario_2 = \"Tight lockdown\")\n\nplot_phase_diagram_scenarios(\n  df_plot = df_plot_tight, \n  x_arrows = c(.7, .5, .3, .2), \n  reprod_number = 2.5) +\n  coord_cartesian(ylim = c(0, .35))\n\n\n\n\nFigure 2.1: Tight Lockdown: Phase Diagram\n\n\n\n\n\n\n\ndf_plot_loose <- \n  reshape_data_phase(\n  df_scenario_1    = df_lockdown,\n  df_scenario_2    = df_lockdown_loose,\n  name_scenario_1  = \"no-lockdown\",\n  label_scenario_1 = \"Laissez-faire\",\n  name_scenario_2  = \"loose-lockdown\",\n  label_scenario_2 = \"Loose lockdown\")\n\nplot_phase_diagram_scenarios(\n  df_plot = df_plot_loose, \n  x_arrows = c(.7, .55, .3), \n  reprod_number = 2.5\n) +\n  coord_cartesian(ylim = c(0, .35))\n\n\n\n\nFigure 2.2: Loose Lockdown: Phase Diagram\n\n\n\n\n\n\n\ndf_plot_mix <- \n  reshape_data_phase(\n  df_scenario_1    = df_lockdown,\n  df_scenario_2    = df_lockdown_mix,\n  name_scenario_1  = \"no-lockdown\",\n  label_scenario_1 = \"Laissez-faire\",\n  name_scenario_2  = \"mix-lockdown\",\n  label_scenario_2 = \"Long-tight lockdown\")\n\nplot_phase_diagram_scenarios(\n  df_plot = df_plot_mix, \n  x_arrows = c(.7, .5), \n  reprod_number = 2.5) +\n  coord_cartesian(ylim = c(0, .35))\n\n\n\n\nFigure 2.3: Long-tight Lockdown: Phase Diagram"
  },
  {
    "objectID": "SIR-animations.html",
    "href": "SIR-animations.html",
    "title": "3  Animations with R",
    "section": "",
    "text": "In this chapter, we use R to make an animated version of the simulation of the epidemics using the SIR model.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(raster)\n\nLoading required package: sp\n\nAttaching package: 'raster'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nWe will consider that individuals live on a square with sides of 5.\n\nbounds <- c(-5,5)\n\nEach time unit, individuals can take a step. The size of the step is defined with the following variable:\n\nstep_size <- abs(max(bounds)-min(bounds))/30\n\nLet us set some key parameters of the epidemics:\n\nThe probability that a \\(S\\) gets infected while in contact with an \\(I\\) is set here to .8\nTo be infected, we consider a radius of .5 around an \\(I\\).\nAt the beginning of the simulation, the number of infected is set to .02.\nOnce an individual has been infected, it takes 7 periods before it turns to \\(R\\).\n\n\nproba_infect <- 0.8\nradius_infec <- 0.5\nI_0 <- 0.02\ntime_recovery <- 7\n\nLet us consider \\(N=150\\) individuals, and a time span of 100 periods.\n\nn <- 150\nnsteps <- 100\n\nThe position of the individuals will be stored in the following object:\n\npositions <- vector(mode = \"list\", length = nsteps)\n\nThe initial positions are randomly drawn from a Uniform distribution \\(\\mathcal{U}(-5,5)\\).\n\nset.seed(123)\nx <- runif(n = n, min = min(bounds), max = max(bounds))\ny <- runif(n = n, min = min(bounds), max = max(bounds))\n\nAt each moment, let us store the status of the infected in an object called status. Here are the random status at the beginning. On average, if we replicate the analysis multiple times, the proportion of infected at the beginning of the epidemic should be equal to \\(0.02\\).\n\nstatus <- sample(\n  c(\"S\", \"I\"),\n  replace = TRUE, \n  size = round(n), \n  prob = c(1-I_0, I_0)\n)\ntable(status)\n\nstatus\n  I   S \n  6 144 \n\nprop.table(table(status))\n\nstatus\n   I    S \n0.04 0.96 \n\n\nThe number of infected at the beginning:\n\nnb_infected_start <- sum(status == \"I\")\nnb_infected_start\n\n[1] 6\n\n\nLet us store the current state of our simulated world in a tibble:\n\ndf_current <- tibble(x = x, y = y, step = 1, status = status)\n\nAt each time, the state of the world will be stored in the ith element of positions.\n\npositions[[1]] <- df_current\n\nWe need to identify the individuals with respect to their status:\n\nid_I <- which(df_current$status == \"I\")\nid_S <- which(df_current$status == \"S\")\nid_R <- NULL\n\nThe time since infection is set to 0 for all individuals…\n\ntime_since_infection <- rep(0, n)\n\n… except for those that are infected at the first period.For those, the time to infection is set to 1:\n\ntime_since_infection[id_I] <- 1\n\nThen we can begin the loop over the periods.\n\nfor(i in 2:nsteps) {\n  # Movements, in both directions\n  deplacement_x <- runif(n = n, min = -step_size, max = step_size)\n  deplacement_y <- runif(n = n, min = -step_size, max = step_size)\n  \n  x <- x+deplacement_x\n  y <- y+deplacement_y\n  \n  x[x>max(x)] <- max(bounds)\n  x[x<min(x)] <- min(bounds)\n  \n  y[y>max(y)] <- max(bounds)\n  y[y<min(y)] <- min(bounds)\n  \n  # Recovery\n  id_new_recovered <- which(time_since_infection>time_recovery)\n  if (length(id_new_recovered) > 0) {\n    id_R <- c(id_R, id_new_recovered)\n    id_I <- id_I[!id_I %in% id_new_recovered]\n    status[id_new_recovered] <- \"R\"\n    time_since_infection[id_new_recovered] <- 0\n  }\n  \n  # New infections\n  dist_matrix <- pointDistance(\n    cbind(x[id_I], y[id_I]),\n    cbind(x[id_S], y[id_S]),\n    lonlat=FALSE\n  )\n  \n  # Identifying close individuals\n  close_points <- which(dist_matrix < radius_infec, arr.ind = TRUE)\n  \n  if (length(close_points) > 0) {\n    ids_potential_new_infected <- id_S[close_points[,\"col\"]]\n    are_infected <- sample(\n      c(TRUE, FALSE), \n      size = nrow(close_points), \n      prob = c(proba_infect, 1 - proba_infect), \n      replace = TRUE\n    )\n    ids_new_infected <- ids_potential_new_infected[are_infected]\n    \n    time_since_infection[time_since_infection != 0] <- \n      time_since_infection[time_since_infection != 0] + 1\n    \n    time_since_infection[ids_new_infected] <- 1\n    \n    id_I <- sort(c(id_I, ids_new_infected))\n    status[ids_new_infected] <- \"I\"\n    id_S <- id_S[!id_S %in% ids_new_infected]\n  }\n  \n  # Storing the current state of the world\n  positions[[i]] <- tibble(x = x, y = y, step = i, status = status) |> \n    mutate(status = factor(status, levels = c(\"S\", \"I\", \"R\")))\n  \n  # Plotting the situation\n  p <- \n    ggplot(\n      data = positions[[i]], \n      mapping = (aes(x = x, y = y, colour = status))) +\n    ggforce::geom_circle(\n      data = positions[[i]] |> \n        filter(status == \"I\") |> \n        mutate(r = radius_infec),\n      mapping = aes(\n        x0 = x, y0 = y, r = r\n      ),\n      fill = \"#d95f02\",\n      colour = NA,\n      alpha = .1,\n      inherit.aes = FALSE\n    ) +\n    geom_point(size = 1) +\n    labs(x = \"\", y = NULL, \n         title = str_c(\"No. Infected: \", length(id_I))) +\n    coord_equal(xlim = bounds, ylim = bounds) +\n    scale_colour_manual(\"Status\", values = c(\n      \"S\" = \"#1b9e77\",\n      \"I\" = \"#d95f02\",\n      \"R\" = \"#7570b3\"\n    ),\n    guide = \"none\") +\n    theme(\n      plot.title.position = \"plot\", \n      panel.border = element_rect(linetype = \"solid\", fill=NA),\n      panel.grid = element_blank(),\n      axis.ticks = element_blank(),\n      axis.text = element_blank()\n    )\n  \n  if (length(close_points) > 0) {\n    if (length(ids_new_infected) > 0) {\n      p <- p + \n        geom_point(\n          data = positions[[i]] |> \n            slice(ids_new_infected),\n          mapping = aes(x = x, y = y),\n          size = 2, colour = \"black\"\n        ) +\n        geom_point(\n          data = positions[[i]] |> \n            slice(ids_new_infected),\n          mapping = aes(x = x, y = y),\n          size = 1.5\n        )\n    }\n    \n  }\n  \n  df_plot <- \n    map_df(\n      positions[1:i],\n      ~group_by(., status) |> count(),\n      .id = \"time\"\n    ) |> \n    ungroup() |> \n    bind_rows(\n      tibble(\n        time = \"0\",\n        status = c(\"S\", \"I\", \"R\"), \n        n = c(n - nb_infected_start, nb_infected_start, 0)\n      )\n    ) |> \n    complete(status, time) |> \n    mutate(\n      n = replace_na(n, 0),\n      time = as.numeric(time),\n      status = factor(status, levels = c(\"S\", \"I\", \"R\"))\n    )\n  \n  p_counts <- \n    ggplot(\n      data = df_plot,\n      mapping = aes(x = time, y = n, colour = status)) +\n    geom_line() +\n    scale_colour_manual(\n      \"Status\", values = c(\n        \"S\" = \"#1b9e77\",\n        \"I\" = \"#d95f02\",\n        \"R\" = \"#7570b3\"\n      )\n    ) +\n    labs(x = \"Time\", y = NULL, title = \"Counts\") +\n    theme(plot.title.position = \"plot\")\n  \n  \n  p_both <- cowplot::plot_grid(p, p_counts)\n  \n  file_name <- str_c(\n    \"figs/simul_\",\n    str_pad(i, width = 3, side = \"left\", pad = 0),\n    \".png\"\n  )\n  cowplot::save_plot(\n    filename = file_name,\n    p_both, ncol = 2,\n    base_asp = 1.1,\n    base_width = 5,\n    base_height = 5,\n    dpi=72\n  )\n}\n\nThe png files can be merged in a gif file with a system command:\n\nsystem(\"convert figs/*.png figs/animation-sir.gif\")\n\nThe resulting gif can be seen in Figure 3.1.\n\nif (knitr::is_html_output()) {\n  knitr::include_graphics(\"figs/animation-sir.gif\")\n} else if (knitr::is_latex_output()) {\n  knitr::include_graphics(\"figs/simul_100.png\")\n}\n\n\n\n\nFigure 3.1: SIR simulation over 100 periods, with \\(I_0\\approx 0.02\\)"
  },
  {
    "objectID": "covid-data.html#load-data",
    "href": "covid-data.html#load-data",
    "title": "4  Covid-19 Data",
    "section": "4.1 Load data",
    "text": "4.1 Load data\nFirst of all, let us load some packages.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(knitr)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(RColorBrewer)\nlibrary(growthmodels)\nlibrary(minpack.lm)\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nlibrary(nlstools)\n\n\n'nlstools' has been loaded.\n\nIMPORTANT NOTICE: Most nonlinear regression models and data set examples\nrelated to predictive microbiolgy have been moved to the package 'nlsMicrobio'\n\nlibrary(ggpubr)\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nLet us also define a theme for the graphical outputs, as in Chapter 1\n\nlibrary(grid)\ntheme_paper <- function(..., size_text = 8)\n  theme(text = element_text(size = size_text),\n        plot.background = element_rect(fill=\"transparent\", color=NA),\n        panel.background = element_rect(fill = \"transparent\", color=NA),\n        panel.border = element_blank(),\n        axis.text = element_text(), \n        legend.text = element_text(size = rel(1.1)),\n        legend.title = element_text(size = rel(1.1)),\n        legend.background = element_rect(fill=\"transparent\", color=NULL),\n        legend.position = \"bottom\", \n        legend.direction = \"horizontal\", legend.box = \"vertical\",\n        legend.key = element_blank(),\n        panel.spacing = unit(1, \"lines\"),\n        panel.grid.major = element_line(colour = \"grey90\"), \n        panel.grid.minor = element_blank(),\n        plot.title = element_text(hjust = 0, size = rel(1.3), face = \"bold\"),\n        plot.title.position = \"plot\",\n        plot.margin = unit(c(1, 1, 1, 1), \"lines\"),\n        strip.background = element_rect(fill=NA, colour = NA),\n        strip.text = element_text(size = rel(1.1)))\n\nLet us modify the locale settings. This depends on the OS. For Windows users:\n\nSys.setlocale(\"LC_ALL\", \"English_United States\")\n\nFor Unix users:\n\n# Only for Unix users\nSys.setlocale(\"LC_ALL\", \"en_US.UTF-8\")\n\n[1] \"en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\"\n\n\n\n4.1.1 Confirmed and Deaths data\nAs mentioned at the beginning of the notebook, we rely on Oxford Covid-19 Government Response Tracker (OxCGRT) data.\nLet us define the vector of country names:\n\nnames_countries <- c(\"United Kingdom\", \"Spain\", \"Italy\", \"Germany\", \"France\", \n                     \"Sweden\", \"Belgium\", \"Netherlands\", \"Ireland\", \"Denmark\")\nnames_countries_large <- c(\"United Kingdom\", \"Spain\", \"Italy\", \"Germany\", \"France\")\nnames_countries_small <- c(\"Sweden\", \"Belgium\", \"Netherlands\", \"Ireland\", \"Denmark\")\n\nThe raw data can be downloaded as follows:\n\ndf_oxford <- \n  read.csv(\"https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_nat_latest.csv\")\n\nThen we save those:\n\ndir.create(\"data\")\nsave(df_oxford, file = \"data/df_oxford.rda\")\n\nLet us load the saved data (data saved on May 29, 2023):\n\n\n\nThe dimensions are the following:\n\ndim(df_oxford)\n\n[1] 202819     61\n\n\nLet us focus first on the number of confirmed cases:\n\nconfirmed_df <- \n  df_oxford |> \n  select(\n    country = CountryName, \n    country_code = CountryCode,\n    date = Date,\n    value = ConfirmedCases,\n    stringency_index = StringencyIndex_Average) |> \n  filter(country %in% names_countries) |> \n  as_tibble() |> \n  mutate(\n    date = ymd(date),\n    days_since_2020_01_22 =\n      lubridate::interval(\n        lubridate::ymd(\"2020-01-22\"), date) / lubridate::ddays(1)\n  )\n\nWe can do the same for the number of deaths:\n\ndeaths_df <- \n  df_oxford |> \n  select(country = CountryName, \n         country_code = CountryCode,\n         date = Date,\n         value = ConfirmedDeaths, \n         stringency_index = StringencyIndex_Average) |> \n  filter(country %in% names_countries) |> \n  as_tibble() |> \n  mutate(\n    date = ymd(date),\n    days_since_2020_01_22 =\n      lubridate::interval(\n        lubridate::ymd(\"2020-01-22\"), date) / lubridate::ddays(1)\n  )\n\n\n4.1.1.1 Confirmed extract\nHere is an extract from the last 3 rows of the confirmed cases:\n\nconfirmed_df |> \n  group_by(country) |> \n  slice_tail(n = 3) |> \n  arrange(country, desc(date)) |> \n  kable()\n\n\n\nTable 4.1:  Last 3 rows of confirmed cases, by country \n \n  \n    country \n    country_code \n    date \n    value \n    stringency_index \n    days_since_2020_01_22 \n  \n \n\n  \n    Belgium \n    BEL \n    2022-12-31 \n    4668248 \n    11.11 \n    1074 \n  \n  \n    Belgium \n    BEL \n    2022-12-30 \n    4668248 \n    11.11 \n    1073 \n  \n  \n    Belgium \n    BEL \n    2022-12-29 \n    4668248 \n    11.11 \n    1072 \n  \n  \n    Denmark \n    DNK \n    2022-12-31 \n    3385836 \n    11.11 \n    1074 \n  \n  \n    Denmark \n    DNK \n    2022-12-30 \n    3385836 \n    11.11 \n    1073 \n  \n  \n    Denmark \n    DNK \n    2022-12-29 \n    3385836 \n    11.11 \n    1072 \n  \n  \n    France \n    FRA \n    2022-12-31 \n    38266999 \n    11.11 \n    1074 \n  \n  \n    France \n    FRA \n    2022-12-30 \n    38266999 \n    11.11 \n    1073 \n  \n  \n    France \n    FRA \n    2022-12-29 \n    38243191 \n    11.11 \n    1072 \n  \n  \n    Germany \n    DEU \n    2022-12-31 \n    37369866 \n    11.11 \n    1074 \n  \n  \n    Germany \n    DEU \n    2022-12-30 \n    37369865 \n    11.11 \n    1073 \n  \n  \n    Germany \n    DEU \n    2022-12-29 \n    37345969 \n    11.11 \n    1072 \n  \n  \n    Ireland \n    IRL \n    2022-12-31 \n    1687668 \n    5.56 \n    1074 \n  \n  \n    Ireland \n    IRL \n    2022-12-30 \n    1687668 \n    5.56 \n    1073 \n  \n  \n    Ireland \n    IRL \n    2022-12-29 \n    1687668 \n    5.56 \n    1072 \n  \n  \n    Italy \n    ITA \n    2022-12-31 \n    25143705 \n    21.99 \n    1074 \n  \n  \n    Italy \n    ITA \n    2022-12-30 \n    25143705 \n    21.99 \n    1073 \n  \n  \n    Italy \n    ITA \n    2022-12-29 \n    25021606 \n    21.99 \n    1072 \n  \n  \n    Netherlands \n    NLD \n    2022-12-31 \n    8569095 \n    11.11 \n    1074 \n  \n  \n    Netherlands \n    NLD \n    2022-12-30 \n    8569095 \n    11.11 \n    1073 \n  \n  \n    Netherlands \n    NLD \n    2022-12-29 \n    8565716 \n    11.11 \n    1072 \n  \n  \n    Spain \n    ESP \n    2022-12-31 \n    13684258 \n    5.56 \n    1074 \n  \n  \n    Spain \n    ESP \n    2022-12-30 \n    13684258 \n    5.56 \n    1073 \n  \n  \n    Spain \n    ESP \n    2022-12-29 \n    13670037 \n    5.56 \n    1072 \n  \n  \n    Sweden \n    SWE \n    2022-12-31 \n    2674862 \n    11.11 \n    1074 \n  \n  \n    Sweden \n    SWE \n    2022-12-30 \n    2674862 \n    11.11 \n    1073 \n  \n  \n    Sweden \n    SWE \n    2022-12-29 \n    2674862 \n    11.11 \n    1072 \n  \n  \n    United Kingdom \n    GBR \n    2022-12-31 \n    24135080 \n    5.56 \n    1074 \n  \n  \n    United Kingdom \n    GBR \n    2022-12-30 \n    24135080 \n    5.56 \n    1073 \n  \n  \n    United Kingdom \n    GBR \n    2022-12-29 \n    24135080 \n    5.56 \n    1072 \n  \n\n\n\n\n\n\nThe number of observation per country:\n\nconfirmed_df |> group_by(country) |> count()\n\n# A tibble: 10 × 2\n# Groups:   country [10]\n   country            n\n   <chr>          <int>\n 1 Belgium         1096\n 2 Denmark         1096\n 3 France          1096\n 4 Germany         1096\n 5 Ireland         1096\n 6 Italy           1096\n 7 Netherlands     1096\n 8 Spain           1096\n 9 Sweden          1096\n10 United Kingdom  1096\n\n\n\n\n4.1.1.2 Deaths extract\nHere is an extract from the last 3 rows of the number of deaths, for each country:\n\ndeaths_df |> \n  group_by(country) |> \n  slice_tail(n = 3) |> \n  arrange(country, desc(date)) |> \n  kable()\n\n\n\nTable 4.2:  Last 3 rows of deaths, by country \n \n  \n    country \n    country_code \n    date \n    value \n    stringency_index \n    days_since_2020_01_22 \n  \n \n\n  \n    Belgium \n    BEL \n    2022-12-31 \n    33228 \n    11.11 \n    1074 \n  \n  \n    Belgium \n    BEL \n    2022-12-30 \n    33228 \n    11.11 \n    1073 \n  \n  \n    Belgium \n    BEL \n    2022-12-29 \n    33228 \n    11.11 \n    1072 \n  \n  \n    Denmark \n    DNK \n    2022-12-31 \n    7758 \n    11.11 \n    1074 \n  \n  \n    Denmark \n    DNK \n    2022-12-30 \n    7758 \n    11.11 \n    1073 \n  \n  \n    Denmark \n    DNK \n    2022-12-29 \n    7758 \n    11.11 \n    1072 \n  \n  \n    France \n    FRA \n    2022-12-31 \n    158385 \n    11.11 \n    1074 \n  \n  \n    France \n    FRA \n    2022-12-30 \n    158385 \n    11.11 \n    1073 \n  \n  \n    France \n    FRA \n    2022-12-29 \n    158270 \n    11.11 \n    1072 \n  \n  \n    Germany \n    DEU \n    2022-12-31 \n    161465 \n    11.11 \n    1074 \n  \n  \n    Germany \n    DEU \n    2022-12-30 \n    161465 \n    11.11 \n    1073 \n  \n  \n    Germany \n    DEU \n    2022-12-29 \n    161321 \n    11.11 \n    1072 \n  \n  \n    Ireland \n    IRL \n    2022-12-31 \n    8293 \n    5.56 \n    1074 \n  \n  \n    Ireland \n    IRL \n    2022-12-30 \n    8293 \n    5.56 \n    1073 \n  \n  \n    Ireland \n    IRL \n    2022-12-29 \n    8293 \n    5.56 \n    1072 \n  \n  \n    Italy \n    ITA \n    2022-12-31 \n    184642 \n    21.99 \n    1074 \n  \n  \n    Italy \n    ITA \n    2022-12-30 \n    184642 \n    21.99 \n    1073 \n  \n  \n    Italy \n    ITA \n    2022-12-29 \n    183936 \n    21.99 \n    1072 \n  \n  \n    Netherlands \n    NLD \n    2022-12-31 \n    22990 \n    11.11 \n    1074 \n  \n  \n    Netherlands \n    NLD \n    2022-12-30 \n    22990 \n    11.11 \n    1073 \n  \n  \n    Netherlands \n    NLD \n    2022-12-29 \n    22971 \n    11.11 \n    1072 \n  \n  \n    Spain \n    ESP \n    2022-12-31 \n    117095 \n    5.56 \n    1074 \n  \n  \n    Spain \n    ESP \n    2022-12-30 \n    117095 \n    5.56 \n    1073 \n  \n  \n    Spain \n    ESP \n    2022-12-29 \n    116899 \n    5.56 \n    1072 \n  \n  \n    Sweden \n    SWE \n    2022-12-31 \n    21827 \n    11.11 \n    1074 \n  \n  \n    Sweden \n    SWE \n    2022-12-30 \n    21827 \n    11.11 \n    1073 \n  \n  \n    Sweden \n    SWE \n    2022-12-29 \n    21827 \n    11.11 \n    1072 \n  \n  \n    United Kingdom \n    GBR \n    2022-12-31 \n    216306 \n    5.56 \n    1074 \n  \n  \n    United Kingdom \n    GBR \n    2022-12-30 \n    216155 \n    5.56 \n    1073 \n  \n  \n    United Kingdom \n    GBR \n    2022-12-29 \n    216005 \n    5.56 \n    1072 \n  \n\n\n\n\n\n\nThe number of observation per country:\n\ndeaths_df |> group_by(country) |> count()\n\n# A tibble: 10 × 2\n# Groups:   country [10]\n   country            n\n   <chr>          <int>\n 1 Belgium         1096\n 2 Denmark         1096\n 3 France          1096\n 4 Germany         1096\n 5 Ireland         1096\n 6 Italy           1096\n 7 Netherlands     1096\n 8 Spain           1096\n 9 Sweden          1096\n10 United Kingdom  1096\n\n\nLet us keep in a table the correspondence between the country name and its ISO 3166-1 alpha-3 code:\n\ncountry_codes <-\n  confirmed_df |>\n  select(country, country_code) |>\n  unique()\ncountry_codes\n\n# A tibble: 10 × 2\n   country        country_code\n   <chr>          <chr>       \n 1 Belgium        BEL         \n 2 Germany        DEU         \n 3 Denmark        DNK         \n 4 Spain          ESP         \n 5 France         FRA         \n 6 United Kingdom GBR         \n 7 Ireland        IRL         \n 8 Italy          ITA         \n 9 Netherlands    NLD         \n10 Sweden         SWE         \n\n\n\n\n\n4.1.2 Population\nLet us define rough values for the population of each country:\n\npopulation <- \n  tribble(\n    ~country, ~pop,\n    \"France\", 70e6,\n    \"Germany\", 83e6,\n    \"Italy\", 61e6,\n    \"Spain\", 47e6,\n    \"United Kingdom\", 67e6,\n    \"Belgium\", 12e6,\n    \"Denmark\", 6e6,\n    \"Ireland\", 5e6,\n    \"Netherlands\", 18e6,\n    \"Sweden\", 11e6\n  )\n\n\n\n4.1.3 Filtering data\nIn this notebook, let us use data up to September 30th, 2020. Let us keep only those data and extend the date up to November 3rd, 2020 (to assess the goodness of fit of the models with unseen data).\n\nend_date_sample <- lubridate::ymd(\"2020-09-30\")\nend_date_data <- lubridate::ymd(\"2020-10-31\")\nconfirmed_df <- confirmed_df |> filter(date <= end_date_data)\ndeaths_df <- deaths_df |> filter(date <= end_date_data)\n\n\n\n4.1.4 Defining start and end dates for a “first wave”\nWe create a table that gives some dates for each country, adopting the following convention:\n\nstart_first_wave: start of the first wave, defined as the first date when the cumulative number of cases is greater than 1\nstart_high_stringency: date at which the stringency index reaches its maximum value during the first 100 days of the sample\nstart_reduce_restrict: moment at which the restrictions of the first wave starts to lower\nstart_date_sample_second_wave: 60 days after the relaxation of restrictions (60 days after after start_reduce_restrict)\nlength_high_stringency: number of days between start_high_stringency and start_reduce_restrict`.\n\n\n# Start of the outbreak\nstart_first_wave <- \n  confirmed_df |> \n  group_by(country) |> \n  arrange(date) |> \n  filter(value > 0) |> \n  slice(1) |> \n  select(country, start_first_wave = date)\n\nThe start of period with highest severity index among the first 100 days:\n\nstart_high_stringency <- \n  confirmed_df |> \n  group_by(country) |> \n  slice(1:100) |>\n  arrange(desc(stringency_index), date) |> \n  slice(1) |> \n  select(country, start_high_stringency = date)\n\nThe moment at which the restrictions of the first wave starts to lower:\n\nstart_reduce_restrict <- \n  confirmed_df |> \n  group_by(country) |> \n  arrange(date) |> \n  left_join(start_high_stringency, by = \"country\") |> \n  filter(date >= start_high_stringency) |> \n  mutate(tmp = dplyr::lag(stringency_index)) |> \n  mutate(same_strin = stringency_index == tmp) |> \n  mutate(same_strin = ifelse(row_number()==1, TRUE, same_strin)) |> \n  filter(same_strin == FALSE) |> \n  slice(1) |>\n  select(country, start_reduce_restrict = date)\n\nThe assumed start of the second wave:\n\nstart_date_sample_second_wave <-\n  start_reduce_restrict |> \n  mutate(\n    start_date_sample_second_wave = start_reduce_restrict + \n      lubridate::ddays(60)\n  ) |>\n  select(country, start_date_sample_second_wave)\n\nThen, we can put all these dates into a single table:\n\nstringency_dates <- \n  start_first_wave |> \n  left_join(start_high_stringency, by = \"country\") |> \n  left_join(start_reduce_restrict, by = \"country\") |> \n  left_join(start_date_sample_second_wave, by = \"country\")"
  },
  {
    "objectID": "covid-data.html#individual-data",
    "href": "covid-data.html#individual-data",
    "title": "4  Covid-19 Data",
    "section": "4.2 Individual Data",
    "text": "4.2 Individual Data\nLi et al. (2020) estimated the distribution of the incubation period by fitting a lognormal distribution on exposure histories, leading to an estimated mean of 5.2 days, a 0.95 confidence interval of [4.1,7.0] and the 95th percentile of 12.5 days.\nThis corresponds to a lognormal density with parameters \\(\\mu = 1.43\\) and \\(\\sigma = 0.67\\):\n\nfunc <- function(x){\n  # For finding gamma parameters of the serial interval distribution Li etal 2020\n  a = x[1]\n  s = x[2]\n  (7.5-a*s)^2+(3.4-sqrt(a)*s)^2\n}\nx = c(4,2)\noptim(x,func)\n\n$par\n[1] 4.866009 1.541308\n\n$value\n[1] 9.522571e-10\n\n$counts\nfunction gradient \n      67       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nThe mean serial interval is defined as the time between the onset of symptoms in a primary case and the onset of symptoms in secondary cases. Li et al. (2020) fitted a gamma distribution to data from cluster investigations. They found a mean time of 7.5 days (SD = 3.4) with a 95% confidence interval of [5.3,19].\nThe corresponding parameters for the Gamma distribution can be obtained as follows:\n\nmean_si <- 7.5\nstd_si <- 3.4\n\nshape <- (mean_si / std_si)^2\nscale <- mean_si / shape\nstr_c(\"Shape = \", shape, \", Scale = \", scale)\n\n[1] \"Shape = 4.8659169550173, Scale = 1.54133333333333\"\n\n\nThe quantile of order .999 for such parameters is equal to:\n\nh <- ceiling(qgamma(p = .99, shape = shape, scale = scale))\nh\n\n[1] 18"
  },
  {
    "objectID": "covid-data.html#descriptive-statistics",
    "href": "covid-data.html#descriptive-statistics",
    "title": "4  Covid-19 Data",
    "section": "4.3 Descriptive statistics",
    "text": "4.3 Descriptive statistics\nLet us assign a colour to each country within each group (large or small countries)\n\ncolours_lugami <- rep(\n  c(\"#1F78B4\", \"#33A02C\", \"#E31A1C\", \"#FF7F00\", \"#6A3D9A\"), 2)\ncolours_lugami_names <- colours_lugami\nnames(colours_lugami_names) <- names_countries\ncolour_table <- \n  tibble(\n    colour = colours_lugami_names,\n    country = names(colours_lugami_names)) |> \n  left_join(country_codes, by = c(\"country\"))\ncolour_table\n\n# A tibble: 10 × 3\n   colour  country        country_code\n   <chr>   <chr>          <chr>       \n 1 #1F78B4 United Kingdom GBR         \n 2 #33A02C Spain          ESP         \n 3 #E31A1C Italy          ITA         \n 4 #FF7F00 Germany        DEU         \n 5 #6A3D9A France         FRA         \n 6 #1F78B4 Sweden         SWE         \n 7 #33A02C Belgium        BEL         \n 8 #E31A1C Netherlands    NLD         \n 9 #FF7F00 Ireland        IRL         \n10 #6A3D9A Denmark        DNK         \n\n\nKeep also a track of that in a vector (useful for graphs with {ggplot2}):\n\ncolour_countries <- colour_table$colour\nnames(colour_countries) <- colour_table$country_code\ncolour_countries\n\n      GBR       ESP       ITA       DEU       FRA       SWE       BEL       NLD \n\"#1F78B4\" \"#33A02C\" \"#E31A1C\" \"#FF7F00\" \"#6A3D9A\" \"#1F78B4\" \"#33A02C\" \"#E31A1C\" \n      IRL       DNK \n\"#FF7F00\" \"#6A3D9A\" \n\n\nLet us create two figures that shows the evolution of the cumulative number of cases through time and the cumulative number of deaths through time, respectively.\nTo that end, we need to reshape the data. First, we need a table in which each row gives the value to be plotted for a given date, a given country and a given type of variable (confirmed cases or recovered).\n\ndf_plot_evolution_numbers <- \n  confirmed_df |> \n  mutate(type = \"Cases\") |> \n  bind_rows(\n    deaths_df |> \n      mutate(type = \"Deaths\")\n  ) |> \n  mutate(type = factor(type, levels = c(\"Cases\", \"Deaths\", \"Recovered\")))\ndf_plot_evolution_numbers\n\n# A tibble: 6,100 × 7\n   country country_code date       value stringency_index days_since_2020_01_22\n   <chr>   <chr>        <date>     <int>            <dbl>                 <dbl>\n 1 Belgium BEL          2020-01-01    NA                0                   -21\n 2 Belgium BEL          2020-01-02    NA                0                   -20\n 3 Belgium BEL          2020-01-03    NA                0                   -19\n 4 Belgium BEL          2020-01-04    NA                0                   -18\n 5 Belgium BEL          2020-01-05    NA                0                   -17\n 6 Belgium BEL          2020-01-06    NA                0                   -16\n 7 Belgium BEL          2020-01-07    NA                0                   -15\n 8 Belgium BEL          2020-01-08    NA                0                   -14\n 9 Belgium BEL          2020-01-09    NA                0                   -13\n10 Belgium BEL          2020-01-10    NA                0                   -12\n# ℹ 6,090 more rows\n# ℹ 1 more variable: type <fct>\n\n\nFrom this table, we can only keep cases and deaths cumulative values, filter the observation to keep only those after January 21, 2020. Let us also add for each line, whether the country it corresponds to is small or large (we will create two panels in the graphs). We can also add the country code.\n\ndf_plot_evolution_numbers_dates <- \n  df_plot_evolution_numbers |> \n  filter(type %in% c(\"Cases\", \"Deaths\")) |> \n  filter(date >= ymd(\"2020-01-21\")) |> \n  mutate(\n    country_type = ifelse(country %in% names_countries_large, yes = \"L\", \"S\"),\n    country_type = factor(\n      country_type, \n      levels = c(\"L\", \"S\"),\n      labels = c(\"Larger countries\", \"Smaller countries\")\n    )\n  ) |> \n  left_join(country_codes, by = c(\"country\", \"country_code\"))\ndf_plot_evolution_numbers_dates\n\n# A tibble: 5,700 × 8\n   country country_code date       value stringency_index days_since_2020_01_22\n   <chr>   <chr>        <date>     <int>            <dbl>                 <dbl>\n 1 Belgium BEL          2020-01-21    NA             0                       -1\n 2 Belgium BEL          2020-01-22     0             0                        0\n 3 Belgium BEL          2020-01-23     0             0                        1\n 4 Belgium BEL          2020-01-24     0             0                        2\n 5 Belgium BEL          2020-01-25     0             0                        3\n 6 Belgium BEL          2020-01-26     0             0                        4\n 7 Belgium BEL          2020-01-27     0             0                        5\n 8 Belgium BEL          2020-01-28     0             5.56                     6\n 9 Belgium BEL          2020-01-29     0             5.56                     7\n10 Belgium BEL          2020-01-30     0             5.56                     8\n# ℹ 5,690 more rows\n# ℹ 2 more variables: type <fct>, country_type <fct>\n\n\nWe want the labels of the legends for the countries to be ordered in a specific way. To that end, let us create two variables that specify this order:\n\norder_countries_L <- c(\"GBR\", \"ESP\", \"ITA\", \"DEU\", \"FRA\")\norder_countries_S <- c(\"SWE\", \"BEL\", \"NLD\", \"IRL\", \"DNK\")\n\n\n4.3.1 Confirmed cases\nLet us focus on the confirmed cases here. We can create two tables: one for the large countries, and another one for small ones:\n\ndf_plot_evolution_numbers_dates_L <- \n  df_plot_evolution_numbers_dates |> \n  filter(country_type == \"Larger countries\") |> \n  filter(type == \"Cases\")\n\nAnd for small countries:\n\ndf_plot_evolution_numbers_dates_S <- \n  df_plot_evolution_numbers_dates |> \n  filter(country_type == \"Smaller countries\") |> \n  filter(type == \"Cases\")\n\nAs we would like the graphs to display the day at which the stringency index reaches its max value for the first time, we need to extract the cumulative number of cases at the corresponding dates.\n\ndf_lockdown_plot <- \n  stringency_dates |> \n  rename(date = start_first_wave) |> \n  left_join(\n    df_plot_evolution_numbers_dates |> \n      filter(type == \"Cases\"),\n    by = c(\"country\", \"date\")\n  )\ndf_lockdown_plot\n\n# A tibble: 10 × 11\n# Groups:   country [10]\n   country        date       start_high_stringency start_reduce_restrict\n   <chr>          <date>     <date>                <date>               \n 1 Belgium        2020-02-04 2020-03-20            2020-05-05           \n 2 Denmark        2020-02-27 2020-03-18            2020-04-15           \n 3 France         2020-01-24 2020-03-17            2020-05-11           \n 4 Germany        2020-01-27 2020-03-22            2020-05-03           \n 5 Ireland        2020-02-29 2020-04-06            2020-05-18           \n 6 Italy          2020-01-31 2020-03-20            2020-04-10           \n 7 Netherlands    2020-02-27 2020-03-23            2020-05-11           \n 8 Spain          2020-02-01 2020-03-30            2020-05-04           \n 9 Sweden         2020-02-01 2020-04-01            2020-06-13           \n10 United Kingdom 2020-01-31 2020-03-24            2020-05-11           \n# ℹ 7 more variables: start_date_sample_second_wave <date>, country_code <chr>,\n#   value <int>, stringency_index <dbl>, days_since_2020_01_22 <dbl>,\n#   type <fct>, country_type <fct>\n\n\nThe two plots can be created:\n\nplot_evolution_numbers_dates_L <- \n  ggplot(\n    data = df_plot_evolution_numbers_dates_L |> \n      mutate(\n        country_code = factor(country_code, levels = order_countries_L)\n      ),\n    mapping = aes(x = date, y = value, colour = country_code)\n  ) +\n  geom_line(linewidth = 1.1) +\n  geom_point(\n    data = df_lockdown_plot |>\n      filter(\n        country %in% unique(df_plot_evolution_numbers_dates_L$country)\n      ),\n    colour = \"black\", size = 4\n  ) +\n  geom_point(\n    data = df_lockdown_plot |> \n      filter(country %in% unique(df_plot_evolution_numbers_dates_L$country)),\n    mapping = aes(colour = country_code), size = 3, show.legend = F\n  ) +\n  scale_shape_discrete(\"Lockdown\") +\n  scale_fill_manual(NULL, values = colour_countries, guide = \"none\") +\n  scale_colour_manual(NULL, values = colour_countries) +\n  labs(x = NULL, y = NULL) +\n  scale_y_continuous(labels = comma) +\n  scale_x_date(\n    breaks = ymd(pretty_dates(df_plot_evolution_numbers_dates$date, n = 5)), \n    date_labels = \"%b %d %Y\"\n  ) +\n  theme_paper() +\n  guides(colour = guide_legend(nrow = 2, byrow = TRUE))\n\nand for small countries:\n\nplot_evolution_numbers_dates_S <- \n  ggplot(\n    data = df_plot_evolution_numbers_dates_S |> \n           mutate(\n             country_code = factor(country_code, levels = order_countries_S)\n             ),\n    mapping = aes(x = date, y = value, colour = country_code)\n    ) +\n  geom_line(linewidth = 1.1) +\n  geom_point(\n    data = df_lockdown_plot |>\n      filter(country %in% unique(df_plot_evolution_numbers_dates_S$country)),\n    colour = \"black\", size = 4\n  ) +\n  geom_point(\n    data = df_lockdown_plot |> \n      filter(country %in% unique(df_plot_evolution_numbers_dates_S$country)),\n    mapping = aes(colour = country_code), size = 3, show.legend = F\n  ) +\n  scale_shape_discrete(\"Lock-down\") +\n  scale_fill_manual(NULL, values = colour_countries, guide = \"none\") +\n  scale_colour_manual(NULL, values = colour_countries) +\n  labs(x = NULL, y = NULL) +\n  scale_y_continuous(labels = comma) +\n  scale_x_date(\n    breaks = ymd(pretty_dates(df_plot_evolution_numbers_dates$date, n = 5)), \n    date_labels = \"%b %d %Y\") +\n  theme_paper() +\n  guides(colour = guide_legend(nrow = 2, byrow = TRUE))\n\nThese two plots can be put together on a single one:\n\np <-\n  arrangeGrob(\n    # Row 1\n    plot_evolution_numbers_dates_L + \n      labs(y = NULL, title = \"(a) Confirmed case, large countries\"),\n    plot_evolution_numbers_dates_S + \n      labs(y = NULL, title = \"(b) Confirmed cases, small countries.\"),\n    nrow = 1\n  ) |> \n  as_ggplot()\n\nWarning: Removed 5 rows containing missing values (`geom_line()`).\nRemoved 5 rows containing missing values (`geom_line()`).\n\np\n\n\n\n\nFigure 4.1: Confirmed cases between the beginning of the Covid-19 epidemics and November 2020.\n\n\n\n\n\n\n4.3.2 Deaths\nLet us focus on the number of deaths here. We can create two tables: one for the large countries, and another one for small ones:\n\ndf_plot_evolution_numbers_dates_L <- \n  df_plot_evolution_numbers_dates |> \n  filter(country_type == \"Larger countries\") |> \n  filter(type == \"Deaths\")\n\nAnd for small countries:\n\ndf_plot_evolution_numbers_dates_S <- \n  df_plot_evolution_numbers_dates |> \n  filter(country_type == \"Smaller countries\") |> \n  filter(type == \"Deaths\")\n\nAs we would like the graphs to display the day at which the stringency index becomes greater or equal to 60 for the first time, we need to extract the cumulative number of cases at the corresponding dates.\n\ndf_lockdown_plot <- \n  stringency_dates |> \n  rename(date = start_first_wave) |> \n  left_join(\n    df_plot_evolution_numbers_dates |> \n      filter(type == \"Deaths\"),\n    by = c(\"country\", \"date\")\n  )\ndf_lockdown_plot\n\n# A tibble: 10 × 11\n# Groups:   country [10]\n   country        date       start_high_stringency start_reduce_restrict\n   <chr>          <date>     <date>                <date>               \n 1 Belgium        2020-02-04 2020-03-20            2020-05-05           \n 2 Denmark        2020-02-27 2020-03-18            2020-04-15           \n 3 France         2020-01-24 2020-03-17            2020-05-11           \n 4 Germany        2020-01-27 2020-03-22            2020-05-03           \n 5 Ireland        2020-02-29 2020-04-06            2020-05-18           \n 6 Italy          2020-01-31 2020-03-20            2020-04-10           \n 7 Netherlands    2020-02-27 2020-03-23            2020-05-11           \n 8 Spain          2020-02-01 2020-03-30            2020-05-04           \n 9 Sweden         2020-02-01 2020-04-01            2020-06-13           \n10 United Kingdom 2020-01-31 2020-03-24            2020-05-11           \n# ℹ 7 more variables: start_date_sample_second_wave <date>, country_code <chr>,\n#   value <int>, stringency_index <dbl>, days_since_2020_01_22 <dbl>,\n#   type <fct>, country_type <fct>\n\n\nThe two plots can be created:\n\nplot_evolution_numbers_dates_L <- \n  ggplot(\n    data = df_plot_evolution_numbers_dates_L |> \n      mutate(\n        country_code = factor(country_code, levels = order_countries_L)\n      ),\n    mapping = aes(x = date, y = value, colour = country_code)\n  ) +\n  geom_line(linewidth = 1.1) +\n  geom_point(\n    data = df_lockdown_plot |>\n      filter(country %in% unique(df_plot_evolution_numbers_dates_L$country)),\n    colour = \"black\", size = 4\n  ) +\n  geom_point(\n    data = df_lockdown_plot |> \n      filter(country %in% unique(df_plot_evolution_numbers_dates_L$country)),\n    mapping = aes(colour = country_code), size = 3, show.legend = F) +\n  labs(x = NULL, y = NULL) +\n  scale_colour_manual(NULL, values = colour_countries) +\n  scale_y_continuous(labels = comma) +\n  scale_x_date(\n    breaks = ymd(pretty_dates(df_plot_evolution_numbers_dates$date, n = 5)), \n    date_labels = \"%b %d %Y\") +\n  theme_paper() +\n  guides(colour = guide_legend(nrow = 2, byrow = TRUE))\n\nand for small countries:\n\nplot_evolution_numbers_dates_S <- \n  ggplot(\n    data = df_plot_evolution_numbers_dates_S |> \n      mutate(country_code = factor(country_code, levels = order_countries_S)),\n    mapping = aes(x = date, y = value, colour = country_code)\n  ) +\n  geom_line(linewidth = 1.1) +\n  geom_point(\n    data = df_lockdown_plot |>\n      filter(country %in% unique(df_plot_evolution_numbers_dates_S$country)),\n    colour = \"black\", size = 4) +\n  geom_point(\n    data = df_lockdown_plot |> \n      filter(country %in% unique(df_plot_evolution_numbers_dates_S$country)),\n    mapping = aes(colour = country_code), size = 3, show.legend = F\n  ) +\n  scale_shape_discrete(\"Lock-down\") +\n  scale_fill_manual(NULL, values = colour_countries, guide = \"none\") +\n  scale_colour_manual(NULL, values = colour_countries) +\n  labs(x = NULL, y = NULL) +\n  scale_y_continuous(labels = comma) +\n  scale_x_date(\n    breaks = ymd(pretty_dates(df_plot_evolution_numbers_dates$date, n = 5)), \n    date_labels = \"%b %d %Y\"\n  ) +\n  theme_paper() +\n  guides(colour = guide_legend(nrow = 2, byrow = TRUE))\n\nThese two plots can be put together on a single one:\n\np <-\n  arrangeGrob(\n    # Row 1\n    plot_evolution_numbers_dates_L + \n      labs(y = NULL, title = \"(a) Confirmed deaths, large countries\"),\n    plot_evolution_numbers_dates_S + \n      labs(y = NULL, title = \"(b) Confirmed deaths, small countries.\"),\n    nrow = 1\n    ) |> \n  as_ggplot()\n\nWarning: Removed 5 rows containing missing values (`geom_line()`).\nRemoved 5 rows containing missing values (`geom_line()`).\n\np\n\n\n\n\nFigure 4.2: Confirmed deaths between the beginning of the Covid-19 epidemics and November 2020.\n\n\n\n\n\n\n4.3.3 Stringency Index\nLet us create a similar plot for the stringency index.\n\ndf_plot_stringency_index <- \n  confirmed_df |> \n  select(country, country_code, date, stringency_index) |> \n  mutate(\n    country_type = ifelse(\n      country %in% c(\"France\", \"Germany\", \"Italy\",\n                     \"Spain\", \"United Kingdom\"),\n      yes = \"L\", \"S\")\n  ) %>%\n  mutate(\n    country_type = factor(\n      country_type, levels = c(\"L\", \"S\"),\n      labels = c(\"Larger countries\", \"Smaller countries\")\n    )\n  )\n\nThen we can create the plot for large countries:\n\nplot_stringency_index_L <- \n  ggplot(\n    data = df_plot_stringency_index |> \n      filter(country_type == \"Larger countries\") |> \n      mutate(country_code = fct_relevel(country_code, order_countries_L)),\n    mapping =  aes(x = date, y = stringency_index, colour = country_code)\n  ) +\n  geom_line(linewidth = 1.1) +\n  geom_hline(yintercept = 70, linetype = \"dotted\") +\n  labs(x = NULL, y = NULL) +\n  scale_colour_manual(NULL, values = colour_countries) +\n  scale_y_continuous(breaks = seq(0, 100, by = 20)) +\n  scale_x_date(\n    breaks = ymd(pretty_dates(df_plot_stringency_index$date, n = 7)), \n    date_labels = \"%b %d %Y\"\n  ) +\n  theme_paper() +\n  guides(colour = guide_legend(nrow = 2, byrow = TRUE))\n\nAnd for small countries:\n\nplot_stringency_index_S <- \n  ggplot(\n    data = df_plot_stringency_index |> \n      filter(country_type == \"Smaller countries\") |> \n      mutate(country_code = fct_relevel(country_code, order_countries_S)),\n    mapping= aes(x = date, y = stringency_index, colour = country_code)\n  ) +\n  geom_line(linewidth = 1.1) +\n  geom_hline(yintercept = 70, linetype = \"dotted\") +\n  labs(x = NULL, y = NULL) +\n  scale_colour_manual(NULL, values = colour_countries) +\n  scale_y_continuous(breaks = seq(0, 100, by = 20)) +\n  scale_x_date(\n    breaks = ymd(pretty_dates(df_plot_stringency_index$date, n = 7)), \n    date_labels = \"%b %d %Y\") +\n  theme_paper() +\n  guides(colour = guide_legend(nrow = 2, byrow = TRUE))\n\nLastly, we can plot these two graphs on a single figure:\n\np <-\n  arrangeGrob(\n    # Row 1\n    plot_stringency_index_L + \n      labs(y = NULL, title = \"(a) Severity index, large countries\"),\n    plot_stringency_index_S + \n      labs(y = NULL, title = \"(b) Severity index, small countries\"),\n    nrow = 1\n  ) |> \n  as_ggplot()\np\n\n\n\n\nFigure 4.3: Severity index between the beginning of the Covid-19 epidemics and November 2020.\n\n\n\n\n\n\n4.3.4 Speed of reaction to the epidemic outbreak\nThe observation of a first case was the sign that the epidemic had reached the country. What was the delay between this first case and a significant reaction identified when the index was greater than 20?\nWe can first extract the date on which the severity index reaches the value of 20 for the first time as follows:\n\nstart_stringency_20 <- \n  confirmed_df |>\n  filter(stringency_index >= 20) |>\n  group_by(country) |>\n  slice(1) |>\n  select(country, date_stringency_20 = date, cases = value) |> \n  ungroup()\n\nThen, we can get the date of the first case for each country:\n\nstart_first_case <- \n  df_plot_evolution_numbers_dates |> \n  filter(type == \"Cases\") |> \n  group_by(country) |> \n  filter(value > 0) |> \n  arrange(date) |> \n  slice(1) |> \n  select(country, first_case = date) |> \n  ungroup()\n\n\nstart_stringency_20 |> \n  left_join(start_first_case) |> \n  mutate(interval = lubridate::interval(first_case, date_stringency_20),\n         delay = interval / lubridate::ddays(1)) |> \n  mutate(country = fct_relevel(country, names_countries)) |> \n  arrange(country) |> \n  select(country, first_case, delay, cases) |>\n  kableExtra::kable()\n\nJoining with `by = join_by(country)`\n\n\n\n\nTable 4.3:  Speed of reaction to the epidemic outbreak \n \n  \n    country \n    first_case \n    delay \n    cases \n  \n \n\n  \n    United Kingdom \n    2020-01-31 \n    46 \n    4452 \n  \n  \n    Spain \n    2020-02-01 \n    37 \n    1073 \n  \n  \n    Italy \n    2020-01-31 \n    21 \n    20 \n  \n  \n    Germany \n    2020-01-27 \n    33 \n    66 \n  \n  \n    France \n    2020-01-24 \n    36 \n    100 \n  \n  \n    Sweden \n    2020-02-01 \n    40 \n    771 \n  \n  \n    Belgium \n    2020-02-04 \n    38 \n    559 \n  \n  \n    Netherlands \n    2020-02-27 \n    12 \n    503 \n  \n  \n    Ireland \n    2020-02-29 \n    12 \n    43 \n  \n  \n    Denmark \n    2020-02-27 \n    5 \n    6 \n  \n\n\n\n\n\n\n\n\n4.3.5 Confinement and deconfinement policies\nLet us check how the different countries proceeded with deconfinement.\nThe date at which the stringency index reached its maximum value within the first 100 days since the end of January:\n\nstart_max_stringency <- \n  confirmed_df |> \n  group_by(country) |> \n  slice(1:100) |>\n  arrange(desc(stringency_index), date) |> \n  slice(1) |> \n  select(country, start = date)\nstart_max_stringency\n\n# A tibble: 10 × 2\n# Groups:   country [10]\n   country        start     \n   <chr>          <date>    \n 1 Belgium        2020-03-20\n 2 Denmark        2020-03-18\n 3 France         2020-03-17\n 4 Germany        2020-03-22\n 5 Ireland        2020-04-06\n 6 Italy          2020-03-20\n 7 Netherlands    2020-03-23\n 8 Spain          2020-03-30\n 9 Sweden         2020-04-01\n10 United Kingdom 2020-03-24\n\n\nWe can easily obtain the date on which the index begins to fall from its maximum value, corresponding to a relaxation of policy measures (i.e., end of lockdown):\n\npolicies <- \n  confirmed_df |> \n  select(country, date, stringency_index) |> \n  left_join(start_max_stringency, by = c(\"country\")) |> \n  group_by(country) |> \n  arrange(date) |> \n  filter(date >= start) |> \n  mutate(tmp = dplyr::lag(stringency_index)) |>\n  mutate(same_strin = stringency_index == tmp) |>\n  mutate(same_strin = ifelse(row_number()==1, TRUE, same_strin)) |>\n  filter(same_strin == FALSE) |>\n  slice(1) |> \n  mutate(length = lubridate::interval(start, date) / ddays(1)) |> \n  select(country, start, end=date, length)\npolicies\n\n# A tibble: 10 × 4\n# Groups:   country [10]\n   country        start      end        length\n   <chr>          <date>     <date>      <dbl>\n 1 Belgium        2020-03-20 2020-05-05     46\n 2 Denmark        2020-03-18 2020-04-15     28\n 3 France         2020-03-17 2020-05-11     55\n 4 Germany        2020-03-22 2020-05-03     42\n 5 Ireland        2020-04-06 2020-05-18     42\n 6 Italy          2020-03-20 2020-04-10     21\n 7 Netherlands    2020-03-23 2020-05-11     49\n 8 Spain          2020-03-30 2020-05-04     35\n 9 Sweden         2020-04-01 2020-06-13     73\n10 United Kingdom 2020-03-24 2020-05-11     48\n\n\nThe average of the strigency index between the max value and the end of containment can be obtained as follows:\n\nstrength_containment <- \n  confirmed_df |> \n  select(country, date, stringency_index) |> \n  left_join(policies, by = \"country\") |> \n  filter(date >= start, date < end) |> \n  group_by(country) |> \n  summarise(strength = mean(stringency_index))\nstrength_containment\n\n# A tibble: 10 × 2\n   country        strength\n   <chr>             <dbl>\n 1 Belgium            81.5\n 2 Denmark            72.2\n 3 France             88.0\n 4 Germany            76.8\n 5 Ireland            90.7\n 6 Italy              85.2\n 7 Netherlands        78.7\n 8 Spain              85.2\n 9 Sweden             64.8\n10 United Kingdom     79.6\n\n\nWe can count how many smoothing and how many restrengthening actions were made till the end of our sample:\n\npolicy_changes <- \n  confirmed_df |> \n  select(country, date, stringency_index) |> \n  left_join(policies, by = \"country\") |> \n  filter(date >=  end, date <= end_date_sample) |> \n  group_by(country) |> \n  mutate(tmp = dplyr::lag(stringency_index)) |> \n  mutate(tmp = ifelse(row_number()==1, stringency_index, tmp)) |> \n  mutate(same_strin = stringency_index == tmp) |>\n  mutate(\n    smoothing = ifelse(!same_strin & stringency_index < tmp, TRUE, FALSE),\n    restrenghtening = ifelse(\n      !same_strin & stringency_index > tmp, TRUE, FALSE)\n  ) |> \n  summarise(\n    changes_smo = sum(smoothing),\n    changes_res = sum(restrenghtening)\n  )\npolicy_changes\n\n# A tibble: 10 × 3\n   country        changes_smo changes_res\n   <chr>                <int>       <int>\n 1 Belgium                  8           3\n 2 Denmark                  5           1\n 3 France                   6           3\n 4 Germany                  9           4\n 5 Ireland                  4           2\n 6 Italy                    5           4\n 7 Netherlands              4           2\n 8 Spain                    9           4\n 9 Sweden                   1           0\n10 United Kingdom           8           4\n\n\nLastly, we can gather all this information in a single table:\n\npolicies |> \n  left_join(strength_containment, by = \"country\") |> \n  left_join(policy_changes, by = \"country\") |> \n  mutate(country = factor(country)) |> \n  mutate(country = fct_relevel(country, names_countries)) |> \n  arrange(country) |> \n  mutate(\n    start = format(start, \"%B %d\"),\n    end = format(end, \"%B %d\")\n  ) |> \n  kableExtra::kable()\n\n\n?(caption)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncountry\n\n\n\n\n\n\n\n\n\n\n\n\nstart\n\n\n\n\n\n\n\n\n\n\n\n\nend\n\n\n\n\n\n\n\n\n\n\n\n\nlength\n\n\n\n\n\n\n\n\n\n\n\n\nstrength\n\n\n\n\n\n\n\n\n\n\n\n\nchanges_smo\n\n\n\n\n\n\n\n\n\n\n\n\nchanges_res\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnited Kingdom\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 24\n\n\n\n\n\n\n\n\n\n\n\n\nMay 11\n\n\n\n\n\n\n\n\n\n\n\n\n48\n\n\n\n\n\n\n\n\n\n\n\n\n79.63\n\n\n\n\n\n\n\n\n\n\n\n\n8\n\n\n\n\n\n\n\n\n\n\n\n\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpain\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 30\n\n\n\n\n\n\n\n\n\n\n\n\nMay 04\n\n\n\n\n\n\n\n\n\n\n\n\n35\n\n\n\n\n\n\n\n\n\n\n\n\n85.19\n\n\n\n\n\n\n\n\n\n\n\n\n9\n\n\n\n\n\n\n\n\n\n\n\n\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nItaly\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 20\n\n\n\n\n\n\n\n\n\n\n\n\nApril 10\n\n\n\n\n\n\n\n\n\n\n\n\n21\n\n\n\n\n\n\n\n\n\n\n\n\n85.19\n\n\n\n\n\n\n\n\n\n\n\n\n5\n\n\n\n\n\n\n\n\n\n\n\n\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGermany\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 22\n\n\n\n\n\n\n\n\n\n\n\n\nMay 03\n\n\n\n\n\n\n\n\n\n\n\n\n42\n\n\n\n\n\n\n\n\n\n\n\n\n76.85\n\n\n\n\n\n\n\n\n\n\n\n\n9\n\n\n\n\n\n\n\n\n\n\n\n\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrance\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 17\n\n\n\n\n\n\n\n\n\n\n\n\nMay 11\n\n\n\n\n\n\n\n\n\n\n\n\n55\n\n\n\n\n\n\n\n\n\n\n\n\n87.96\n\n\n\n\n\n\n\n\n\n\n\n\n6\n\n\n\n\n\n\n\n\n\n\n\n\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSweden\n\n\n\n\n\n\n\n\n\n\n\n\nApril 01\n\n\n\n\n\n\n\n\n\n\n\n\nJune 13\n\n\n\n\n\n\n\n\n\n\n\n\n73\n\n\n\n\n\n\n\n\n\n\n\n\n64.81\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBelgium\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 20\n\n\n\n\n\n\n\n\n\n\n\n\nMay 05\n\n\n\n\n\n\n\n\n\n\n\n\n46\n\n\n\n\n\n\n\n\n\n\n\n\n81.48\n\n\n\n\n\n\n\n\n\n\n\n\n8\n\n\n\n\n\n\n\n\n\n\n\n\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetherlands\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 23\n\n\n\n\n\n\n\n\n\n\n\n\nMay 11\n\n\n\n\n\n\n\n\n\n\n\n\n49\n\n\n\n\n\n\n\n\n\n\n\n\n78.70\n\n\n\n\n\n\n\n\n\n\n\n\n4\n\n\n\n\n\n\n\n\n\n\n\n\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIreland\n\n\n\n\n\n\n\n\n\n\n\n\nApril 06\n\n\n\n\n\n\n\n\n\n\n\n\nMay 18\n\n\n\n\n\n\n\n\n\n\n\n\n42\n\n\n\n\n\n\n\n\n\n\n\n\n90.74\n\n\n\n\n\n\n\n\n\n\n\n\n4\n\n\n\n\n\n\n\n\n\n\n\n\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDenmark\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 18\n\n\n\n\n\n\n\n\n\n\n\n\nApril 15\n\n\n\n\n\n\n\n\n\n\n\n\n28\n\n\n\n\n\n\n\n\n\n\n\n\n72.22\n\n\n\n\n\n\n\n\n\n\n\n\n5\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview of the smoothing and restrenghthening actions during between the beginning of the Codiv-19 epidemic and November 2020."
  },
  {
    "objectID": "covid-data.html#saving-the-results",
    "href": "covid-data.html#saving-the-results",
    "title": "4  Covid-19 Data",
    "section": "4.4 Saving the results",
    "text": "4.4 Saving the results\nLet us save the following R objects for later use.\n\nsave(\n  confirmed_df,\n  deaths_df,\n  population,\n  h,\n  stringency_dates,\n  names_countries,\n  names_countries_large,\n  names_countries_small,\n  order_countries_L,\n  order_countries_S,\n  colour_countries,\n  colour_table,\n  country_codes,\n  theme_paper,\n  file = \"data/data_after_load.rda\"\n  )\n\n\n\n\n\nLi, Qun, Xuhua Guan, Peng Wu, Xiaoye Wang, Lei Zhou, Yeqing Tong, Ruiqi Ren, et al. 2020. “Early Transmission Dynamics in Wuhan, China, of Novel Coronavirus-Infected Pneumonia.” New England Journal of Medicine 382 (13): 1199–1207. https://doi.org/10.1056/NEJMoa2001316."
  },
  {
    "objectID": "reproduction-number.html#load-data",
    "href": "reproduction-number.html#load-data",
    "title": "5  Estimating the reproduction number",
    "section": "5.1 Load data",
    "text": "5.1 Load data\n\n# FOR UNIX USERS \nSys.setlocale(\"LC_ALL\", \"en_US.UTF-8\")\n\n[1] \"en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\"\n\n# FOR WINDOWS USERS\n# Sys.setlocale(\"LC_ALL\", \"English_United States\")\n\nSome packages that will be used:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nlibrary(minpack.lm)\nlibrary(mvtnorm)\n\nLet us load the data (results obtained from Chapter 4).\n\nload(\"data/data_after_load.rda\")"
  },
  {
    "objectID": "reproduction-number.html#objective",
    "href": "reproduction-number.html#objective",
    "title": "5  Estimating the reproduction number",
    "section": "5.2 Objective",
    "text": "5.2 Objective\nIn this section, we present the codes to estimate the reproduction number of the first and second waves using either an exponential model or a generalized exponential model. Based on those models, we estimate the reproduction number \\(\\mathcal{R}_0\\) for both waves.\nWe consider the following start and end dates for each sample, for each country:\n\n\n\n\n\n\n\n\nWave\nStart\nEnd\n\n\n\n\nFirst\nDate at which the number of cases exceeds 0\nSeven days after the stringency index reaches its max value (during the first 100 days of the epidemics)\n\n\nSecond\nSixty days after the stringency index begins to decrease from its maximum value\nSeptember 30, 2020\n\n\n\nFor Sweden, the severity index does not reach 70. Here, we use the dates from Ireland for the definition of those for Sweden.\nFor the estimation of the reproduction number \\(\\mathcal{R}_0\\), we rely on the estimations made by Li et al. (2020). We propose to set the window size \\(h\\) so that it represents 0.99 of the probability in the Gamma distribution of the serial interval.\n\n# Individual data\n# From Li et al. (2020):\nmean_si <- 7.5\nstd_si <- 3.4\n\nshape <- (mean_si / std_si)^2\nscale <- mean_si / shape\nh <- ceiling(qgamma(p = .99, shape = shape, scale = scale))\nh\n\n[1] 18"
  },
  {
    "objectID": "reproduction-number.html#some-functions-needed-for-the-estimation",
    "href": "reproduction-number.html#some-functions-needed-for-the-estimation",
    "title": "5  Estimating the reproduction number",
    "section": "5.3 Some functions needed for the estimation",
    "text": "5.3 Some functions needed for the estimation\nLet us create a function that, when provided with the name of the country, returns a table for that country that contains the following columns:\n\nstart_first_wave: start of the first wave, defined as the first date when the cumulative number of cases is greater than 1\nstart_high_stringency: date at which the stringency index reaches 70 for the first time within the first 100 days of the sample (for Sweden, as the index never reached 70, we use the time at which the stringency is at its maximum value for the first time within the same time interval)\nstart_reduce_restrict: moment at which the restrictions of the first wave starts to lower\nstart_date_sample_second_wave: 60 days after the relaxation of restrictions (60 days after after start_reduce_restrict)\nlength_high_stringency: number of days between start_high_stringency and start_reduce_restrict`.\n\n\n#' Gives the dates of the different periods (first wave, start of containment, ...)\n#' @param country_name name of the country\n#' @param type if `\"deaths\"` returns the number of deaths, otherwise the number of cases\nget_dates <- function(country_name) {\n  df_country <- confirmed_df |> \n    filter(country == !!country_name)\n  \n  # Start of the first wave\n  start_first_wave <- \n    df_country |> \n    arrange(date) |> \n    filter(value > 0) |> \n    slice(1) |> \n    magrittr::extract2(\"date\")\n  \n  # Start of period with severity greater or equal than 70 index among the first 100 days\n  start_high_stringency <- \n    df_country |> \n    slice(1:100) |>\n    filter(stringency_index >= 70) |> \n    slice(1) |> \n    magrittr::extract2(\"date\")\n  \n  # Max for Sweden\n  if(country_name == \"Sweden\"){\n    start_high_stringency <- \n      df_country |> \n      slice(1:100) |>\n      arrange(desc(stringency_index), date) |>\n      slice(1) |> \n      magrittr::extract2(\"date\")\n  }\n  \n  # Max stringency first 100 days\n  start_max_stringency <- \n    df_country |> \n    slice(1:100) |>\n    arrange(desc(stringency_index), date) |>\n    slice(1) |> \n    magrittr::extract2(\"date\")\n\n  # Moment at which the restrictions of the first wave starts to lower\n  start_reduce_restrict <- \n    df_country |> \n    arrange(date) |> \n    filter(date >= start_max_stringency) |> \n    mutate(tmp = dplyr::lag(stringency_index)) |> \n    mutate(same_strin = stringency_index == tmp) |> \n    mutate(same_strin = ifelse(row_number()==1, TRUE, same_strin)) |> \n    filter(same_strin == FALSE) |> \n    slice(1) |>\n    magrittr::extract2(\"date\")\n  \n  start_date_sample_second_wave <- start_reduce_restrict + lubridate::ddays(60)\n  \n  \n  # Length of high stringency period\n  length_high_stringency <- lubridate::interval(\n    start_high_stringency, start_reduce_restrict) / lubridate::ddays(1)\n  \n  tibble(\n    country = country_name,\n    start_first_wave = start_first_wave,\n    start_high_stringency = start_high_stringency,\n    start_reduce_restrict = start_reduce_restrict,\n    start_date_sample_second_wave = start_date_sample_second_wave,\n    length_high_stringency = length_high_stringency\n  )\n}# End of get_dates()\n\nIf we apply this function for each of the 10 countries of interest:\n\nmap_df(names_countries, get_dates) |> \n  kableExtra::kable()\n\n\n\nTable 5.1:  Key dates for each countries \n \n  \n    country \n    start_first_wave \n    start_high_stringency \n    start_reduce_restrict \n    start_date_sample_second_wave \n    length_high_stringency \n  \n \n\n  \n    United Kingdom \n    2020-01-31 \n    2020-03-23 \n    2020-05-11 \n    2020-07-10 \n    49 \n  \n  \n    Spain \n    2020-02-01 \n    2020-03-17 \n    2020-05-04 \n    2020-07-03 \n    48 \n  \n  \n    Italy \n    2020-01-31 \n    2020-03-04 \n    2020-04-10 \n    2020-06-09 \n    37 \n  \n  \n    Germany \n    2020-01-27 \n    2020-03-22 \n    2020-05-03 \n    2020-07-02 \n    42 \n  \n  \n    France \n    2020-01-24 \n    2020-03-17 \n    2020-05-11 \n    2020-07-10 \n    55 \n  \n  \n    Sweden \n    2020-02-01 \n    2020-04-01 \n    2020-06-13 \n    2020-08-12 \n    73 \n  \n  \n    Belgium \n    2020-02-04 \n    2020-03-18 \n    2020-05-05 \n    2020-07-04 \n    48 \n  \n  \n    Netherlands \n    2020-02-27 \n    2020-03-23 \n    2020-05-11 \n    2020-07-10 \n    49 \n  \n  \n    Ireland \n    2020-02-29 \n    2020-03-27 \n    2020-05-18 \n    2020-07-17 \n    52 \n  \n  \n    Denmark \n    2020-02-27 \n    2020-03-18 \n    2020-04-15 \n    2020-06-14 \n    28 \n  \n\n\n\n\n\n\nBased on those dates, we can create a function that will prepare the dataset that will be used to estimate the exponential model, for each country, for the first wave (wave=\"first\") or for the second (wave=\"second\"). This functions returns a list of two elements:\n\nThe dataset\nThe table which gives the dates obtained with the function get_dates(). We add two columns to that table : the start and end date of the sample.\n\n\n#' Extracts the cases data for a country\n#' @param country_name name of the country\n#' @param sample \nget_cases_country <- function(country_name,\n                              sample = c(\"first\", \"second\")) {\n  df_country <- \n    confirmed_df |> \n    filter(country == !!country_name)\n  dates_country <- get_dates(country_name)\n  \n  # Maximum of the severity index\n  max_severity <- max(df_country$stringency_index, na.rm=TRUE)\n  dates_country$max_severity <- max_severity\n  \n  if (sample == \"first\") {\n    df_country <- \n      df_country |> \n      # `out_of_sample_horizon` more days for out-of-sample pred\n      filter(\n        date >= dates_country$start_first_wave,\n        date <= (dates_country$start_high_stringency + \n                   lubridate::ddays(7) + \n                   lubridate::ddays(out_of_sample_horizon))\n      )\n  } else {\n    df_country <- \n      df_country |> \n      filter(date >= dates_country$start_date_sample_second_wave)\n    \n    # Let us remove the number of cases of the first date of this sample\n    # to all observation (translation to 1)\n    start_val_cases <- df_country$value[1]\n    df_country <- \n      df_country |> \n      mutate(value = value - start_val_cases + 1)\n  }\n  \n  \n  # Moving Average for missing values (i.e., for Ireland)\n  if (any(is.na(df_country$value))) {\n    replacement_values <- round(\n      zoo::rollapply(\n        df_country$value, \n        width=3, \n        FUN=function(x) mean(x, na.rm=TRUE), \n        by=1, by.column=TRUE, partial=TRUE, fill=NA, align=\"center\"\n      )\n    )\n    # Replace only missing values\n    df_country <- \n      df_country |> \n      mutate(replacement_values = !!replacement_values) |> \n      mutate(\n        value = ifelse(\n          is.na(value), \n          yes = replacement_values, \n          no = value\n        )\n      ) |> \n      select(-replacement_values)\n  }\n  \n  df_country <- \n    df_country |> \n    mutate(t = row_number() - 1) |> \n    mutate(y = value)\n  \n  dates_country <- \n    dates_country |> \n    mutate(\n      start_sample = first(df_country$date),\n      end_sample_in = last(df_country$date) - \n        lubridate::ddays(out_of_sample_horizon),\n      end_sample_out = last(df_country$date)\n    )\n\n  list(df_country = df_country, dates_country = dates_country)\n}\n\nFor example, for the UK:\n\nout_of_sample_horizon <- 0 # This variable is explained later\ncases_uk <- get_cases_country(\n  country_name = \"United Kingdom\", sample = \"first\"\n)\ncases_uk$df_country\n\n# A tibble: 60 × 8\n   country  country_code date       value stringency_index days_since_2020_01_22\n   <chr>    <chr>        <date>     <int>            <dbl>                 <dbl>\n 1 United … GBR          2020-01-31     2             8.33                     9\n 2 United … GBR          2020-02-01     2             8.33                    10\n 3 United … GBR          2020-02-02     2            11.1                     11\n 4 United … GBR          2020-02-03     8            11.1                     12\n 5 United … GBR          2020-02-04     8            11.1                     13\n 6 United … GBR          2020-02-05     9            11.1                     14\n 7 United … GBR          2020-02-06     9            11.1                     15\n 8 United … GBR          2020-02-07     9            11.1                     16\n 9 United … GBR          2020-02-08    13            11.1                     17\n10 United … GBR          2020-02-09    14            11.1                     18\n# ℹ 50 more rows\n# ℹ 2 more variables: t <dbl>, y <int>\n\ncases_uk$dates_country\n\n# A tibble: 1 × 10\n  country        start_first_wave start_high_stringency start_reduce_restrict\n  <chr>          <date>           <date>                <date>               \n1 United Kingdom 2020-01-31       2020-03-23            2020-05-11           \n# ℹ 6 more variables: start_date_sample_second_wave <date>,\n#   length_high_stringency <dbl>, max_severity <dbl>, start_sample <date>,\n#   end_sample_in <date>, end_sample_out <date>\n\n\nWe need to write the prediction function for the exponential model and for the generalized exponential model. To compute the reproduction number \\(\\mathcal{R}_0\\), we also need to write down the first derivative of these functions, with respect to the time component (x in the functions).\nFor the exponential model:\n\n#' Exponential model function \n#' @param theta vector of named parameters\n#' @param x observation / training example\nexponential_f <- function(theta, x) {\n  c0 <- theta[[\"c0\"]]\n  r <- theta[[\"r\"]]\n  c0 * exp(r * x)\n}\n\n\n#' Derivative of exponential for R0\n#' @param theta vector of coefficients\n#' @param x time values\nderivative_exponential <- function(theta, x) {\n  c0 <- theta[[\"c0\"]]\n  r <- theta[[\"r\"]]\n  c0 * r * exp(r * x)\n}\n\nFor the generalized exponential model:\n\n#' General Exponential model function \n#' @param theta vector of named parameters\n#' @param x observation / training example\ngen_exponential_f <- function(theta, x) {\n  A <- theta[[\"A\"]]\n  r <- theta[[\"r\"]]\n  alpha <- theta[[\"alpha\"]]\n  ((1 - alpha) * r * x + A)^( 1 / (1 - alpha))\n}\n\n\n#' Derivative of generalized exponential for R0\nderivative_gen_exponential <- function(theta, x) {\n  A <- theta[[\"A\"]]\n  r <- theta[[\"r\"]]\n  alpha <- theta[[\"alpha\"]]\n  # r * ( ( 1 - alpha ) * r * x + A )^( alpha / ( 1 - alpha ) )\n  numb <- A-alpha*r*x + r*x\n  expon <- alpha / ( 1 - alpha )\n  r * sign(numb) * abs(numb)^expon\n}\n\nThe effective reproduction number \\(R_t\\) is obtained, with the exponential model as follows (Cori et al. 2013):\n\\[\nR_t = \\frac{I_t}{\\sum_{s=1}^t I_{t-s} \\omega(s)},\n\\] An estimation of the reproduction number \\(\\mathcal{R}_0\\), can be obtained by truncating this summation: \\[\nR_t = \\frac{I_t}{\\sum_{s=1}^h I_{t-s} \\omega(s)}\n\\]\n\n#' R0 for exponential\n#' \n#' @param ti\n#' @param h size of the window\n#' @param theta estimated coefficients\n#' @param shape @param scale shape and scale parameters of the Gamma distribution\nR0_expo <- function(ti,\n                    h, \n                    theta,\n                    shape,\n                    scale) {\n  s_R <- 0\n  for (s in 1:h) {\n    s_R <- s_R + \n      derivative_exponential(theta, ti-s) * \n      dgamma(s, shape = shape, scale = scale)\n  }\n  \n  R0 <- derivative_exponential(theta, ti) / s_R\n  R0\n}\n\nFor the generalized exponential model:\n\\[\nR_t = \\frac{((1-\\alpha)\\,r\\,t+A)^{\\alpha/(1-\\alpha)}}{\\sum_{s=1}^h ((1-\\alpha)\\,r\\,(t-s)+A)^{\\alpha/(1-\\alpha)} \\omega(s)}.\n\\]\n\n#' R0 for generalized exponential\n#' \n#' @param ti\n#' @param h size of the window\n#' @param theta estimated coefficients\n#' @param shape @param scale shape and scale parameters of the Gamma distribution\nR0_gen_expo <- function(ti,\n                        h,\n                        theta,\n                        shape,\n                        scale) {\n  s_R <- 0\n  for (s in 1:h) {\n    s_R <- s_R + \n      derivative_gen_exponential(theta, ti-s) * \n      dgamma(s, shape = shape, scale = scale)\n  }\n  \n  R0 <- derivative_gen_exponential(theta, ti) / s_R\n  R0\n}\n\nLet us define a loss function. We will use that function to try to find the parameters of the model (either the exponential model or the generalized exponential model) which minimize it. Note that we use the nls.lm() function from {minpack.lm}; hence we only need to compute the residuals and not the residual sum of square.\n\n#' Loss function\n#' \n#' @param theta vector of named parameters of the model\n#' @param fun prediction function of the model\n#' @param y target variable\n#' @param t time component (feature)\nloss_function <- function(theta,\n                          fun,\n                          y,\n                          t) {\n  (y - fun(theta = theta, x = t))\n}\n\nOnce the model are estimated, we can compute some goodness of fit criteria. Let us create a function that computes the AIC, the BIC and the RMSE for a specific model. The function expects three arguments: the prediction function of the model (f), the values for the parameters of the model (in a named vector – theta), and the observations (data).\n\n#' Compute some goodness of fit criteria\n#' \n#' @param f prediction function of the model\n#' @param data data that contains the two columns `y` and `t`\n#' @param theta estimated coefficients for the model (used in `f`)\nget_criteria <- function(f,\n                         data,\n                         theta) {\n  n <- nrow(data)\n  k <- length(theta)\n  w <- rep(1, n)\n  \n  errors <- loss_function(\n    theta = theta,\n    fun = f,\n    y = data$y,\n    t = data$t\n  )\n  \n  mse <- sum(errors^2) / n\n  rmse <- sqrt(mse)\n  \n  # Log-likelihood\n  ll <- 0.5 * \n    (sum(log(w)) - n * \n       (log(2 * pi) + 1 - log(n) + log(sum(w * errors^2)))\n    )\n  aic <- 2 * (k + 1) - 2 * ll\n  bic <- -2 * ll + log(n) * (k + 1)\n  \n  c(AIC = aic, BIC = bic, RMSE = rmse)\n}\n\nLastly, to get a confidence interval for the estimated reproduction number \\(\\mathcal{R}_0\\), we create a function that performs simulations. From the estimated exponential model (or generalized exponential model), we compute the variance-covariance matrix and then randomly draw n_repl observations from a multivariate normal distribution. Based on these simulated numbers, we estimate the reproduction number using the R0_expo() or R0_gen_expo() function previously defined. We finally compute the average \\(\\mathcal{R}_0\\) and its standard deviation based on the n_repl simulations.\n\n#' Compute variance-covariance matrix from nls.lm\n#' Simulate a Normal and compute the corresponding $\\mathcal{R}_0$\n#' \n#' @param out result of nls.lm estimation\n#' @param n_repl numbr of desired replications (default to 1,000)\n#' @param ti \n#' @param h window length\n#' @param model_name if `\"Exponential\"` then uses the Exponential model formula.\n#'  Otherwise, the General Exponential one.\nsim_ec <- function(out,\n                   n_repl = 1000,\n                   ti,\n                   h,\n                   model_name = c(\"Exponential\", \"Gen_Exponential\")) {\n  \n  ibb    <- chol(out$hessian)\n  ih     <- chol2inv(ibb)\n  p      <- length(out$par)\n  rdf    <- length(out$fvec) - p\n  resvar <- out$deviance/rdf\n  se     <- sqrt(diag(ih) * resvar)\n  mean   <- out$par\n  \n  Sigv <- ih*resvar\n  the  <- rmvnorm(n = n_repl, mean = unlist(mean), sigma = Sigv)\n  \n  Ro <- rep(0,n_repl)\n  if (model_name == \"Exponential\") {\n    for (i in 1:n_repl) {\n      Ro[i] <- R0_expo(\n        ti = ti, \n        h = h,\n        theta = the[i,],\n        shape = shape,\n        scale = scale\n      )\n    }\n  }else{\n    for (i in 1:n_repl) {\n      Ro[i] <- R0_gen_expo(\n        ti = ti,\n        h = h,\n        theta = the[i,], \n        shape = shape,\n        scale = scale\n      )\n    }\n  }\n  \n  R0_mu <- mean(Ro)\n  R0_sd <- sd(Ro)\n  \n  c(R0_mu = R0_mu, R0_sd = R0_sd)\n}"
  },
  {
    "objectID": "reproduction-number.html#example-with-only-one-country",
    "href": "reproduction-number.html#example-with-only-one-country",
    "title": "5  Estimating the reproduction number",
    "section": "5.4 Example with only one country",
    "text": "5.4 Example with only one country\nLet us estimate an exponential model first and then a generalized exponential model on the number of cases for one country, United Kingdom. Then we can create a wraping function to apply the codes to all countries.\n\ncountry_name <- \"United Kingdom\"\n\n\n5.4.1 Exponential model\n\nmodel_function <- exponential_f\n\nWe need to get the data that contain the number of cases for the UK. The previously defined get_cases_country() function can be used:\n\ncases_country <- get_cases_country(country_name, sample = \"first\")\ndf_country <- cases_country$df_country\ndates_country <- cases_country$dates_country\ncases_country\n\n$df_country\n# A tibble: 60 × 8\n   country  country_code date       value stringency_index days_since_2020_01_22\n   <chr>    <chr>        <date>     <int>            <dbl>                 <dbl>\n 1 United … GBR          2020-01-31     2             8.33                     9\n 2 United … GBR          2020-02-01     2             8.33                    10\n 3 United … GBR          2020-02-02     2            11.1                     11\n 4 United … GBR          2020-02-03     8            11.1                     12\n 5 United … GBR          2020-02-04     8            11.1                     13\n 6 United … GBR          2020-02-05     9            11.1                     14\n 7 United … GBR          2020-02-06     9            11.1                     15\n 8 United … GBR          2020-02-07     9            11.1                     16\n 9 United … GBR          2020-02-08    13            11.1                     17\n10 United … GBR          2020-02-09    14            11.1                     18\n# ℹ 50 more rows\n# ℹ 2 more variables: t <dbl>, y <int>\n\n$dates_country\n# A tibble: 1 × 10\n  country        start_first_wave start_high_stringency start_reduce_restrict\n  <chr>          <date>           <date>                <date>               \n1 United Kingdom 2020-01-31       2020-03-23            2020-05-11           \n# ℹ 6 more variables: start_date_sample_second_wave <date>,\n#   length_high_stringency <dbl>, max_severity <dbl>, start_sample <date>,\n#   end_sample_in <date>, end_sample_out <date>\n\ndates_country\n\n# A tibble: 1 × 10\n  country        start_first_wave start_high_stringency start_reduce_restrict\n  <chr>          <date>           <date>                <date>               \n1 United Kingdom 2020-01-31       2020-03-23            2020-05-11           \n# ℹ 6 more variables: start_date_sample_second_wave <date>,\n#   length_high_stringency <dbl>, max_severity <dbl>, start_sample <date>,\n#   end_sample_in <date>, end_sample_out <date>\n\n\nHere are some starting values for the optimization algorithm:\n\n# The starting values\nstart <- list(\n  c0 = 1,\n  r  = .14\n)\n\nThe function we want to minimize is the loss function, previously defined in loss_function(). It expects four arguments:\n\ntheta: a vector of named coefficients\nfun: a prediction function (for the exponential model, we pass on the function exponential_f())\ny: a vector of observed valued\nt: a vector containing the time component.\n\nThe nls.lm() function can then be used. We provide the starting values to the par argument. The fn argument is provided with the function to minimize. We also set the maxiter component of the control argument to 100 (maximum number of iterations). The argument y, t and fun are directly passed on to the arguments of the function given to the fn argument.\n\n# The estimated coefficients\nout <- nls.lm(\n  par = start, \n  fn = loss_function,\n  y = df_country$y,\n  t = df_country$t,\n  fun = model_function,\n  control = nls.lm.control(maxiter = 100),\n  jac = NULL, lower = NULL, upper = NULL\n)\n\nThe results can be summarized as follows:\n\nsummary(out)\n\n\nParameters:\n   Estimate Std. Error t value Pr(>|t|)    \nc0 4.542031   0.587775   7.727 1.76e-10 ***\nr  0.151973   0.002299  66.092  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 523.9 on 58 degrees of freedom\nNumber of iterations to termination: 9 \nReason for termination: Relative error in the sum of squares is at most `ftol'. \n\n\nThe estimated coefficients can be extracted and saved in a tibble:\n\nparams <- tibble(\n  model_type = \"Exponential\",\n  country = country_name,\n  coef_estimate_name = names(coef(out)),\n  coef_estimate = coef(out)\n)\nparams\n\n# A tibble: 2 × 4\n  model_type  country        coef_estimate_name coef_estimate\n  <chr>       <chr>          <chr>                      <dbl>\n1 Exponential United Kingdom c0                         4.54 \n2 Exponential United Kingdom r                          0.152\n\n\nThe goodness of fit criterion can be computed using the get_criteria() function previously defined.\n\ncrit <- get_criteria(\n  f = model_function, \n  data = df_country,\n  theta = params$coef_estimate\n)\ncrit\n\n     AIC      BIC     RMSE \n925.5892 931.8723 515.0712 \n\n\nAnd they can be stored in a tibble:\n\ncriteria <- \n  tibble(\n    model_type = \"Exponential\",\n    country = country_name,\n    bind_rows(crit)\n  )\ncriteria\n\n# A tibble: 1 × 5\n  model_type  country          AIC   BIC  RMSE\n  <chr>       <chr>          <dbl> <dbl> <dbl>\n1 Exponential United Kingdom  926.  932.  515.\n\n\nThe \\(\\mathcal{R}_0\\) can be estimated with the sim_ec() function:\n\nR0_i <- sim_ec(\n  out = out,\n  n_repl = 1000,\n  ti = h,\n  h = h,\n  model_name = \"Exponential\"\n)\nR0_i\n\n     R0_mu      R0_sd \n2.78484622 0.03818434 \n\n\nThey can also be saved in a tibble:\n\nR0_df <- \n  tibble(\n    model_type = \"Exponential\",\n    country = country_name,\n    bind_rows(R0_i)\n  )\nR0_df\n\n# A tibble: 1 × 4\n  model_type  country        R0_mu  R0_sd\n  <chr>       <chr>          <dbl>  <dbl>\n1 Exponential United Kingdom  2.78 0.0382\n\n\nThen, we can plot the observed values and the estimated ones. First, let us get the estimated values, using the obtained parameters:\n\nfitted_val_expo_uk <- \n  df_country |> \n  mutate(index = row_number()-1) |> \n  mutate(\n    model_type   = \"Exponential\",\n    fitted_value = model_function(theta = params$coef_estimate, x = index)\n  )\nfitted_val_expo_uk\n\n# A tibble: 60 × 11\n   country  country_code date       value stringency_index days_since_2020_01_22\n   <chr>    <chr>        <date>     <int>            <dbl>                 <dbl>\n 1 United … GBR          2020-01-31     2             8.33                     9\n 2 United … GBR          2020-02-01     2             8.33                    10\n 3 United … GBR          2020-02-02     2            11.1                     11\n 4 United … GBR          2020-02-03     8            11.1                     12\n 5 United … GBR          2020-02-04     8            11.1                     13\n 6 United … GBR          2020-02-05     9            11.1                     14\n 7 United … GBR          2020-02-06     9            11.1                     15\n 8 United … GBR          2020-02-07     9            11.1                     16\n 9 United … GBR          2020-02-08    13            11.1                     17\n10 United … GBR          2020-02-09    14            11.1                     18\n# ℹ 50 more rows\n# ℹ 5 more variables: t <dbl>, y <int>, index <dbl>, model_type <chr>,\n#   fitted_value <dbl>\n\n\n\nggplot(\n  data = fitted_val_expo_uk |> \n    select(date, value, fitted_value) |> \n    pivot_longer(cols = c(value, fitted_value)) |> \n    mutate(name = factor(name, levels = c(\"value\", \"fitted_value\"))),\n  mapping = aes(x = date, y = value, linetype = name)) +\n  geom_line() +\n  labs(x = NULL, y = \"Cases\") +\n  scale_y_continuous(labels = comma) +\n  scale_linetype_discrete(\n    NULL, \n    labels = c(\"value\" = \"Observed values\",\n               \"fitted_value\" = \"Fitted values\")) +\n  theme(\n    legend.position = \"bottom\",\n    plot.title.position = \"plot\"\n  )\n\n\n\n\nFigure 5.1: Number of cases, Exponential model\n\n\n\n\nWe can compute the predicted values up to a given horizon. Let us assume that we want to make predictions up to the 80th day.\n\nhorizon_pred <- 80\nobs <- df_country$y\n\ntype_obs <- rep(\"obs\", length(obs))\nif (length(obs) < horizon_pred) {\n  obs <- c(obs, rep(NA, horizon_pred-length(obs)))\n  type_obs <- c(\n    type_obs,\n    rep(\"out_of_sample\", horizon_pred-length(type_obs))\n  )\n}\n\nlength(obs)\n\n[1] 80\n\ntable(type_obs)\n\ntype_obs\n          obs out_of_sample \n           60            20 \n\n\nLet us keep track on the corresponding dates.\n\ndates <- df_country$date\nif (length(dates) < horizon_pred) {\n  dates <- dates[1] + lubridate::ddays(seq_len(horizon_pred) - 1)\n}\ntail(dates)\n\n[1] \"2020-04-14\" \"2020-04-15\" \"2020-04-16\" \"2020-04-17\" \"2020-04-18\"\n[6] \"2020-04-19\"\n\n\nThe predictions can be made, using the estimated parameters, and stored in a tibble.\n\nfitted_val_expo_uk_80 <- tibble(\n  country  = country_name,\n  index    = seq_len(horizon_pred) - 1,\n  value    = obs,\n  type_obs = type_obs,\n  date     = dates\n) |> \n  mutate(\n    model_type   = \"Exponential\",\n    fitted_value = model_function(theta = params$coef_estimate, x = index)\n  )\nfitted_val_expo_uk_80\n\n# A tibble: 80 × 7\n   country        index value type_obs date       model_type  fitted_value\n   <chr>          <dbl> <int> <chr>    <date>     <chr>              <dbl>\n 1 United Kingdom     0     2 obs      2020-01-31 Exponential         4.54\n 2 United Kingdom     1     2 obs      2020-02-01 Exponential         5.29\n 3 United Kingdom     2     2 obs      2020-02-02 Exponential         6.16\n 4 United Kingdom     3     8 obs      2020-02-03 Exponential         7.17\n 5 United Kingdom     4     8 obs      2020-02-04 Exponential         8.34\n 6 United Kingdom     5     9 obs      2020-02-05 Exponential         9.71\n 7 United Kingdom     6     9 obs      2020-02-06 Exponential        11.3 \n 8 United Kingdom     7     9 obs      2020-02-07 Exponential        13.2 \n 9 United Kingdom     8    13 obs      2020-02-08 Exponential        15.3 \n10 United Kingdom     9    14 obs      2020-02-09 Exponential        17.8 \n# ℹ 70 more rows\n\n\nWe can plot those values:\n\nggplot(\n  data = fitted_val_expo_uk_80 |> \n    pivot_longer(cols = c(value, fitted_value)),\n  mapping = aes(x = date, y = value, linetype = name, colour = type_obs)) +\n  geom_line() +\n  labs(x = NULL, y = \"Cases\") +\n  scale_y_continuous(labels = comma) +\n  scale_linetype_discrete(\n    NULL, \n    labels = c(\"value\" = \"Observed values\",\n               \"fitted_value\" = \"Fitted values\")) +\n  scale_colour_manual(\n    NULL,\n    values = c(\n      \"obs\" = \"#44AA99\",\n      \"out_of_sample\" = \"#AA4499\"\n    ),\n    labels = c(\n      \"obs\" = \"Observed\",\n      \"out_of_sample\" = \"Out-of-sample\"\n    )\n  ) +\n  theme(\n    legend.position = \"bottom\",\n    plot.title.position = \"plot\"\n  )\n\nWarning: Removed 20 rows containing missing values (`geom_line()`).\n\n\n\n\n\nFigure 5.2: Number of cases, Exponential model, out-of-sample predictions\n\n\n\n\n\n\n5.4.2 Generalized exponential model\nNow let us turn to the generalized exponential model.\n\nmodel_function <- gen_exponential_f\n\nAgain, we rely on the nls.lm() function from package {minpack.lm}.\nHere are some starting values for the optimization algorithm:\n\n# The starting values\nstart <- list(\n  A = 1,\n  r  = .14,\n  alpha = .99\n)\n\nWe need to change the prediction function that will be passed on to the loss_function() that will be minimized:\nThe nls.lm() function can then be used.\n\n# The estimated coefficients\nout <- nls.lm(\n  par = start, \n  fn = loss_function,\n  y = df_country$y,\n  t = df_country$t,\n  fun = model_function,\n  control = nls.lm.control(maxiter = 100),\n  jac = NULL,\n  lower = NULL,\n  upper = NULL\n)\n\nThe results can be summarized as follows:\n\nsummary(out)\n\n\nParameters:\n       Estimate Std. Error t value Pr(>|t|)    \nA     1.469e-07  9.854e-02    0.00        1    \nr     5.011e-01  1.431e-02   35.01   <2e-16 ***\nalpha 8.751e-01  2.981e-03  293.51   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 302.5 on 57 degrees of freedom\nNumber of iterations to termination: 38 \nReason for termination: Relative error between `par' and the solution is at most `ptol'. \n\n\nThe estimated coefficients can be extracted and saved in a tibble:\n\nparams <- tibble(\n  model_type = \"Gen_Exponential\",\n  country = country_name,\n  coef_estimate_name = names(coef(out)),\n  coef_estimate = coef(out)\n)\nparams\n\n# A tibble: 3 × 4\n  model_type      country        coef_estimate_name coef_estimate\n  <chr>           <chr>          <chr>                      <dbl>\n1 Gen_Exponential United Kingdom A                    0.000000147\n2 Gen_Exponential United Kingdom r                    0.501      \n3 Gen_Exponential United Kingdom alpha                0.875      \n\n\nThe goodness of fit criterion can be computed using the get_criteria() function previously defined.\n\ncrit <- get_criteria(\n  f = model_function,\n  data = df_country,\n  theta = params$coef_estimate\n)\ncrit\n\n     AIC      BIC     RMSE \n860.6478 869.0251 294.8479 \n\n\nAnd they can be stored in a tibble:\n\ncriteria <- tibble(\n  model_type = \"Gen_Exponential\",\n  country = country_name,\n  bind_rows(crit)\n)\ncriteria\n\n# A tibble: 1 × 5\n  model_type      country          AIC   BIC  RMSE\n  <chr>           <chr>          <dbl> <dbl> <dbl>\n1 Gen_Exponential United Kingdom  861.  869.  295.\n\n\nThe \\(\\mathcal{R}_0\\) can be estimated with the sim_ec() function. But the generalized exponential does not provide good estimates for the reproduction number.\n\nR0_i <- sim_ec(\n  out = out,\n  n_repl = 1000,\n  ti = h,\n  h = h,\n  model_name = \"Gen_Exponential\"\n)\nR0_i\n\n    R0_mu     R0_sd \n13.785842  2.167253 \n\n\nThey can also be saved in a tibble:\n\nR0_df <- tibble(\n  model_type = \"Gen_Exponential\",\n  country = country_name,\n  bind_rows(R0_i)\n)\nR0_df\n\n# A tibble: 1 × 4\n  model_type      country        R0_mu R0_sd\n  <chr>           <chr>          <dbl> <dbl>\n1 Gen_Exponential United Kingdom  13.8  2.17\n\n\nWe can compute the predicted values and store those in a tibble:\n\nfitted_val_genexpo_uk <- \n  df_country |> \n  mutate(index = row_number()-1) |> \n  mutate(\n    model_type   = \"Gen_Exponential\",\n    fitted_value = model_function(theta = params$coef_estimate, x = index)\n  )\nfitted_val_genexpo_uk\n\n# A tibble: 60 × 11\n   country  country_code date       value stringency_index days_since_2020_01_22\n   <chr>    <chr>        <date>     <int>            <dbl>                 <dbl>\n 1 United … GBR          2020-01-31     2             8.33                     9\n 2 United … GBR          2020-02-01     2             8.33                    10\n 3 United … GBR          2020-02-02     2            11.1                     11\n 4 United … GBR          2020-02-03     8            11.1                     12\n 5 United … GBR          2020-02-04     8            11.1                     13\n 6 United … GBR          2020-02-05     9            11.1                     14\n 7 United … GBR          2020-02-06     9            11.1                     15\n 8 United … GBR          2020-02-07     9            11.1                     16\n 9 United … GBR          2020-02-08    13            11.1                     17\n10 United … GBR          2020-02-09    14            11.1                     18\n# ℹ 50 more rows\n# ℹ 5 more variables: t <dbl>, y <int>, index <dbl>, model_type <chr>,\n#   fitted_value <dbl>\n\n\n\nggplot(\n  data = fitted_val_genexpo_uk |> \n    select(date, value, fitted_value) |> \n    pivot_longer(cols = c(value, fitted_value)) |> \n    mutate(name = factor(name, levels = c(\"value\", \"fitted_value\"))),\n  mapping = aes(x = date, y = value, linetype = name)) +\n  geom_line() +\n  labs(x = NULL, y = \"Cases\") +\n  scale_y_continuous(labels = comma) +\n  scale_linetype_discrete(\n    NULL,\n    labels = c(\"fitted_value\" = \"Fitted values\",\n               \"value\" = \"Observed values\")\n  ) +\n  theme(\n    legend.position = \"bottom\",\n    plot.title.position = \"plot\"\n  )\n\n\n\n\nFigure 5.3: Number of cases, Generalized Exponential Model\n\n\n\n\nWe can compute the predicted values up to a given horizon. Let us assume that we want to make predictions up to the 80th day.\n\nhorizon_pred <- 80\nobs <- df_country$y\n\ntype_obs <- rep(\"obs\", length(obs))\nif (length(obs) < horizon_pred) {\n  obs <- c(obs, rep(NA, horizon_pred-length(obs)))\n  type_obs <- c(\n    type_obs,\n    rep(\"out_of_sample\", horizon_pred-length(type_obs))\n  )\n}\n\nlength(obs)\n\n[1] 80\n\ntable(type_obs)\n\ntype_obs\n          obs out_of_sample \n           60            20 \n\n\nLet us keep track on the corresponding dates.\n\ndates <- df_country$date\nif (length(dates) < horizon_pred) {\n  dates <- dates[1] + lubridate::ddays(seq_len(horizon_pred) - 1)\n}\ntail(dates)\n\n[1] \"2020-04-14\" \"2020-04-15\" \"2020-04-16\" \"2020-04-17\" \"2020-04-18\"\n[6] \"2020-04-19\"\n\n\nThe predictions can be made, using the estimated parameters, and stored in a tibble.\n\nfitted_val_genexpo_uk_80 <- tibble(\n  country  = country_name,\n  index    = seq_len(horizon_pred) - 1,\n  value    = obs,\n  type_obs = type_obs,\n  date     = dates\n) |> \n  mutate(\n    model_type   = \"Gen_Exponential\",\n    fitted_value = model_function(theta = params$coef_estimate, x = index)\n  )\nfitted_val_genexpo_uk_80\n\n# A tibble: 80 × 7\n   country        index value type_obs date       model_type      fitted_value\n   <chr>          <dbl> <int> <chr>    <date>     <chr>                  <dbl>\n 1 United Kingdom     0     2 obs      2020-01-31 Gen_Exponential     1.97e-55\n 2 United Kingdom     1     2 obs      2020-02-01 Gen_Exponential     2.32e-10\n 3 United Kingdom     2     2 obs      2020-02-02 Gen_Exponential     5.95e- 8\n 4 United Kingdom     3     8 obs      2020-02-03 Gen_Exponential     1.53e- 6\n 5 United Kingdom     4     8 obs      2020-02-04 Gen_Exponential     1.53e- 5\n 6 United Kingdom     5     9 obs      2020-02-05 Gen_Exponential     9.13e- 5\n 7 United Kingdom     6     9 obs      2020-02-06 Gen_Exponential     3.93e- 4\n 8 United Kingdom     7     9 obs      2020-02-07 Gen_Exponential     1.35e- 3\n 9 United Kingdom     8    13 obs      2020-02-08 Gen_Exponential     3.93e- 3\n10 United Kingdom     9    14 obs      2020-02-09 Gen_Exponential     1.01e- 2\n# ℹ 70 more rows\n\n\nWe can plot those values:\n\nggplot(\n  data = fitted_val_genexpo_uk_80 |> \n    pivot_longer(cols = c(value, fitted_value)),\n  mapping = aes(x = date, y = value, linetype = name, colour = type_obs)) +\n  geom_line() +\n  labs(x = NULL, y = \"Cases\") +\n  scale_y_continuous(labels = comma) +\n  scale_linetype_discrete(\n    NULL, \n    labels = c(\"value\" = \"Observed values\",\n               \"fitted_value\" = \"Fitted values\")) +\n  scale_colour_manual(\n    NULL,\n    values = c(\n      \"obs\" = \"#44AA99\",\n      \"out_of_sample\" = \"#AA4499\"\n    ),\n    labels = c(\n      \"obs\" = \"Observed\",\n      \"out_of_sample\" = \"Out-of-sample\"\n    )\n  ) +\n  theme(\n    legend.position = \"bottom\",\n    plot.title.position = \"plot\"\n  )\n\nWarning: Removed 20 rows containing missing values (`geom_line()`).\n\n\n\n\n\nFigure 5.4: Number of cases, Generalized Exponential model, out-of-sample predictions\n\n\n\n\n\n\n\n\nCori, Anne, Neil M. Ferguson, Christophe Fraser, and Simon Cauchemez. 2013. “A New Framework and Software to Estimate Time-Varying Reproduction Numbers During Epidemics.” American Journal of Epidemiology 178 (9): 1505–12. https://doi.org/10.1093/aje/kwt133.\n\n\nLi, Qun, Xuhua Guan, Peng Wu, Xiaoye Wang, Lei Zhou, Yeqing Tong, Ruiqi Ren, et al. 2020. “Early Transmission Dynamics in Wuhan, China, of Novel Coronavirus-Infected Pneumonia.” New England Journal of Medicine 382 (13): 1199–1207. https://doi.org/10.1056/NEJMoa2001316."
  },
  {
    "objectID": "phenomeological-background.html#simple-phenomenological-models",
    "href": "phenomeological-background.html#simple-phenomenological-models",
    "title": "6  Background on Phenomenological Models",
    "section": "6.1 Simple phenomenological models",
    "text": "6.1 Simple phenomenological models\nWe will present three models to estimate the number of cases and the number of deaths: the logistic model, the Gompertz model, and the Richards model.\n\n6.1.1 A Generic Equation\nWang, Wu, and Yang (2012) develop the similarity between the SIR model and the Richards population model Richards (1959) which lead them to consider the following growth equation for confirmed cases noted \\(C(t)\\): \\[\n\\frac{d\\, C}{d\\, t} = r C^\\alpha\\left[1-\\left(\\frac{C}{K}\\right)^\\delta\\right],\n\\tag{6.1}\\] with \\(r\\), \\(\\alpha\\), \\(\\delta\\) and \\(K\\) being positive real numbers with the further restriction \\(0\\leq\\alpha\\leq 1\\).\nA general solution to this equation has the form: \\[\nC(t) = F(r,\\alpha,\\delta,K,t),\n\\] with the property that \\(\\lim_{t \\rightarrow\\infty} C(t) = K\\). If \\(C(t)\\) corresponds to the total number of cases, the number of new cases is found by computing the derivative of \\(C(t)\\) with respect to \\(t\\) and noted \\(c(t)\\). The relative speed of the process is defined as \\(c(t)/C(t)\\) and is constant over time only when \\(\\alpha=1\\), \\(\\delta=0\\). The doubling time is constant over time only under those restrictions.\nA crucial question is to characterize the speed at which the process will reach its peak and when. Tsoularis and Wallace (2002) have shown that the value of the peak is given by the value of \\(C\\) at the inflexion point of the curve: \\[\nC_{inf} = K \\left(1+\\frac{\\delta}{\\alpha}\\right)^{-1/\\delta}.\n\\tag{6.2}\\] The corresponding time, that we shall note \\(\\tau\\) is obtained by inverting \\(C(t)\\). For some models, when an analytical expression for \\(C(t)\\) is available, \\(\\tau\\) can be directly included in the parameterization. This point is of particular importance because it corresponds to the period when the epidemic starts to regress, or equivalently when the effective reproduction number \\(R_t\\) starts to be below 1.\n\n\n6.1.2 Logistic Model\nThe logistic model initiated by Verhulst (1845) provides the most simple solution to the Equation 6.1 and corresponds to \\(\\alpha=\\delta=1\\): \\[\nC(t) = \\frac{K}{1+\\exp\\left(-r (t - \\tau)\\right)}.\n\\tag{6.3}\\]\nIt has been recently applied to study the evolution of an epidemic Ma (2020). We have introduced a parameterization where \\(\\tau\\) directly represents the inflection point with that \\(C(\\tau) = K/2\\). So the peak is at the mid of the epidemic which reaches its cumulated maximum \\(K\\), \\(r\\) representing the growth rate. The first order derivative of this function provides an estimate of the number of cases at each point in time: \\[\n\\frac{\\partial C(t)}{\\partial t} = \\frac{K r \\exp\\left(-r(t-\\tau)\\right)}{\\left[1+\\exp(-r(t-\\tau)\\right]^2}.\n\\] The relative speed is thus: \\[\n\\frac{c(t)}{C(t)} = r \\frac{e^{-r(t-\\tau)}}{1+e^{-r(t-\\tau)}}.\n\\] This model might appear as a nice solution to represents the three phases of an epidemic, but we shall discover below that its symmetry tends to be in contradiction with epidemic data. So we have to explore alternative solutions.\nIn R, we define the logistic function as follows:\n\n#' Logistic function\n#' \n#' @param theta vector of named parameters\n#' @param x time\nlogistic_f <-  function(theta, x) {\n  k <- theta[[\"k\"]]\n  tau <- theta[[\"tau\"]]\n  r <- theta[[\"r\"]]\n  k / ( 1+exp( -r*( x - tau ) ) )\n}\n\n#' First derivative of the logistic function\n#' \n#' @param theta vector of named parameters\n#' @param x time\nlogistic_f_first_d <- function(theta, x) {\n  k <- theta[[\"k\"]]\n  tau <- theta[[\"tau\"]]\n  r <- theta[[\"r\"]]\n  \n  (k * r * exp( -r * (x - tau) )) / (1+ exp( -r * (x - tau) ))^2\n}\n\n#' Second derivative of the logistic function\n#' \n#' @param theta vector of named parameters\n#' @param x time\nlogistic_f_second_d <- function(theta, x) {\n  k <- theta[[\"k\"]]\n  tau <- theta[[\"tau\"]]\n  r <- theta[[\"r\"]]\n  \n  k*((2*r^2 * exp(-2*r*(x - tau)))/\n       (exp(-r*(x - tau)) + 1)^3 - \n       (r^2*exp(-r*(x - tau)))/(exp(-r*(x - tau)) + 1)^2)\n}\n\nNow, let us make some graph to give an idea of how the dynamic changes with the parameters \\(r\\) and \\(tau\\).\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\ng_legend<-function(a.gplot){\n  tmp <- ggplot_gtable(ggplot_build(a.gplot))\n  leg <- which(sapply(tmp$grobs, function(x) x$name) == \"guide-box\")\n  legend <- tmp$grobs[[leg]]\n  return(legend)}\n\nThe different values for \\(r\\) and \\(tau\\) (\\(k\\) is set to 5000).\n\nsituations <- \n  expand_grid(tau = c(25,35), r = c(.15, .25, .35)) |> \n  mutate(k = 5000)\n\nLet us apply, for a set of parameters, at different time horizon (from 0 to 60 by steps of .1) the logistic function, its first and second derivatives with respect to time. To do so, we define a “simulation” function:\n\n#' Simulation of the logistic model for some parameters\n#' \n#' @param i row number of situations\nsimu <- function(i) {\n  current_params <- situations |> slice(i)\n  current_params <- c(as_vector(current_params)) |> as.list()\n  \n  n     <- 60\n  step  <- .01\n  sim   <-  logistic_f(\n    theta = current_params, \n    x = seq(0,n, by = step)\n  )\n  simD  <-  logistic_f_first_d(\n    theta = current_params, \n    x = seq(0,n, by = step)\n  )\n  simD2 <-  logistic_f_second_d(\n    theta = current_params, \n    x = seq(0,n, by = step)\n  )\n  \n  tibble(\n    t = seq(0, n, by = step),\n    k = current_params[[\"k\"]], \n    tau = current_params[[\"tau\"]],\n    r = current_params[[\"r\"]],\n    sim = sim\n  ) |> \n    mutate(\n      simD = simD,\n      simD2 = simD2\n    )\n}\n\nLet us do so for all the different sets of parameters:\n\nsimu_res <- map_df(1:nrow(situations), simu)\n\nWe would like to show the thresholds on the graph:\n\nthreshold_times <- \n  simu_res |> \n  group_by(k, tau, r) |> \n  summarise(threshold_time = unique(tau)) |> \n  ungroup() |> \n  mutate(\n    r = str_c(\"$\\\\r = \", r, \"$\"),\n    tau = str_c(\"$\\\\tau = \", tau, \"$\")\n  )\n\n`summarise()` has grouped output by 'k', 'tau'. You can override using the\n`.groups` argument.\n\nthreshold_times\n\n# A tibble: 6 × 4\n      k tau            r              threshold_time\n  <dbl> <chr>          <chr>                   <dbl>\n1  5000 \"$\\\\tau = 25$\" \"$\\\\r = 0.15$\"             25\n2  5000 \"$\\\\tau = 25$\" \"$\\\\r = 0.25$\"             25\n3  5000 \"$\\\\tau = 25$\" \"$\\\\r = 0.35$\"             25\n4  5000 \"$\\\\tau = 35$\" \"$\\\\r = 0.15$\"             35\n5  5000 \"$\\\\tau = 35$\" \"$\\\\r = 0.25$\"             35\n6  5000 \"$\\\\tau = 35$\" \"$\\\\r = 0.35$\"             35\n\n\nNow, we are ready to create the plots:\n\np_simu_logis <- \n  ggplot(\n    data =  simu_res |> \n      mutate(\n        r = str_c(\"$\\\\r = \", r, \"$\"),\n        tau = str_c(\"$\\\\tau = \", tau, \"$\")\n      ),\n    mapping = aes(x = t)\n  ) +\n  geom_line(mapping = aes(y = sim, linetype = \"sim\")) +\n  geom_line(mapping = aes(y = simD*10, linetype = \"simD\")) +\n  geom_line(mapping = aes(y = simD2*10, linetype = \"simD2\")) +\n  geom_vline(\n    data = threshold_times, \n    mapping = aes(xintercept = threshold_time),\n    colour = \"red\", linetype = \"dotted\") +\n  facet_grid(\n    r~tau, \n    labeller = as_labeller(\n      latex2exp::TeX, \n      default = label_parsed\n    )\n  ) +\n  labs(x = \"Time\", y = latex2exp::TeX(\"$F(t)$\")) +\n  scale_y_continuous(\n    labels = scales::comma,\n    sec.axis = sec_axis(\n      ~./10, \n      name = latex2exp::TeX(\n        \"$\\\\partial F(t) / \\\\partial t$, $\\\\partial^2 F(t) / \\\\partial t^2$\"\n      ),\n      labels = comma)) +\n  scale_linetype_manual(\n    NULL,\n    values = c(\"sim\" = \"solid\", \"simD\" = \"dashed\", \"simD2\" = \"dotdash\"),\n    labels = c(\n      \"sim\" = latex2exp::TeX(\"$F(t)$\"), \n      \"simD\" = latex2exp::TeX(\"$\\\\partial F(t) / \\\\partial t$\"),\n      \"simD2\" = latex2exp::TeX(\"$\\\\partial^2 F(t) / \\\\partial t^2$\"))) +\n  theme(axis.ticks.y = element_blank())\n\np_simu_logis +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 6.1: Illustration of the logistic model for different values for \\(\\tau\\) and \\(r\\), with \\(k=5,000\\)\n\n\n\n\n\n6.1.2.1 Key moments\nLet us focus on the key moment of the epidemics. Let us illustrate them using one of the scenarios.\n\ndata_sim <- simu(1)\n\nThe starting point:\n\ndf_starting <- \n  data_sim %>% \n  filter(sim >= 1) %>% \n  slice(1)\n\nThe acceleration point\n\ndf_acceleration <- \n  data_sim %>% \n  filter(simD2 == max(simD2)) %>% \n  slice(1)\n\nThe peak:\n\ndf_peak <- \n  data_sim %>% \n  filter(simD == max(simD)) %>% \n  slice(1)\n\nThe deceleration point:\n\ndf_deceleration <- \n  data_sim %>% \n  filter(simD2 == min(simD2)) %>% \n  slice(1)\n\nThe return point:\n\ndf_return <- \n  data_sim %>% \n  filter(sim > k - 1) %>% \n  slice(1)\n\nLet us reshape the data:\n\ndf_plot_key_moments <- \n  data_sim %>% \n  gather(key, value, sim, simD, simD2)\n\ndf_plot_key_moments_points <- \n  df_acceleration %>% mutate(label = \"$t_A$\") %>% \n  bind_rows(df_peak %>% mutate(label = \"$t_P$\")) %>% \n  bind_rows(df_deceleration %>% mutate(label = \"$t_D$\"))\n\nAnd the plot:\n\np_key_moments <- \n  ggplot() +\n  geom_hline(yintercept = 0, col = \"grey\") +\n  geom_line(\n    data = df_plot_key_moments,\n    mapping = aes(x = t, y = value, linetype = key)\n  ) +\n  geom_segment(\n    data = df_plot_key_moments_points,\n    mapping = aes(x = t, xend = t, y = 0, yend = sim),\n    linetype = \"dotted\", colour = \"red\"\n  ) +\n  geom_point(\n    data = df_plot_key_moments_points,\n    mapping = aes(x = t, y = sim), size = 2\n  ) +\n  geom_point(\n    data = df_plot_key_moments_points,\n    mapping = aes(x = t, y = sim),\n    size = 1, colour = \"white\"\n    ) +\n  geom_text(\n    data = df_plot_key_moments_points,\n    mapping = aes(x = t-2, y = sim + .05*k, label = label)\n  ) +\n  labs(x = \"Time\", y = latex2exp::TeX(\"$F(t)$\")) +\n  scale_linetype_manual(\n    NULL,\n    values = c(\"sim\" = \"solid\", \"simD\" = \"dashed\", \"simD2\" = \"dotdash\"),\n    labels = c(\n      \"sim\" = latex2exp::TeX(\"$F(t)$\"),\n      \"simD\" = latex2exp::TeX(\"$\\\\partial F(t) / \\\\partial t$\"),\n      \"simD2\" = latex2exp::TeX(\"$\\\\partial^2 F(t) / \\\\partial t^2$\")\n    )\n  ) +\n  theme(axis.text = element_blank(), axis.ticks = element_blank())\n\np_key_moments\n\n\n\n\nFigure 6.2: Key moments of the epidemics, using a logistim model.\n\n\n\n\n\n\n\n6.1.3 Gompertz Model\nThe Gompertz model (Gompertz 1825) has three parameters, but corresponds to alternative restrictions with \\(\\alpha=1\\) but this time \\(\\delta=0\\). The solution to Equation 6.1 can be written as follows Tjørve and Tjørve (2017): \\[\nC(t) = K \\exp\\left[ -\\exp \\left(-r(t-\\tau) \\right)\\right].\n\\tag{6.4}\\] In Equation 6.4, the parameters are interpreted in the same way as those of the logistic model. The main difference between the logistic and Gompertz models is that the latter is not symmetric around the inflection point, which now appears earlier as \\(C(\\tau) = K/e\\). The first order derivative of this function provides an estimate of the number of cases at each point in time: \\[\n\\frac{\\partial C(t)}{\\partial t} = K r\\exp\\left[r(\\tau-t) - \\exp\\left(r(\\tau-t)\\right)\\right].\n\\] So that the relative speed of the epidemic is given by: \\[\n\\frac{c(t)}{C(t)} = r e^{-r(t-\\tau)},\n\\] with of course a maximum speed at the peak.\nIn R, this translates to:\n\n#' Gompertz function with three parameters\n#' \n#' @param theta vector of named parameters\n#' @param x time\ngompertz_f <- function(theta, x) {\n  k <- theta[[\"k\"]]\n  tau <- theta[[\"tau\"]]\n  r <- theta[[\"r\"]]\n  k*exp( -exp( -r * (x - tau) ) )\n}\n#' First order derivative of Gompertz wrt x\n#' \n#' @param theta vector of named parameters\n#' @param x time\ngompertz_f_first_d <- function(theta, x) {\n  k <- theta[[\"k\"]]\n  tau <- theta[[\"tau\"]]\n  r <- theta[[\"r\"]]\n  \n  k * (x - tau) * exp(r * (tau - x) - exp(r * (tau - x)))\n}\n\n#' Second order derivative of Gompertz\n#' \n#' @param theta vector of named parameters\n#' @param x time\ngompertz_f_second_d <- function(theta, x) {\n  k <- theta[[\"k\"]]\n  tau <- theta[[\"tau\"]]\n  r <- theta[[\"r\"]]\n  \n  -k * r^2 * exp(-r * (x - tau)) * \n    exp(-exp(-r * (x - tau))) + \n    k * r^2 * (exp(-r * (x - tau)))^2 * exp(-exp(-r * (x - tau)))\n}\n\nLet us make some graph to give an idea of how the dynamic changes with the parameters \\(r\\) and \\(tau\\).\n\nk <- 5000\nsituations <- \n  expand_grid(tau = c(25,35), r = c(.15, .25, .35)) |> \n  mutate(k = k)\n\n#' Simulation of the logistic model for some parameters\n#' \n#' @param i row number of situations\nsimu_gomp <- function(i) {\n  current_params <- situations |> slice(i)\n  current_params <- c(as_vector(current_params)) |> as.list()\n  \n  n <- 60\n  step <- .01\n  sim <-  gompertz_f(theta = current_params, x = seq(1,n, by = step))\n  simD <-  gompertz_f_first_d(theta = current_params, x = seq(1,n, by = step))\n  simD2 <-  gompertz_f_second_d(theta = current_params, x = seq(1,n, by = step))\n  \n  tibble(\n    t = seq(1, n, by = step),\n    k = current_params[[\"k\"]],\n    tau = current_params[[\"tau\"]],\n    r = current_params[[\"r\"]],\n    sim = sim) |> \n    mutate(simD = simD,\n           simD2 = simD2)\n  \n}\n\nThe values of \\(C(t)\\), its first and second derivatives with respect to time can be computed for the different scenarios:\n\nsimu_gomp_res <- map_df(1:nrow(situations), simu_gomp)\n\nThe threshold for each scenario:\n\nthreshold_times <-\n  simu_gomp_res |>\n  group_by(k, tau, r) |>\n  summarise(threshold_time = unique(tau)) |>\n  ungroup() |> \n  mutate(\n    r = str_c(\"$\\\\r = \", r, \"$\"),\n    tau = str_c(\"$\\\\tau = \", tau, \"$\")\n  )\n\n`summarise()` has grouped output by 'k', 'tau'. You can override using the\n`.groups` argument.\n\n\nAnd we can plot the results:\n\np_simu_gomp <- \n  ggplot(\n  data = simu_gomp_res |> \n    mutate(\n      r = str_c(\"$\\\\r = \", r, \"$\"),\n      tau = str_c(\"$\\\\tau = \", tau, \"$\")\n    ),\n  mapping = aes(x = t)) +\n  geom_line(mapping = aes(y = sim, linetype = \"sim\")) +\n  geom_line(mapping = aes(y = simD, linetype = \"simD\")) +\n  geom_line(mapping = aes(y = simD2, linetype = \"simD2\")) +\n  facet_grid(\n    r~tau,\n    labeller = as_labeller(\n      latex2exp::TeX, \n      default = label_parsed\n    )\n  ) +\n  geom_vline(\n    data = threshold_times, \n    mapping = aes(xintercept = threshold_time), \n    colour = \"red\", linetype = \"dotted\") +\n  labs(x = \"Time\", y = latex2exp::TeX(\"$F(t)$\")) +\n  scale_linetype_manual(\n    NULL,\n    values = c(\"sim\" = \"solid\", \"simD\" = \"dashed\", \"simD2\" = \"dotdash\"),\n    labels = c(\"sim\" = latex2exp::TeX(\"$F(t)$\"),\n               \"simD\" = latex2exp::TeX(\"$\\\\partial F(t) / \\\\partial t$\"),\n               \"simD2\" = latex2exp::TeX(\"$\\\\partial^2 F(t) / \\\\partial t^2$\")\n    )\n  ) +\n  theme(axis.ticks.y = element_blank())\n\np_simu_gomp +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 6.3: Illustration of Gompertz model for different values for \\(\\tau\\) and \\(r\\), with \\(k=5,000\\)\n\n\n\n\n\n\n6.1.4 Richards Model\nThe question “what happens with \\(0<\\delta<1\\)” finds an answer with Richards (1959) and the Richards model which is widely used in nonlinear regression analysis. The solution to Equation 6.1 provided in Lee, Lei, and Mallick (2020) corresponds to: \\[\nC(t) = K\\left[ 1 + \\delta\\exp\\left( -r (t-\\tau)\\right)\\right]^{-1/\\delta},\n\\] where parameters \\(K\\) and \\(\\tau\\) and are interpreted in the same way as those of the logistic and Gompertz models, while the growth rate is now \\(r/\\delta\\). The parameter \\(\\delta>0\\) is used to control the value of the curve at the inflection point \\(C(\\tau) = K/(1+\\delta)^{1/\\delta}\\), and thus the asymmetry around \\(t = \\tau\\). This model considers one more parameter than the Gompertz model to model this asymmetry. The first order derivative of this function will provide an estimate of the number of cases at each point in time: \\[\n\\frac{\\partial C(t)}{\\partial t} =  \\frac{K r\\left[\\delta\\exp\\left(r(\\tau-t)\\right)+1 \\right]^{-1/\\delta}}{\\delta+\\exp\\left(r(t-\\tau)\\right)},\n\\] leading to a relative speed of: \\[\n\\frac{c(t)}{C(t)} = r \\frac{e^{-r(t-\\tau)}}{1+\\delta e^{-r(t-\\tau)}}.\n\\] This model is also a generalization of the two previous ones as can be seen for instance when comparing the different relative speeds.\nIn R, this translates to the following functions:\n\n#' Richards function with four parameters\n#' \n#' @param theta vector of named parameters\n#' @param x time\nrichards_f <- function(theta, x) {\n  k <- theta[[\"k\"]]\n  tau <- theta[[\"tau\"]]\n  r <- theta[[\"r\"]]\n  delta <- theta[[\"delta\"]]\n  \n  k / (1 + delta * exp(-r * delta * (x - tau)))^(1 / delta)\n}\n\n\n#' First order derivative of Richards function wrt time (x)\n#' \n#' @param theta vector of named parameters\n#' @param x time\nrichards_f_first_d <- function(theta, x) {\n  k <- theta[[\"k\"]]\n  tau <- theta[[\"tau\"]]\n  r <- theta[[\"r\"]]\n  delta <- theta[[\"delta\"]]\n  \n  delta * k * r * exp(delta * (-r) * (x - tau)) * \n    (delta * exp(delta * (-r) * (x - tau)) + 1)^(-1 / delta - 1)\n}\n\n\n#' Second order derivative of Richards function wrt time (x)\n#' \n#' @param theta vector of named parameters\n#' @param x time\nrichards_f_second_d <- function(theta, x) {\n  k <- theta[[\"k\"]]\n  tau <- theta[[\"tau\"]]\n  r <- theta[[\"r\"]]\n  delta <- theta[[\"delta\"]]\n  \n  k * ((-1 / delta - 1) * \n         delta^3 * r^2 * (-exp(-2 * delta * r * (x - tau))) *\n         (delta * exp(delta * (-r) * (x - tau)) + 1)^(-1 / delta - 2) - \n         delta^2 * r^2 * exp(delta * (-r) * (x - tau)) * \n         (delta * exp(delta * (-r) * (x - tau)) + 1)^(-1 / delta - 1))\n}\n\nAgain, let us provide different scenarios to plot the curves for \\(C(t)\\), its first and second derivatives with respect to time. We will make the parameters \\(\\delta\\) and \\(r\\) vary. We set \\(k\\) to \\(5,000\\) and \\(\\tau\\) to 25.\n\nsituations <- \n  expand_grid(delta = c(.5, 1.5), r = c(.15, .25, .35)) |> \n  mutate(k = 5000, tau = 25)\n\nWe create a function to get the values of \\(C(t)\\) and its derivatives depending on the scenario.\n\n#' Simulation of Richards model for some parameters\n#' \n#' @param i row number of situations\nsimu_richards <- function(i) {\n  current_params <- situations |> slice(i)\n  current_params <- c(as_vector(current_params)) |> as.list()\n  \n  n <- 60\n  step <- .01\n  sim <-  richards_f(theta = current_params, x = seq(1,n, by = step))\n  simD <-  richards_f_first_d(theta = current_params, x = seq(1,n, by = step))\n  simD2 <-  richards_f_second_d(theta = current_params, x = seq(1,n, by = step))\n  \n  tibble(\n    t = seq(1, n, by = step),\n    k = current_params[[\"k\"]], \n    tau = current_params[[\"tau\"]],\n    r = current_params[[\"r\"]],\n    delta = current_params[[\"delta\"]],\n    sim = sim\n    ) |> \n    mutate(simD = simD,\n           simD2 = simD2)\n}\n\nThis function is then applied to the different scenarios:\n\nsimu_richards_res <- map_df(1:nrow(situations), simu_richards)\n\nThe thresholds:\n\nthreshold_times <-\n  simu_richards_res |>\n  group_by(k, tau, r, delta) |>\n  summarise(threshold_time = unique(tau)) |>\n  ungroup() |>\n  mutate(\n    r = str_c(\"$r = \", r, \"$\"),\n    delta = str_c(\"$\\\\delta = \", delta, \"$\")\n  )\n\n`summarise()` has grouped output by 'k', 'tau', 'r'. You can override using the\n`.groups` argument.\n\n\n\np_simu_richards <- \n  ggplot(\n    data = simu_richards_res |> \n      mutate(\n        r = str_c(\"$r = \", r, \"$\"),\n        delta = str_c(\"$\\\\delta = \", delta, \"$\")\n      ),\n    mapping = aes(x = t)\n  ) +\n  geom_line(mapping = aes(y = sim, linetype = \"sim\")) +\n  geom_line(mapping = aes(y = simD * 10, linetype = \"simD\")) +\n  geom_line(mapping = aes(y = simD2 * 10, linetype = \"simD2\")) +\n  facet_grid(\n    r ~ delta,\n    labeller = as_labeller(\n      latex2exp::TeX, \n      default = label_parsed\n    )\n  ) +\n  geom_vline(\n    data = threshold_times, \n    mapping = aes(xintercept = threshold_time),\n    colour = \"red\", linetype = \"dotted\") +\n  labs(x = \"Time\", y = latex2exp::TeX(\"$F(t)$\")) +\n  scale_y_continuous(\n    labels = comma,\n    breaks = seq(0, 5000, by = 1000),\n    sec.axis = sec_axis(\n      ~./10,\n      name = latex2exp::TeX(\n        \"$\\\\partial F(t)/\\\\partial t$, $\\\\partial^2 F(t)/\\\\partial t^2$\"\n      ),\n      labels = comma)\n  ) +\n  scale_linetype_manual(\n    NULL,\n    values = c(\"sim\" = \"solid\", \"simD\" = \"dashed\", \"simD2\" = \"dotdash\"),\n    labels = c(\"sim\" = latex2exp::TeX(\"$F(t)$\"),\n               \"simD\" = latex2exp::TeX(\"$\\\\partial F(t) / \\\\partial t$\"),\n               \"simD2\" = latex2exp::TeX(\"$\\\\partial^2 F(t) / \\\\partial t^2$\"))\n  ) +\n  theme(axis.ticks.y = element_blank())\n\np_simu_richards +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 6.4: Illustration of Richards’ model for different values for \\(\\delta\\) and \\(r\\), with \\(k=5,000\\) and \\(\\tau=25\\)"
  },
  {
    "objectID": "phenomeological-background.html#double-sigmoid-functions-to-account-for-a-second-wave",
    "href": "phenomeological-background.html#double-sigmoid-functions-to-account-for-a-second-wave",
    "title": "6  Background on Phenomenological Models",
    "section": "6.2 Double sigmoid functions to account for a second wave",
    "text": "6.2 Double sigmoid functions to account for a second wave\nOne possibility to account for two distinct phases is to use a double sigmoid. This is done by adding or multiplying two sigmoid functions, as follows: \\[\nC(t) = C_{1}^{(m_1)}(t) + C_{2}^{(m_2)}(t),\n\\] where \\(C_{1}^{(m_1)}(t)\\) is the first sigmoid function and \\(C_{2}^{(m_2)}(t)\\) is the second one. The types \\(m_1\\) and \\(m_2\\) of sigmoid can be, for example, a logistic curve (see, e.g., Lipovetsky (2010) or Bock et al. (1973)), a Gompertz curve Thissen et al. (1976), or a Richards curve Oswald et al. (2012).\nWe limit ourselves here to the presentation of a double-Gompertz curve: \\[\n\\begin{aligned}\nC(t)  = & K_1\\exp\\left[ -\\exp\\left( -r_1(t-\\tau_1)\\right)\\right] \\\\\n& + (K_2 - K_1)\\exp\\left[ -\\exp\\left( -r_2(t-\\tau_2)\\right)\\right],\n\\end{aligned}\n\\] where \\(K_1\\) and \\(K_2\\) are the intermediate and final plateau of saturation, respectively. The parameters \\(\\tau_1\\) and \\(\\tau_2\\) define the inflection points of each phase when the intermediate plateau is long enough while the growth of the process is determined by \\(r_1\\) and \\(r_2\\), for the first and second sigmoid. However there is no analytical formula to determine the value and position of the two peaks. We have to evaluate numerically the extremum of the second order derivative of \\(C(t)\\). The first order derivative of \\(C(t)\\) provides an estimate of the number of cases at each point of time: \\[\n\\begin{aligned}\n\\frac{\\partial C(t)}{\\partial t}  =  & K_1 r_1 \\exp(-r_1(t-\\tau_1) \\exp(-\\exp(-r1(t-\\tau1))) \\\\\n& + (K_2-K_1)r_2\\exp(-r_2(t-\\tau_2) \\exp(-\\exp(-r2(t-\\tau2))).\n\\end{aligned}\n\\] The formula giving the relative speed \\(c(t)/C(t)\\) does not lead to any simplification and will not be detailed.\n\n6.2.1 Double Logistic\nThe double logistic function, its first and second derivatives with respect to time can be coded as follows:\n\n# Double logistic\ndouble_logistic_f <-  function(theta, x) {\n  K1 <- theta[[\"K1\"]]\n  tau1 <- theta[[\"tau1\"]]\n  r1 <- theta[[\"r1\"]]\n  K2 <- theta[[\"K2\"]]\n  tau2 <- theta[[\"tau2\"]]\n  r2 <- theta[[\"r2\"]]\n  \n  (K1/(1 + exp(-r1 * (x - tau1)))) + ((K2 - K1)/(1 + exp(-r2 * (x - tau2))))\n}\n# Double logistic in two parts\ndouble_logistic_f_2 <- function(theta, x) {\n  K1 <- theta[[\"K1\"]]\n  tau1 <- theta[[\"tau1\"]]\n  r1 <- theta[[\"r1\"]]\n  K2 <- theta[[\"K2\"]]\n  tau2 <- theta[[\"tau2\"]]\n  r2 <- theta[[\"r2\"]]\n  \n  f_1 <- K1 / (1 + exp(-r1 * (x - tau1)))\n  f_2 <- (K2 - K1)/(1 + exp(-r2 * (x - tau2)))\n  tibble(x = x, f_1 = f_1, f_2 = f_2)\n}\n\n# First derivative\ndouble_logistic_f_first_d <-  function(theta, x) {\n  K1 <- theta[[\"K1\"]]\n  tau1 <- theta[[\"tau1\"]]\n  r1 <- theta[[\"r1\"]]\n  K2 <- theta[[\"K2\"]]\n  tau2 <- theta[[\"tau2\"]]\n  r2 <- theta[[\"r2\"]]\n  \n  (r1 * K1 * exp(-r1 * (x - tau1))) / ((exp(-r1 * (x-tau1)) + 1)^2) +\n    (r2 * (K2-K1) * exp(-r2 * (x - tau2))) / ((exp(-r2 * (x-tau2)) + 1)^2)\n}\n# Second derivative\ndouble_logistic_f_second_d <-  function(theta, x) {\n  K1 <- theta[[\"K1\"]]\n  tau1 <- theta[[\"tau1\"]]\n  r1 <- theta[[\"r1\"]]\n  K2 <- theta[[\"K2\"]]\n  tau2 <- theta[[\"tau2\"]]\n  r2 <- theta[[\"r2\"]]\n  \n  K1 *  ((2 * r1^2 * exp(-2 * r1 * (x - tau1)))/(exp(-r1 * (x - tau1)) + 1)^3 - \n           ( r1^2 * exp(- r1 * (x - tau1)))/(exp(-r1 * (x - tau1)) + 1)^2) +\n    \n    (K2-K1) *  \n    ((2 * r2^2 * exp(-2 * r2 * (x - tau2)))/(exp(-r2 * (x - tau2)) + 1)^3 - \n       ( r2^2 * exp(- r2 * (x - tau2)))/(exp(-r2 * (x - tau2)) + 1)^2)\n  \n}\n\nAnd now, let us make some illustrations by varying \\(r_1\\) and \\(r_2\\).\n\nsituations <- \n  expand_grid(r1 = c(.2, .5), r2 = c(.2, .5)) |> \n  mutate(\n    K1   = 2500,\n    K2   = 5000,\n    tau1 = 20,\n    tau2 = 40\n  )\n\nThe function that simulates \\(C(t)\\) and its first and second derivatives with respect to time for a single scenario:\n\n#' Simulation of the logistic model for some parameters\n#' \n#' @param i row number of situations\nsimu <- function(i) {\n  current_params <- situations |> slice(i)\n  current_params <- c(as_vector(current_params)) |> as.list()\n  \n  n <- 60\n  step <- .01\n  sim <-  double_logistic_f(theta = current_params, x = seq(1,n, by = step))\n  simD <-  double_logistic_f_first_d(\n    theta = current_params, x = seq(1,n, by = step))\n  simD2 <-  double_logistic_f_second_d(\n    theta = current_params, x = seq(1,n, by = step))\n  \n  tibble(\n    t = seq(1, n, by = step),\n    K1 = current_params[[\"K1\"]],\n    r1 = current_params[[\"r1\"]],\n    tau1 = current_params[[\"tau1\"]],\n    K2 = current_params[[\"K2\"]],\n    r2 = current_params[[\"r2\"]],\n    tau2 = current_params[[\"tau2\"]],\n    sim = sim) |> \n    mutate(simD = simD,\n           simD2 = simD2)\n  \n}\n\nThe simulated values for all the scenarios:\n\nsimu_res <- map_df(1:nrow(situations), simu)\n\nThe thresholds:\n\nthreshold_times <- \n  simu_res |> \n  group_by(K1, tau1, r1, K2, tau2, r2) |> \n  summarise(\n    threshold_time_1 = unique(tau1),\n    threshold_time_2 = unique(tau2)\n  ) |> \n  ungroup() |> \n  mutate(\n    r1 = str_c(\"$r_1 = \", r1, \"$\"),\n    r2 = str_c(\"$r_2 = \", r2, \"$\")\n  )\n\n`summarise()` has grouped output by 'K1', 'tau1', 'r1', 'K2', 'tau2'. You can\noverride using the `.groups` argument.\n\n\n\np_simu_double_logis <- \n  ggplot(\n    data = simu_res |> \n      mutate(\n        r1 = str_c(\"$r_1 = \", r1, \"$\"),\n        r2 = str_c(\"$r_2 = \", r2, \"$\")\n      ),\n    mapping = aes(x = t)\n  ) +\n  geom_line(aes(y = sim, linetype = \"sim\")) +\n  geom_line(aes(y = simD * 10, linetype = \"simD\")) +\n  geom_line(aes(y = simD2 * 10, linetype = \"simD2\")) +\n  geom_vline(\n    data = threshold_times,\n    mapping = aes(xintercept = threshold_time_1),\n    colour = \"red\", linetype = \"dotted\") +\n  geom_vline(\n    data = threshold_times,\n    mapping = aes(xintercept = threshold_time_2),\n    colour = \"red\", linetype = \"dotted\") +\n  facet_grid(\n    r2 ~ r1,\n    labeller = as_labeller(\n      latex2exp::TeX, \n      default = label_parsed\n    )\n  ) +\n  labs(x = \"Time\", y = \"$F(t)$\") +\n  scale_y_continuous(\n    labels = comma,\n    sec.axis = sec_axis(\n      ~./10,\n      name = latex2exp::TeX(\n        \"$\\\\partial F(t) / \\\\partial t$, $\\\\partial^2 F(t) / \\\\partial t^2$\"\n      ),\n      labels = comma)\n  ) +\n  scale_linetype_manual(\n    NULL,\n    values = c(\"sim\" = \"solid\", \"simD\" = \"dashed\", \"simD2\" = \"dotdash\"),\n    labels = c(\"sim\" = latex2exp::TeX(\"$F(t)$\"), \n               \"simD\" = latex2exp::TeX(\"$\\\\partial F(t) / \\\\partial t$\"),\n               \"simD2\" = latex2exp::TeX(\"$\\\\partial^2 F(t) / \\\\partial t^2$\"))\n               ) +\n  theme(axis.ticks.y = element_blank())\n\np_simu_double_logis +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 6.5: Illustration of a Double Logistic model for different values for \\(r_1\\) and \\(r_2\\), with \\(K_1=2,500\\), \\(K_2 = 5,000\\), \\(\\tau_1 = 20\\), and \\(\\tau_2 = 40\\)\n\n\n\n\n\n\n6.2.2 Double Gompertz Model\nThe double Gompertz function, its first and second derivatives with respect to time can be coded as follows:\n\n# Double-Gompertz function\n#' @param theta vector of named parameters\n#' @param x observation / training example\ndouble_gompertz_f <- function(theta, x) {\n  K1 <- theta[[\"K1\"]]\n  tau1 <- theta[[\"tau1\"]]\n  r1 <- theta[[\"r1\"]]\n  K2 <- theta[[\"K2\"]]\n  tau2 <- theta[[\"tau2\"]]\n  r2 <- theta[[\"r2\"]]\n  \n  f_1 <- K1 * exp(-exp(-r1 * (x - tau1)))\n  f_2 <- (K2-K1) * exp(-exp(-r2 * (x - tau2)))\n  \n  f_1 + f_2\n}\n\n#' Double-Gompertz function in two parts\n#' \ndouble_gompertz_f_2 <- function(theta, x) {\n  K1 <- theta[[\"K1\"]]\n  tau1 <- theta[[\"tau1\"]]\n  r1 <- theta[[\"r1\"]]\n  K2 <- theta[[\"K2\"]]\n  tau2 <- theta[[\"tau2\"]]\n  r2 <- theta[[\"r2\"]]\n\n  f_1 <- K1 * exp(-exp(-r1 * (x - tau1)))\n  f_2 <- (K2-K1) * exp(-exp(-r2 * (x-tau2)))\n  tibble(x = x, f_1 = f_1, f_2 = f_2)\n}\n\n#' First derivative\n#' \ndouble_gompertz_f_first_d <- function(theta, x) {\n  K1 <- theta[[\"K1\"]]\n  tau1 <- theta[[\"tau1\"]]\n  r1 <- theta[[\"r1\"]]\n  K2 <- theta[[\"K2\"]]\n  tau2 <- theta[[\"tau2\"]]\n  r2 <- theta[[\"r2\"]]\n\n  f_1_d <- r1 * K1 * exp(-r1 * (x - tau1) - exp(- r1 * (x - tau1)))\n  f_2_d <- r2 * (K2-K1) * exp(-r2 * (x - tau2) - exp(- r2 * (x - tau2)))\n  \n  f_1_d + f_2_d\n}\n\n#' Second derivative\ndouble_gompertz_f_second_d <- function(theta, x) {\n  K1 <- theta[[\"K1\"]]\n  tau1 <- theta[[\"tau1\"]]\n  r1 <- theta[[\"r1\"]]\n  K2 <- theta[[\"K2\"]]\n  tau2 <- theta[[\"tau2\"]]\n  r2 <- theta[[\"r2\"]]\n  \n  -K1 * r1^2 * exp(-r1*(x - tau1)) * exp(-exp(-r1*(x - tau1))) +\n    K1 * r1^2 * (exp( -r1*(x - tau1)))^2 * exp(-exp(-r1*(x - tau1))) - \n    (K2 - K1) * r2^2 * exp( -r2 * (x - tau2)) * exp( -exp(-r2*(x - tau2))) +\n    (K2 - K1) * r2^2 * (exp( -r2 * (x - tau2)))^2 * exp(-exp(-r2*(x-tau2)))\n}\n\nLet us make some illustrations by varying \\(r_1\\) and \\(r_2\\).\n\nsituations <- \n  expand_grid(r1 = c(.2, .5), r2 = c(.2, .5)) |> \n  mutate(\n    K1     = 2500,\n    K2     = 5000,\n    tau1   = 20,\n    tau2   = 40\n  )\n\nThe function that simulates \\(C(t)\\) and its first and second derivatives with respect to time for a single scenario:\n\n#' Simulation of the logistic model for some parameters\n#' \n#' @param i row number of situations\nsimu <- function(i) {\n  current_params <- situations |> slice(i)\n  current_params <- c(as_vector(current_params)) |> as.list()\n  \n  n <- 60\n  step <- .01\n  sim <-  double_gompertz_f(\n    theta = current_params, x = seq(1,n, by = step))\n  simD <-  double_gompertz_f_first_d(\n    theta = current_params, x = seq(1,n, by = step))\n  simD2 <-  double_gompertz_f_second_d(\n    theta = current_params, x = seq(1,n, by = step))\n  \n  tibble(\n    t = seq(1, n, by = step),\n    K1 = current_params[[\"K1\"]],\n    r1 = current_params[[\"r1\"]],\n    tau1 = current_params[[\"tau1\"]],\n    K2 = current_params[[\"K2\"]],\n    r2 = current_params[\"r2\"],\n    tau2 = current_params[[\"tau2\"]],\n    sim = sim) |> \n    mutate(simD = simD,\n           simD2 = simD2)\n  \n}\n\nLet us apply this function to all scenarios:\n\nsimu_res <- map_df(1:nrow(situations), simu)\n\nThe thresholds:\n\nthreshold_times <- \n  simu_res |> \n  group_by(K1, tau1, r1, K2, tau2, r2) |> \n  summarise(threshold_time_1 = unique(tau1),\n            threshold_time_2 = unique(tau2)) |> \n  ungroup() |> \n  mutate(\n    r1 = str_c(\"$\\\\r_1 = \", r1, \"$\"),\n    r2 = str_c(\"$\\\\r_2 = \", r2, \"$\")\n  )\n\n`summarise()` has grouped output by 'K1', 'tau1', 'r1', 'K2', 'tau2'. You can\noverride using the `.groups` argument.\n\n\nAnd the the plots:\n\np_simu_double_gompertz <- \n  ggplot(\n    data =  simu_res |> \n      mutate(r1 = str_c(\"$\\\\r_1 = \", r1, \"$\"),\n             r2 = str_c(\"$\\\\r_2 = \", r2, \"$\")\n      ),\n    mapping = aes(x = t)) +\n  geom_line(aes(y = sim, linetype = \"sim\")) +\n  geom_line(aes(y = simD*10, linetype = \"simD\")) +\n  geom_line(aes(y = simD2*10, linetype = \"simD2\")) +\n  geom_vline(\n    data = threshold_times, \n    mapping = aes(xintercept = threshold_time_1),\n    colour = \"red\", linetype = \"dotted\") +\n  geom_vline(\n    data = threshold_times,\n    mapping = aes(xintercept = threshold_time_2),\n    colour = \"red\", linetype = \"dotted\") +\n  facet_grid(\n    r2 ~ r1,\n    labeller = as_labeller(\n      latex2exp::TeX, \n      default = label_parsed\n    )\n  ) +\n  labs(x = \"Time\", y = latex2exp::TeX(\"$F(t)$\")) +\n  scale_y_continuous(\n    labels = comma,\n    sec.axis = sec_axis(\n      ~./10, \n      name = latex2exp::TeX(\n        \"$\\\\partial F(t) / \\\\partial t$, $\\\\partial^2 F(t) / \\\\partial t^2$\"\n      ),\n      labels = comma)) +\n  scale_linetype_manual(\n    NULL,\n    values = c(\"sim\" = \"solid\", \"simD\" = \"dashed\", \"simD2\" = \"dotdash\"),\n    labels = c(\"sim\" = latex2exp::TeX(\"$F(t)$\"),\n               \"simD\" = latex2exp::TeX(\"$\\\\partial F(t) / \\\\partial t$\"),\n               \"simD2\" = latex2exp::TeX(\"$\\\\partial^2 F(t) / \\\\partial t^2$\"))\n    ) +\n  theme(axis.ticks.y = element_blank())\n\np_simu_double_gompertz +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 6.6: Illustration of a Double Gompertz model for different values for \\(r_1\\) and \\(r_2\\), with \\(K_1=2,500\\), \\(K_2 = 5,000\\), \\(\\tau_1 = 20\\), and \\(\\tau_2 = 40\\)\n\n\n\n\n\n\n6.2.3 Double Richards Model\nThe double Richards function, its first and second derivatives with respect to time can be coded as follows:\n\n#' Double Richards\n#' \ndouble_richards_f <- function(theta, x) {\n  K1 <- theta[[\"K1\"]]\n  tau1 <- theta[[\"tau1\"]]\n  r1 <- theta[[\"r1\"]]\n  delta1 <- theta[[\"delta1\"]]\n  K2 <- theta[[\"K2\"]]\n  tau2 <- theta[[\"tau2\"]]\n  r2 <- theta[[\"r2\"]]\n  delta2 <- theta[[\"delta2\"]]\n  \n  K1 * (1 + delta1 * exp(-r1 * (x - tau1)))^(-1 / delta1) +\n    (K2 - K1) * (1 + delta2 * exp(-r2 * (x - tau2)))^(-1 / delta2)\n}\n\n#' Double Richards in two parts\n#' \ndouble_richards_f_2 <- function(theta, x) {\n  \n  K1 <- theta[[\"K1\"]]\n  tau1 <- theta[[\"tau1\"]]\n  r1 <- theta[[\"r1\"]]\n  delta1 <- theta[[\"delta1\"]]\n  K2 <- theta[[\"K2\"]]\n  tau2 <- theta[[\"tau2\"]]\n  r2 <- theta[[\"r2\"]]\n  delta2 <- theta[[\"delta2\"]]\n  \n  f_1 <- K1 * (1 + delta1 * exp(-r1 * (x - tau1)))^(-1 / delta1)\n  f_2 <- (K2 - K1) * (1 + delta2 * exp(-r2 * (x - tau2)))^(-1 / delta2)\n  \n  tibble(x = x, f_1 = f_1, f_2 = f_2)\n}\n\n#' First derivative\n#' \ndouble_richards_f_first_d <- function(theta, x) {\n  K1 <- theta[[\"K1\"]]\n  tau1 <- theta[[\"tau1\"]]\n  r1 <- theta[[\"r1\"]]\n  delta1 <- theta[[\"delta1\"]]\n  K2 <- theta[[\"K2\"]]\n  tau2 <- theta[[\"tau2\"]]\n  r2 <- theta[[\"r2\"]]\n  delta2 <- theta[[\"delta2\"]]\n  \n  r1 * K1 * exp(-r1 * (x - tau1)) * \n    (delta1 * exp(-r1 * (x - tau1)) + 1)^(-1/delta1 - 1) +\n    r2 * (K2-K1) * exp(-r2 * (x - tau2)) * \n    (delta2 * exp(-r2 * (x - tau2)) + 1)^(-1/delta2 - 1)\n}\n\n#' Second derivative\n#' \ndouble_richards_f_second_d <- function(theta, x) {\n  K1 <- theta[[\"K1\"]]\n  tau1 <- theta[[\"tau1\"]]\n  r1 <- theta[[\"r1\"]]\n  delta1 <- theta[[\"delta1\"]]\n  K2 <- theta[[\"K2\"]]\n  tau2 <- theta[[\"tau2\"]]\n  r2 <- theta[[\"r2\"]]\n  delta2 <- theta[[\"delta2\"]]\n  \n  K1 * (r1^2 * (-1/delta1 - 1) * delta1 * (-exp(-2 * r1 * (x - tau1))) * \n          (delta1 * exp(-r1 * (x - tau1)) + 1)^(-1/delta1 - 2) - r1^2 * \n          exp(-r1 * (x - tau1)) * \n          (delta1 * exp(-r1 * (x - tau1)) + 1)^(-1/delta1 - 1)) +\n    \n    (K2 - K1) * \n    (r2^2 * (-1 / delta2 - 1) * \n       delta2 * (-exp(-2 * r2 * (x - tau2))) * \n       (delta2 * exp(-r2 * (x - tau2)) + 1)^(-1/delta2 - 2) - \n       r2^2 * \n       exp(-r2 * (x - tau2)) * \n       (delta2 * exp(-r2 * (x - tau2)) + 1)^(-1/delta2 - 1))\n}\n\nLet us make some illustrations by varying \\(r_1\\) and \\(r_2\\).\n\nsituations <- \n  expand_grid(r1 = c(.2, .5), r2 = c(.2, .5)) |> \n  mutate(\n    K1     = 2500,\n    K2     = 5000,\n    tau1   = 20,\n    tau2   = 40,\n    delta1 = .5,\n    delta2 = .5\n  )\n\nThe function that simulates \\(C(t)\\) and its first and second derivatives with respect to time for a single scenario:\n\n#' Simulation of the logistic model for some parameters\n#' \n#' @param i row number of situations\nsimu <- function(i) {\n  current_params <- situations |> slice(i)\n  current_params <- c(as_vector(current_params)) |> as.list()\n  \n  n <- 60\n  step <- .01\n  sim <-  double_richards_f(\n    theta = current_params, x = seq(1,n, by = step))\n  simD <-  double_richards_f_first_d(\n    theta = current_params, x = seq(1,n, by = step))\n  simD2 <-  double_richards_f_second_d(\n    theta = current_params, x = seq(1,n, by = step))\n  \n  tibble(\n    t = seq(1, n, by = step),\n    K1 = current_params[[\"K1\"]],\n    r1 = current_params[[\"r1\"]],\n    tau1 = current_params[[\"tau1\"]],\n    delta1 = current_params[[\"delta1\"]],\n    K2 = current_params[[\"K2\"]],\n    r2 = current_params[\"r2\"],\n    tau2 = current_params[[\"tau2\"]],\n    delta2 = current_params[[\"delta2\"]],\n    sim = sim) |> \n    mutate(simD = simD,\n           simD2 = simD2)\n  \n}\n\nLet us apply this function to all scenarios:\n\nsimu_res <- map_df(1:nrow(situations), simu)\n\nThe thresholds:\n\nthreshold_times <- \n  simu_res |> \n  group_by(K1, tau1, r1, K2, tau2, r2, delta1, delta2) |> \n  summarise(threshold_time_1 = unique(tau1),\n            threshold_time_2 = unique(tau2)) |> \n  ungroup() |> \n  mutate(\n    r1 = str_c(\"$\\\\r_1 = \", r1, \"$\"),\n    r2 = str_c(\"$\\\\r_2 = \", r2, \"$\")\n  )\n\n`summarise()` has grouped output by 'K1', 'tau1', 'r1', 'K2', 'tau2', 'r2',\n'delta1'. You can override using the `.groups` argument.\n\n\nAnd the the plots:\n\np_simu_double_richards <- \n  ggplot(\n    data =  simu_res |> \n      mutate(r1 = str_c(\"$\\\\r_1 = \", r1, \"$\"),\n             r2 = str_c(\"$\\\\r_2 = \", r2, \"$\")\n      ),\n    mapping = aes(x = t)) +\n  geom_line(aes(y = sim, linetype = \"sim\")) +\n  geom_line(aes(y = simD*10, linetype = \"simD\")) +\n  geom_line(aes(y = simD2*10, linetype = \"simD2\")) +\n  geom_vline(\n    data = threshold_times, \n    mapping = aes(xintercept = threshold_time_1),\n    colour = \"red\", linetype = \"dotted\") +\n  geom_vline(\n    data = threshold_times,\n    mapping = aes(xintercept = threshold_time_2),\n    colour = \"red\", linetype = \"dotted\") +\n  facet_grid(\n    r2 ~ r1,\n    labeller = as_labeller(\n      latex2exp::TeX, \n      default = label_parsed\n    )\n  ) +\n  labs(x = \"Time\", y = latex2exp::TeX(\"$F(t)$\")) +\n  scale_y_continuous(\n    labels = comma,\n    sec.axis = sec_axis(\n      ~./10, \n      name = latex2exp::TeX(\n        \"$\\\\partial F(t) / \\\\partial t$, $\\\\partial^2 F(t) / \\\\partial t^2$\"\n      ),\n      labels = comma)) +\n  scale_linetype_manual(\n    NULL,\n    values = c(\"sim\" = \"solid\", \"simD\" = \"dashed\", \"simD2\" = \"dotdash\"),\n    labels = c(\"sim\" = latex2exp::TeX(\"$F(t)$\"),\n               \"simD\" = latex2exp::TeX(\"$\\\\partial F(t) / \\\\partial t$\"),\n               \"simD2\" = latex2exp::TeX(\"$\\\\partial^2 F(t) / \\\\partial t^2$\"))\n    ) +\n  theme(axis.ticks.y = element_blank())\n\np_simu_double_richards +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 6.7: Illustration of a Double Richards model for different values for \\(r_1\\) and \\(r_2\\), with \\(K_1=2,500\\), \\(K_2 = 5,000\\), \\(\\tau_1 = 20\\), \\(\\tau_2 = 40\\)“, \\(\\delta_1 = \\delta_2 = .5\\)\n\n\n\n\n\n\n\n\nBock, R. Darrell, Howard Wainer, Anne Petersen, David Thissen, James Murray, and Alex Roche. 1973. “A Parameterization for Individual Human Growth Curves.” Human Biology 45 (1): 63–80. http://www.jstor.org/stable/41459847.\n\n\nGompertz, Benjamin. 1825. “On the Nature of the Function Expressive of the Law of Human Mortality, and on a New Mode of Determining the Value of Life Contingencies.” Philosophical Transactions of the Royal Society of London 115 (December): 513–83. https://doi.org/10.1098/rstl.1825.0026.\n\n\nLee, Se Yoon, Bowen Lei, and Bani Mallick. 2020. “Estimation of COVID-19 Spread Curves Integrating Global Data and Borrowing Information.” PLOS ONE 15 (7): e0236860. https://doi.org/10.1371/journal.pone.0236860.\n\n\nLipovetsky, Stan. 2010. “Double Logistic Curve in Regression Modeling.” Journal of Applied Statistics 37 (11): 1785–93. https://doi.org/10.1080/02664760903093633.\n\n\nMa, Junling. 2020. “Estimating Epidemic Exponential Growth Rate and Basic Reproduction Number.” Infectious Disease Modelling 5 (January): 129–41. https://doi.org/10.1016/j.idm.2019.12.009.\n\n\nOswald, Stephen A., Ian C. T. Nisbet, Andre Chiaradia, and Jennifer M. Arnold. 2012. “FlexParamCurve: R Package for Flexible Fitting of Nonlinear Parametric Curves.” Methods in Ecology and Evolution 3 (6): 1073–77. https://doi.org/10.1111/j.2041-210x.2012.00231.x.\n\n\nRichards, F. J. 1959. “A Flexible Growth Function for Empirical Use.” Journal of Experimental Botany 10 (2): 290–301. https://doi.org/10.1093/jxb/10.2.290.\n\n\nThissen, David, R. Darrell Bock, Howard Wainer, and Alex F. Roche. 1976. “Individual Growth in Stature: A Comparison of Four Growth Studies in the U.S.A.” Annals of Human Biology 3 (6): 529–42. https://doi.org/10.1080/03014467600001791.\n\n\nTjørve, Kathleen M. C., and Even Tjørve. 2017. “The Use of Gompertz Models in Growth Analyses, and New Gompertz-Model Approach: An Addition to the Unified-Richards Family.” PLOS ONE 12 (6): e0178691. https://doi.org/10.1371/journal.pone.0178691.\n\n\nTsoularis, A., and J. Wallace. 2002. “Analysis of Logistic Growth Models.” Mathematical Biosciences 179 (1): 21–55. https://doi.org/10.1016/S0025-5564(02)00096-2.\n\n\nVerhulst, P. F. 1845. “Recherches Mathématiques Sur La Loi d’accroissement de La Population.” Nouveaux mémoires de l’Académie Royale Des Sciences Et Belles-Lettres de Bruxelles 18: 14–54. https://eudml.org/doc/182533.\n\n\nWang, Xiang-Sheng, Jianhong Wu, and Yong Yang. 2012. “Richards Model Revisited: Validation by and Application to Infection Dynamics.” Journal of Theoretical Biology 313: 12–19. https://doi.org/10.1016/j.jtbi.2012.07.024."
  },
  {
    "objectID": "phenomenological-models.html#simple-phenomenological-models",
    "href": "phenomenological-models.html#simple-phenomenological-models",
    "title": "7  Phenomenological Models",
    "section": "7.1 Simple phenomenological models",
    "text": "7.1 Simple phenomenological models\n\n7.1.1 Logistic Model\nIn R, we define the logistic function as follows:\n\n#' Logistic function\n#' \n#' @param theta vector of named parameters\n#' @param x time\nlogistic_f <-  function(theta, x) {\n  k <- theta[[\"k\"]]\n  tau <- theta[[\"tau\"]]\n  r <- theta[[\"r\"]]\n  k / ( 1+exp( -r*( x - tau ) ) )\n}\n\n#' First derivative of the logistic function\n#' \n#' @param theta vector of named parameters\n#' @param x time\nlogistic_f_first_d <- function(theta, x) {\n  k <- theta[[\"k\"]]\n  tau <- theta[[\"tau\"]]\n  r <- theta[[\"r\"]]\n  \n  (k * r * exp( -r * (x - tau) )) / (1+ exp( -r * (x - tau) ))^2\n}\n\n#' Second derivative of the logistic function\n#' \n#' @param theta vector of named parameters\n#' @param x time\nlogistic_f_second_d <- function(theta, x) {\n  k <- theta[[\"k\"]]\n  tau <- theta[[\"tau\"]]\n  r <- theta[[\"r\"]]\n  \n  k*((2*r^2 * exp(-2*r*(x - tau)))/\n       (exp(-r*(x - tau)) + 1)^3 - \n       (r^2*exp(-r*(x - tau)))/(exp(-r*(x - tau)) + 1)^2)\n}\n\n\n\n7.1.2 Gompertz Model\nIn R, the Gompertz Model can be written as:\n\n#' Gompertz function with three parameters\n#' \n#' @param theta vector of named parameters\n#' @param x time\ngompertz_f <- function(theta, x) {\n  k <- theta[[\"k\"]]\n  tau <- theta[[\"tau\"]]\n  r <- theta[[\"r\"]]\n  k*exp( -exp( -r * (x - tau) ) )\n}\n#' First order derivative of Gompertz wrt x\n#' \n#' @param theta vector of named parameters\n#' @param x time\ngompertz_f_first_d <- function(theta, x) {\n  k <- theta[[\"k\"]]\n  tau <- theta[[\"tau\"]]\n  r <- theta[[\"r\"]]\n  \n  k * (x - tau) * exp(r * (tau - x) - exp(r * (tau - x)))\n}\n\n#' Second order derivative of Gompertz\n#' \n#' @param theta vector of named parameters\n#' @param x time\ngompertz_f_second_d <- function(theta, x) {\n  k <- theta[[\"k\"]]\n  tau <- theta[[\"tau\"]]\n  r <- theta[[\"r\"]]\n  \n  -k * r^2 * exp(-r * (x - tau)) * \n    exp(-exp(-r * (x - tau))) + \n    k * r^2 * (exp(-r * (x - tau)))^2 * exp(-exp(-r * (x - tau)))\n}\n\n\n\n7.1.3 Richards Model\nIn R, Richards’ Model can be coded as follows:\n\n#' Richards function with four parameters\n#' \n#' @param theta vector of named parameters\n#' @param x time\nrichards_f <- function(theta, x) {\n  k <- theta[[\"k\"]]\n  tau <- theta[[\"tau\"]]\n  r <- theta[[\"r\"]]\n  delta <- theta[[\"delta\"]]\n  \n  k / (1 + delta * exp(-r * delta * (x - tau)))^(1 / delta)\n}\n\n\n#' First order derivative of Richards function wrt time (x)\n#' \n#' @param theta vector of named parameters\n#' @param x time\nrichards_f_first_d <- function(theta, x) {\n  k <- theta[[\"k\"]]\n  tau <- theta[[\"tau\"]]\n  r <- theta[[\"r\"]]\n  delta <- theta[[\"delta\"]]\n  \n  delta * k * r * exp(delta * (-r) * (x - tau)) * \n    (delta * exp(delta * (-r) * (x - tau)) + 1)^(-1 / delta - 1)\n}\n\n\n#' Second order derivative of Richards function wrt time (x)\n#' \n#' @param theta vector of named parameters\n#' @param x time\nrichards_f_second_d <- function(theta, x) {\n  k <- theta[[\"k\"]]\n  tau <- theta[[\"tau\"]]\n  r <- theta[[\"r\"]]\n  delta <- theta[[\"delta\"]]\n  \n  k * ((-1 / delta - 1) * \n         delta^3 * r^2 * (-exp(-2 * delta * r * (x - tau))) *\n         (delta * exp(delta * (-r) * (x - tau)) + 1)^(-1 / delta - 2) - \n         delta^2 * r^2 * exp(delta * (-r) * (x - tau)) * \n         (delta * exp(delta * (-r) * (x - tau)) + 1)^(-1 / delta - 1))\n}\n\n\n\n7.1.4 Help functions\nSome functions can be helpful to fit the models both for the number of cases and deaths. First of all, we can create a function that gives the dates for the different periods:\n\nstart_first_wave: start of the first wave, defined as the first date when the cumulative number of cases is greater than 1\nstart_high_stringency: date at which the stringency index reaches its maximum value during the first 100 days of the sample\nstart_reduce_restrict: moment at which the restrictions of the first wave starts to lower\nstart_date_sample_second_wave: 60 days after the relaxation of restrictions (60 days after after start_reduce_restrict)\nlength_high_stringency: number of days between start_high_stringency and start_reduce_restrict`.\n\n\n#' Gives the dates of the different periods (first wave, start of containment, ...)\n#' \n#' @param country_name name of the country\n#' @param type if `\"deaths\"` returns the number of deaths, otherwise the number of cases\nget_dates <- function(country_name,\n                      type = c(\"cases\", \"deaths\")) {\n  if (type == \"deaths\") {\n    df_country <- deaths_df |> filter(country == !!country_name)\n  } else {\n    df_country <- confirmed_df |> filter(country == !!country_name)\n  }\n  \n  # Start of the first wave\n  start_first_wave <- \n    df_country |> \n    arrange(date) |> \n    filter(value > 0) |> \n    slice(1) |> \n    magrittr::extract2(\"date\")\n  \n  # Start of period with severity greater or equal than 70 index among the first 100 days\n  start_high_stringency <- \n    df_country |> \n    slice(1:100) |>\n    filter(stringency_index >= 70) |> \n    slice(1) |> \n    magrittr::extract2(\"date\")\n  \n  # Max for Sweden\n  if (country_name == \"Sweden\") {\n    start_high_stringency <- \n      df_country |> \n      slice(1:100) |>\n      arrange(desc(stringency_index), date) |>\n      slice(1) |> \n      magrittr::extract2(\"date\")\n  }\n  \n  # Max stringency first 100 days\n  start_max_stringency <- \n    df_country |> \n    slice(1:100) |>\n    arrange(desc(stringency_index), date) |>\n    slice(1) |> \n    magrittr::extract2(\"date\")\n\n  # Moment at which the restrictions of the first wave starts to lower\n  start_reduce_restrict <- \n    df_country |> \n    arrange(date) |> \n    filter(date >= start_max_stringency) |> \n    mutate(tmp = dplyr::lag(stringency_index)) |> \n    mutate(same_strin = stringency_index == tmp) |> \n    mutate(same_strin = ifelse(row_number() == 1, TRUE, same_strin)) |> \n    filter(same_strin == FALSE) |> \n    slice(1) |>\n    magrittr::extract2(\"date\")\n  \n  start_date_sample_second_wave <- start_reduce_restrict + \n    lubridate::ddays(60)\n  \n  \n  # Length of high stringency period\n  length_high_stringency <- lubridate::interval(\n    start_high_stringency, start_reduce_restrict) / \n    lubridate::ddays(1)\n  \n  \n  tibble(\n    country = country_name,\n    start_first_wave = start_first_wave,\n    start_high_stringency = start_high_stringency,\n    start_reduce_restrict = start_reduce_restrict,\n    start_date_sample_second_wave = start_date_sample_second_wave,\n    length_high_stringency = length_high_stringency\n  )\n}\n\nWe can then create a function to prepare the dataset fot a given country. This function allows to provide data for:\n\nthe first wave sample: between the first case and 60 days after the day at which the severity index reaches its max value (during the first 100 days) ; we also add some data for the out-of-sample prediction (of length set outside the function by out_of_sample_horizon)\nthe second wave sample: data posterior to 60 days after the relaxation of restrictions\nall available data\n\nThe argument type of the function controls whether we get number of cases or deaths.\n\n#' Extracts the cases data for a country\n#' \n#' @description\n#' If one only wants the first wave (first_wave=TRUE) the following start and\n#' end date are used:\n#' - start: date of the first case\n#' - end: when the severity index is greater than 70\n#' \n#' @param country_name name of the country\n#' @param sample if `\"first\"`, returns the sample for the first wave; if \"second\", \n#' for the second wave, and `\"all\"` for all data up to the end\n#' @param type if `\"deaths\"` returns the number of deaths, otherwise the number of cases\nget_values_country <- function(country_name,\n                               sample = c(\"first\", \"second\", \"all\"),\n                               type = c(\"cases\", \"deaths\")) {\n  if (type == \"deaths\") {\n    df_country <- deaths_df |> filter(country == !!country_name)\n  } else {\n    df_country <- confirmed_df |> filter(country == !!country_name)\n  }\n  dates_country <- get_dates(country_name, type = type)\n  \n  # Maximum of the severity index\n  max_severity <- max(df_country$stringency_index, na.rm=TRUE)\n  dates_country$max_severity <- max_severity\n  \n  if (sample == \"first\") {\n    df_country <- \n      df_country |> \n      filter(date >= dates_country$start_first_wave,\n             # 60 days after end max stringency in the first interval of 100 \n             # days + `out_of_sample_horizon` more days for out-of-sample pred\n             date <= dates_country$start_date_sample_second_wave +\n               lubridate::ddays(out_of_sample_horizon)\n             ) \n  } else if (sample == \"second\") {\n    df_country <- \n      df_country |> \n      filter(date >= dates_country$start_date_sample_second_wave)\n    \n    # Let us remove the number of cases of the first date of this sample\n    # to all observation (translation to 1)\n    start_val_cases <- df_country$value[1]\n    df_country <- \n      df_country |> \n      mutate(value = value - start_val_cases + 1)\n  } else {\n    df_country <- \n      df_country |> \n      filter(date >= dates_country$start_first_wave)\n  }\n  \n  # Moving Average for missing values (i.e., for Ireland)\n  if (any(is.na(df_country$value))) {\n    replacement_values <- round(\n      zoo::rollapply(\n        df_country$value, \n        width=3, \n        FUN=function(x) mean(x, na.rm=TRUE), \n        by=1,\n        by.column=TRUE,\n        partial=TRUE,\n        fill=NA,\n        align=\"center\")\n    )\n    \n    # Replace only missing values\n    df_country <- \n      df_country |> \n      mutate(replacement_values = !!replacement_values) |> \n      mutate(\n        value = ifelse(\n          is.na(value), \n          yes = replacement_values,\n          no = value)\n      ) |> \n      select(-replacement_values)\n  }\n  \n  df_country <- \n    df_country |> \n    mutate(t = row_number()-1) |> \n    mutate(y = value)\n\n  list(df_country = df_country, dates_country = dates_country)\n}\n\nWe need to define a loss function that will be minimized to get the estimates of the models.\n\n#' Loss function\n#' \n#' @param theta vector of named parameters of the model\n#' @param fun prediction function of the model\n#' @param y target variable\n#' @param t time component (feature)\nloss_function <- function(theta,\n                          fun,\n                          y,\n                          t) {\n  (y - fun(theta = theta, x = t))\n}\n\nOnce the model are estimated, we can compute some goodness of fit criteria. Let us create a function that computes the AIC, the BIC and the RMSE for a specific model. The function expects three arguments: the prediction function of the model (f), the values for the parameters of the model (in a named vector – theta), and the observations (data).\n\n#' Compute some goodness of fit criteria\n#' \n#' @param f prediction function of the model\n#' @param data data that contains the two columns `y` and `t`\n#' @param theta estimated coefficients for the model (used in `f`)\nget_criteria <- function(f,\n                         data,\n                         theta) {\n  n <- nrow(data)\n  k <- length(theta)\n  w <- rep(1, n)\n  \n  errors <- loss_function(theta = theta, fun = f, y = data$y, t = data$t)\n  \n  mse <- sum(errors^2) / n\n  rmse <- sqrt(mse)\n  \n  # Log-likelihood\n  ll <- 0.5 * (sum(log(w)) - n * \n                 (log(2 * pi) + 1 - log(n) + log(sum(w * errors^2))))\n  aic <- 2 * (k+1) - 2*ll\n  bic <- -2 * ll + log(n) * (k+1)\n  \n  c(AIC = aic, BIC = bic, RMSE = rmse)\n  \n}"
  },
  {
    "objectID": "phenomenological-models.html#predictions-at-the-end-of-the-first-wave",
    "href": "phenomenological-models.html#predictions-at-the-end-of-the-first-wave",
    "title": "7  Phenomenological Models",
    "section": "7.2 Predictions at the end of the first wave",
    "text": "7.2 Predictions at the end of the first wave\nLet us turn to the estimation of the number of cases and the estimation of the number of deaths for the first wave. Recall that the sample used is defined as follows: from the first date where the cumulative number of cases (deaths) is greater or equal to 1 to 60 days after the maximum value of the stringency index (start of containment).\nLet us focus here first on the estimation of the Gompertz model, for the number of cased, in the UK. Then, we can wrap up the code and estimate all models to all countries.\n\n7.2.1 Example for the UK\nWe would like to make out-of-sample predictions, to assess how the models perform with unseen data. To that end, let us define a horizon at which to look at (30 days):\n\nout_of_sample_horizon <- 30\n\nLet us also define a limit of the number of fitted values to return:\n\nhorizon_pred <- 250\n\nWe want to focus on the UK:\n\ncountry_name <- \"United Kingdom\"\npop_country <- population |> \n  filter(country == !!country_name) |>\n  magrittr::extract2(\"pop\")\npop_country\n\n[1] 6.7e+07\n\n\nThe data for the sample can be obtained using the get_values_country() function.\n\ndata_country <- \n  get_values_country(\n    country_name = country_name,\n    sample = \"first\",\n    type = \"cases\"\n  )\ndata_country\n\n$df_country\n# A tibble: 192 × 8\n   country  country_code date       value stringency_index days_since_2020_01_22\n   <chr>    <chr>        <date>     <int>            <dbl>                 <dbl>\n 1 United … GBR          2020-01-31     2             8.33                     9\n 2 United … GBR          2020-02-01     2             8.33                    10\n 3 United … GBR          2020-02-02     2            11.1                     11\n 4 United … GBR          2020-02-03     8            11.1                     12\n 5 United … GBR          2020-02-04     8            11.1                     13\n 6 United … GBR          2020-02-05     9            11.1                     14\n 7 United … GBR          2020-02-06     9            11.1                     15\n 8 United … GBR          2020-02-07     9            11.1                     16\n 9 United … GBR          2020-02-08    13            11.1                     17\n10 United … GBR          2020-02-09    14            11.1                     18\n# ℹ 182 more rows\n# ℹ 2 more variables: t <dbl>, y <int>\n\n$dates_country\n# A tibble: 1 × 7\n  country        start_first_wave start_high_stringency start_reduce_restrict\n  <chr>          <date>           <date>                <date>               \n1 United Kingdom 2020-01-31       2020-03-23            2020-05-11           \n# ℹ 3 more variables: start_date_sample_second_wave <date>,\n#   length_high_stringency <dbl>, max_severity <dbl>\n\n\nWe obtained both the sample to learn from and the dates of interest, in a list. Let us store those in single objects:\n\ndf_country <- data_country$df_country\ndates_country <- data_country$dates_country\n\nThe model we wish to estimate is the Gompertz model:\n\nmodel_name <- \"Gompertz\"\n\nThe different functions of the model are the following:\n\nmodel_function <- gompertz_f\nmodel_function_d <- gompertz_f_first_d\nmodel_function_dd <- gompertz_f_second_d\n\nThe training sample:\n\n# Training sample\ndf_country_training <- \n  df_country |> \n  slice(1:(nrow(df_country) - out_of_sample_horizon))\ndf_country_training\n\n# A tibble: 162 × 8\n   country  country_code date       value stringency_index days_since_2020_01_22\n   <chr>    <chr>        <date>     <int>            <dbl>                 <dbl>\n 1 United … GBR          2020-01-31     2             8.33                     9\n 2 United … GBR          2020-02-01     2             8.33                    10\n 3 United … GBR          2020-02-02     2            11.1                     11\n 4 United … GBR          2020-02-03     8            11.1                     12\n 5 United … GBR          2020-02-04     8            11.1                     13\n 6 United … GBR          2020-02-05     9            11.1                     14\n 7 United … GBR          2020-02-06     9            11.1                     15\n 8 United … GBR          2020-02-07     9            11.1                     16\n 9 United … GBR          2020-02-08    13            11.1                     17\n10 United … GBR          2020-02-09    14            11.1                     18\n# ℹ 152 more rows\n# ℹ 2 more variables: t <dbl>, y <int>\n\n\nThen, we can fit the model. But first, we need starting values for the optimization algorithm:\n\n# The starting values\nstart <- list(\n  k   = pop_country,\n  tau = 80,\n  r   = .24\n)\n\nThen we can proceed with the estimation of the model:\n\nout <- nls.lm(\n  par = start, \n  fn = loss_function,\n  y = df_country$y,\n  t = df_country$t,\n  fun = model_function,\n  nls.lm.control(maxiter = 100),\n  jac = NULL, \n  lower = NULL,\n  upper = NULL)\n\nHere are the results:\n\nsummary(out)\n\n\nParameters:\n     Estimate Std. Error t value Pr(>|t|)    \nk   2.992e+05  6.512e+02   459.4   <2e-16 ***\ntau 7.587e+01  1.410e-01   538.2   <2e-16 ***\nr   4.296e-02  3.861e-04   111.3   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3738 on 189 degrees of freedom\nNumber of iterations to termination: 12 \nReason for termination: Relative error in the sum of squares is at most `ftol'. \n\n\nLet us store the estimated parameters of the model:\n\nparams <- tibble(\n  model_type = model_name,\n  country = country_name,\n  coef_estimate_name = names(coef(out)),\n  coef_estimate = coef(out)\n)\nparams\n\n# A tibble: 3 × 4\n  model_type country        coef_estimate_name coef_estimate\n  <chr>      <chr>          <chr>                      <dbl>\n1 Gompertz   United Kingdom k                    299189.    \n2 Gompertz   United Kingdom tau                      75.9   \n3 Gompertz   United Kingdom r                         0.0430\n\n\nThe goodness of fit can be obtained using the previously defined function get_criteria(), and the results can be saved in a table:\n\n# Goodness of fit\ncrit <- get_criteria(\n  f = model_function,\n  data = df_country_training,\n  theta = params$coef_estimate\n)\ncriteria <- tibble(\n  model_type = model_name,\n  country = country_name,\n  bind_rows(crit)\n)\ncriteria\n\n# A tibble: 1 × 5\n  model_type country          AIC   BIC  RMSE\n  <chr>      <chr>          <dbl> <dbl> <dbl>\n1 Gompertz   United Kingdom 3049. 3062. 2885.\n\n\nLet us now turn to the out-of-sample predictions. We need to compare the predictions of the model 30 days after the end of the sample, with those that were observed (but not used in the estimation). The test sample becomes:\n\ndf_country_test <- \n  df_country |> \n  slice((nrow(df_country) - out_of_sample_horizon + 1):(nrow(df_country)))\ndf_country_test\n\n# A tibble: 30 × 8\n   country country_code date        value stringency_index days_since_2020_01_22\n   <chr>   <chr>        <date>      <int>            <dbl>                 <dbl>\n 1 United… GBR          2020-07-11 288953             64.4                   171\n 2 United… GBR          2020-07-12 289603             64.4                   172\n 3 United… GBR          2020-07-13 290133             64.4                   173\n 4 United… GBR          2020-07-14 291373             64.4                   174\n 5 United… GBR          2020-07-15 291911             64.4                   175\n 6 United… GBR          2020-07-16 292552             64.4                   176\n 7 United… GBR          2020-07-17 293239             64.4                   177\n 8 United… GBR          2020-07-18 294066             64.4                   178\n 9 United… GBR          2020-07-19 294792             64.4                   179\n10 United… GBR          2020-07-20 295372             64.4                   180\n# ℹ 20 more rows\n# ℹ 2 more variables: t <dbl>, y <int>\n\n\nThe different criteria can be computed on the predictions made for that 30 days interval:\n\ncrit_oos <- get_criteria(\n  f = model_function, \n  data = df_country_test,\n  theta = params$coef_estimate\n)\ncriteria_oos <- tibble(\n  model_type = model_name,\n  country = country_name,\n  bind_rows(crit_oos)\n)\ncriteria_oos\n\n# A tibble: 1 × 5\n  model_type country          AIC   BIC  RMSE\n  <chr>      <chr>          <dbl> <dbl> <dbl>\n1 Gompertz   United Kingdom  620.  626. 6564.\n\n\nLet us now process the data so that we know, for each observations, whether the prediction corresponds to seen or unseen data:\n\nobs <- df_country_training$value\ntype_obs <- rep(\"obs\", length(obs))\nif (length(obs) < horizon_pred) {\n  obs <- c(obs, rep(NA, horizon_pred-length(obs)))\n  type_obs <- c(\n    type_obs,\n    rep(\"out_of_sample\", horizon_pred - length(type_obs))\n  )\n}\nhead(obs)\n\n[1] 2 2 2 8 8 9\n\ntail(obs)\n\n[1] NA NA NA NA NA NA\n\nlength(obs)\n\n[1] 250\n\nhead(type_obs)\n\n[1] \"obs\" \"obs\" \"obs\" \"obs\" \"obs\" \"obs\"\n\ntail(type_obs)\n\n[1] \"out_of_sample\" \"out_of_sample\" \"out_of_sample\" \"out_of_sample\"\n[5] \"out_of_sample\" \"out_of_sample\"\n\nlength(type_obs)\n\n[1] 250\n\n\nLet us create a vector of corresponding dates for those:\n\ndates <- df_country_training$date\nif (length(dates) < horizon_pred) {\n  dates <- dates[1] + lubridate::ddays(seq_len(horizon_pred) - 1)\n}\nhead(dates)\n\n[1] \"2020-01-31\" \"2020-02-01\" \"2020-02-02\" \"2020-02-03\" \"2020-02-04\"\n[6] \"2020-02-05\"\n\ntail(dates)\n\n[1] \"2020-10-01\" \"2020-10-02\" \"2020-10-03\" \"2020-10-04\" \"2020-10-05\"\n[6] \"2020-10-06\"\n\nlength(dates)\n\n[1] 250\n\n\nThen, the predictions obtained with the model can be saved in a table:\n\nfitted_val <- tibble(\n  country  = !!country_name,\n  index    = seq_len(horizon_pred) - 1,\n  value    = obs,\n  type_obs = type_obs,\n  date     = dates\n) |> \n  mutate(\n    model_type   = model_name,\n    fitted_value = model_function(\n      theta = params$coef_estimate,\n      x = index)\n  )\nfitted_val\n\n# A tibble: 250 × 7\n   country        index value type_obs date       model_type fitted_value\n   <chr>          <dbl> <int> <chr>    <date>     <chr>             <dbl>\n 1 United Kingdom     0     2 obs      2020-01-31 Gompertz     0.00000149\n 2 United Kingdom     1     2 obs      2020-02-01 Gompertz     0.00000446\n 3 United Kingdom     2     2 obs      2020-02-02 Gompertz     0.0000127 \n 4 United Kingdom     3     8 obs      2020-02-03 Gompertz     0.0000347 \n 5 United Kingdom     4     8 obs      2020-02-04 Gompertz     0.0000909 \n 6 United Kingdom     5     9 obs      2020-02-05 Gompertz     0.000228  \n 7 United Kingdom     6     9 obs      2020-02-06 Gompertz     0.000552  \n 8 United Kingdom     7     9 obs      2020-02-07 Gompertz     0.00129   \n 9 United Kingdom     8    13 obs      2020-02-08 Gompertz     0.00289   \n10 United Kingdom     9    14 obs      2020-02-09 Gompertz     0.00628   \n# ℹ 240 more rows\n\n\nUsing the second order derivative with respect to time of the model, we can compute the three key moments, and store those in a table:\n\n# Key moments\ntau <- out$par$tau\n\n# First wave\nacceleration <- model_function_dd(\n  theta = out$par, x = seq(1, tau)\n) |> which.max()\n\ndeceleration <- tau + \n  model_function_dd(\n    theta = out$par,\n    x = seq(tau, horizon_pred)\n  ) |> which.min()\n\n# Peak\nabs_second_d_vals <- abs(\n  model_function_dd(\n    theta = out$par,\n    seq(acceleration, deceleration)\n  )\n)\nabs_second_d_vals <- abs_second_d_vals / min(abs_second_d_vals)\npeak <- mean(which(abs_second_d_vals == 1)) + acceleration\n\n# Relative speed at max speed\nrelative_speed <- model_function_d(theta = out$par, x = peak) /\n  model_function(theta = out$par, x = peak)\n\nkey_moments <- tibble(\n  country  = country_name,\n  model_type = model_name,\n  t = round(c(acceleration, peak, deceleration)),\n  value = model_function(theta = out$par, t),\n  moment = c(\"acceleration\", \"peak\", \"deceleration\")\n) |> \n  bind_rows(\n    tibble(\n      country = country_name,\n      model_type = model_name,\n      t = peak,\n      value = relative_speed,\n      moment = \"relative_speed\"\n    )\n  ) |> \n  mutate(\n    date = first(df_country_training$date) + lubridate::ddays(t) - 1\n  )\nkey_moments\n\n# A tibble: 4 × 6\n  country        model_type     t     value moment         date      \n  <chr>          <chr>      <dbl>     <dbl> <chr>          <date>    \n1 United Kingdom Gompertz      53  20708.   acceleration   2020-03-23\n2 United Kingdom Gompertz      77 115420.   peak           2020-04-16\n3 United Kingdom Gompertz      99 206618.   deceleration   2020-05-08\n4 United Kingdom Gompertz      77      1.08 relative_speed 2020-04-16\n\n\nWe can plot the results showing observed values and predicted ones. First, let us prepare the data.\n\nthe_dates <- map_df(\"United Kingdom\", get_dates, type = \"cases\")\ndf_key_moments <- \n    key_moments |> \n    left_join(colour_table, by = \"country\")\ndf_plot <- \n  fitted_val |> \n  pivot_longer(cols = c(value, fitted_value)) |> \n  filter(! (type_obs == \"out_of_sample\" & name == \"value\") ) |> \n  mutate(linetype = str_c(type_obs, \"_\", name)) |> \n  mutate(\n    linetype = ifelse(\n      linetype %in% c(\"obs_value\", \"out_of_sample_value\"),\n      yes = \"obs\",\n      no = linetype\n    )\n  ) |> \n  select(country, date, value, linetype)\n\ndf_plot <- \n  df_plot |> \n  left_join(colour_table, by = \"country\") |>   \n  mutate(\n    linetype = factor(\n      linetype, \n      levels = c(\"obs\", \"obs_fitted_value\", \"out_of_sample_fitted_value\"),\n      labels = c(\"Obs.\", \"Fitted values\", \"Predictions\")\n    )\n  )\n\ndf_dots <- \n  the_dates |> \n  mutate(end_sample = start_reduce_restrict + lubridate::ddays(60)) |> \n  left_join(\n    df_plot |> \n      filter(linetype == \"Obs.\") |> \n      select(country, date, value), \n    by = c(\"end_sample\" = \"date\", \"country\" = \"country\")\n  ) |> \n  left_join(colour_table, by = \"country\")\n\nAnd then we can plot:\n\nggplot(data = df_plot) +\n    geom_line(\n      mapping = aes(\n        x = date, y = value,\n        colour = country_code, linetype = linetype)\n    ) +\n    geom_point(\n      data = df_dots,\n      mapping = aes(x = end_sample, y = value,\n                    colour = country_code),\n      shape = 2,\n      show.legend = FALSE\n    ) +\n    geom_point(\n      data = df_key_moments,\n      mapping = aes(\n        x = date, y = value,\n        colour = country_code\n      )\n    ) +\n    scale_colour_manual(NULL, values = colour_countries) +\n    scale_y_continuous(labels = comma) +\n    labs(x = NULL, y = \"Cases\") +\n    scale_x_date(\n      labels = date_format(\"%b %d\"),\n      breaks = lubridate::ymd(lubridate::pretty_dates(df_plot$date, n = 5))) +\n    scale_linetype_manual(\n      NULL,\n      values = c(\"Obs.\" = \"solid\",\n                 \"Fitted values\" = \"dashed\",\n                 \"Predictions\" = \"dotted\")) +\n    theme_paper()\n\n\n\n\nFigure 7.1: Prediction of cases using the Gompertz model, for the UK (single wave)\n\n\n\n\nIn Figure 7.1, big dots represent the three stages of the epidemic. A triangle indicates the end of the estimation period corresponding to 60 days after the end of the most severe confinement. The Solid line corresponds to the observed series and dotted lines the predictions using Gompertz model."
  },
  {
    "objectID": "phenomenological-models.html#double-sigmoid-functions-to-account-for-a-second-wave",
    "href": "phenomenological-models.html#double-sigmoid-functions-to-account-for-a-second-wave",
    "title": "7  Phenomenological Models",
    "section": "7.3 Double sigmoid functions to account for a second wave",
    "text": "7.3 Double sigmoid functions to account for a second wave\nLet us now turn to double sigmoid functions to model two waves of the epidemic.\n\n7.3.1 Double Logistic\nThe double logistic function, its first and second derivatives with respect to time can be coded as follows:\n\n# Double logistic\ndouble_logistic_f <-  function(theta, x) {\n  K1 <- theta[[\"K1\"]]\n  tau1 <- theta[[\"tau1\"]]\n  r1 <- theta[[\"r1\"]]\n  K2 <- theta[[\"K2\"]]\n  tau2 <- theta[[\"tau2\"]]\n  r2 <- theta[[\"r2\"]]\n  \n  (K1/(1 + exp(-r1 * (x - tau1)))) + ((K2 - K1)/(1 + exp(-r2 * (x - tau2))))\n}\n# Double logistic in two parts\ndouble_logistic_f_2 <- function(theta, x) {\n  K1 <- theta[[\"K1\"]]\n  tau1 <- theta[[\"tau1\"]]\n  r1 <- theta[[\"r1\"]]\n  K2 <- theta[[\"K2\"]]\n  tau2 <- theta[[\"tau2\"]]\n  r2 <- theta[[\"r2\"]]\n  \n  f_1 <- K1 / (1 + exp(-r1 * (x - tau1)))\n  f_2 <- (K2 - K1)/(1 + exp(-r2 * (x - tau2)))\n  tibble(x = x, f_1 = f_1, f_2 = f_2)\n}\n\n# First derivative\ndouble_logistic_f_first_d <-  function(theta, x) {\n  K1 <- theta[[\"K1\"]]\n  tau1 <- theta[[\"tau1\"]]\n  r1 <- theta[[\"r1\"]]\n  K2 <- theta[[\"K2\"]]\n  tau2 <- theta[[\"tau2\"]]\n  r2 <- theta[[\"r2\"]]\n  \n  (r1 * K1 * exp(-r1 * (x - tau1))) / ((exp(-r1 * (x-tau1)) + 1)^2) +\n    (r2 * (K2-K1) * exp(-r2 * (x - tau2))) / ((exp(-r2 * (x-tau2)) + 1)^2)\n}\n# Second derivative\ndouble_logistic_f_second_d <-  function(theta, x) {\n  K1 <- theta[[\"K1\"]]\n  tau1 <- theta[[\"tau1\"]]\n  r1 <- theta[[\"r1\"]]\n  K2 <- theta[[\"K2\"]]\n  tau2 <- theta[[\"tau2\"]]\n  r2 <- theta[[\"r2\"]]\n  \n  K1 *  ((2 * r1^2 * exp(-2 * r1 * (x - tau1)))/(exp(-r1 * (x - tau1)) + 1)^3 - \n           ( r1^2 * exp(- r1 * (x - tau1)))/(exp(-r1 * (x - tau1)) + 1)^2) +\n    \n    (K2-K1) *  \n    ((2 * r2^2 * exp(-2 * r2 * (x - tau2)))/(exp(-r2 * (x - tau2)) + 1)^3 - \n       ( r2^2 * exp(- r2 * (x - tau2)))/(exp(-r2 * (x - tau2)) + 1)^2)\n  \n}\n\n\n\n7.3.2 Double Gompertz Model\nThe double Gompertz function, its first and second derivatives with respect to time can be coded as follows:\n\n# Double-Gompertz function\n#' @param theta vector of named parameters\n#' @param x observation / training example\ndouble_gompertz_f <- function(theta, x) {\n  K1 <- theta[[\"K1\"]]\n  tau1 <- theta[[\"tau1\"]]\n  r1 <- theta[[\"r1\"]]\n  K2 <- theta[[\"K2\"]]\n  tau2 <- theta[[\"tau2\"]]\n  r2 <- theta[[\"r2\"]]\n  \n  f_1 <- K1 * exp(-exp(-r1 * (x - tau1)))\n  f_2 <- (K2-K1) * exp(-exp(-r2 * (x - tau2)))\n  \n  f_1 + f_2\n}\n\n#' Double-Gompertz function in two parts\n#' \ndouble_gompertz_f_2 <- function(theta, x) {\n  K1 <- theta[[\"K1\"]]\n  tau1 <- theta[[\"tau1\"]]\n  r1 <- theta[[\"r1\"]]\n  K2 <- theta[[\"K2\"]]\n  tau2 <- theta[[\"tau2\"]]\n  r2 <- theta[[\"r2\"]]\n\n  f_1 <- K1 * exp(-exp(-r1 * (x - tau1)))\n  f_2 <- (K2-K1) * exp(-exp(-r2 * (x-tau2)))\n  tibble(x = x, f_1 = f_1, f_2 = f_2)\n}\n\n#' First derivative\n#' \ndouble_gompertz_f_first_d <- function(theta, x) {\n  K1 <- theta[[\"K1\"]]\n  tau1 <- theta[[\"tau1\"]]\n  r1 <- theta[[\"r1\"]]\n  K2 <- theta[[\"K2\"]]\n  tau2 <- theta[[\"tau2\"]]\n  r2 <- theta[[\"r2\"]]\n\n  f_1_d <- r1 * K1 * exp(-r1 * (x - tau1) - exp(- r1 * (x - tau1)))\n  f_2_d <- r2 * (K2-K1) * exp(-r2 * (x - tau2) - exp(- r2 * (x - tau2)))\n  \n  f_1_d + f_2_d\n}\n\n#' Second derivative\ndouble_gompertz_f_second_d <- function(theta, x) {\n  K1 <- theta[[\"K1\"]]\n  tau1 <- theta[[\"tau1\"]]\n  r1 <- theta[[\"r1\"]]\n  K2 <- theta[[\"K2\"]]\n  tau2 <- theta[[\"tau2\"]]\n  r2 <- theta[[\"r2\"]]\n  \n  -K1 * r1^2 * exp(-r1*(x - tau1)) * exp(-exp(-r1*(x - tau1))) +\n    K1 * r1^2 * (exp( -r1*(x - tau1)))^2 * exp(-exp(-r1*(x - tau1))) - \n    (K2 - K1) * r2^2 * exp( -r2 * (x - tau2)) * exp( -exp(-r2*(x - tau2))) +\n    (K2 - K1) * r2^2 * (exp( -r2 * (x - tau2)))^2 * exp(-exp(-r2*(x-tau2)))\n}\n\n\n\n7.3.3 Double Richards Model\nThe double Richards function, its first and second derivatives with respect to time can be coded as follows:\n\n#' Double Richards\n#' \ndouble_richards_f <- function(theta, x) {\n  K1 <- theta[[\"K1\"]]\n  tau1 <- theta[[\"tau1\"]]\n  r1 <- theta[[\"r1\"]]\n  delta1 <- theta[[\"delta1\"]]\n  K2 <- theta[[\"K2\"]]\n  tau2 <- theta[[\"tau2\"]]\n  r2 <- theta[[\"r2\"]]\n  delta2 <- theta[[\"delta2\"]]\n  \n  K1 * (1 + delta1 * exp(-r1 * (x - tau1)))^(-1 / delta1) +\n    (K2 - K1) * (1 + delta2 * exp(-r2 * (x - tau2)))^(-1 / delta2)\n}\n\n#' Double Richards in two parts\n#' \ndouble_richards_f_2 <- function(theta, x) {\n  \n  K1 <- theta[[\"K1\"]]\n  tau1 <- theta[[\"tau1\"]]\n  r1 <- theta[[\"r1\"]]\n  delta1 <- theta[[\"delta1\"]]\n  K2 <- theta[[\"K2\"]]\n  tau2 <- theta[[\"tau2\"]]\n  r2 <- theta[[\"r2\"]]\n  delta2 <- theta[[\"delta2\"]]\n  \n  f_1 <- K1 * (1 + delta1 * exp(-r1 * (x - tau1)))^(-1 / delta1)\n  f_2 <- (K2 - K1) * (1 + delta2 * exp(-r2 * (x - tau2)))^(-1 / delta2)\n  \n  tibble(x = x, f_1 = f_1, f_2 = f_2)\n}\n\n#' First derivative\n#' \ndouble_richards_f_first_d <- function(theta, x) {\n  K1 <- theta[[\"K1\"]]\n  tau1 <- theta[[\"tau1\"]]\n  r1 <- theta[[\"r1\"]]\n  delta1 <- theta[[\"delta1\"]]\n  K2 <- theta[[\"K2\"]]\n  tau2 <- theta[[\"tau2\"]]\n  r2 <- theta[[\"r2\"]]\n  delta2 <- theta[[\"delta2\"]]\n  \n  r1 * K1 * exp(-r1 * (x - tau1)) * \n    (delta1 * exp(-r1 * (x - tau1)) + 1)^(-1/delta1 - 1) +\n    r2 * (K2-K1) * exp(-r2 * (x - tau2)) * \n    (delta2 * exp(-r2 * (x - tau2)) + 1)^(-1/delta2 - 1)\n}\n\n#' Second derivative\n#' \ndouble_richards_f_second_d <- function(theta, x) {\n  K1 <- theta[[\"K1\"]]\n  tau1 <- theta[[\"tau1\"]]\n  r1 <- theta[[\"r1\"]]\n  delta1 <- theta[[\"delta1\"]]\n  K2 <- theta[[\"K2\"]]\n  tau2 <- theta[[\"tau2\"]]\n  r2 <- theta[[\"r2\"]]\n  delta2 <- theta[[\"delta2\"]]\n  \n  K1 * (r1^2 * (-1/delta1 - 1) * delta1 * (-exp(-2 * r1 * (x - tau1))) * \n          (delta1 * exp(-r1 * (x - tau1)) + 1)^(-1/delta1 - 2) - r1^2 * \n          exp(-r1 * (x - tau1)) * \n          (delta1 * exp(-r1 * (x - tau1)) + 1)^(-1/delta1 - 1)) +\n    \n    (K2 - K1) * \n    (r2^2 * (-1 / delta2 - 1) * \n       delta2 * (-exp(-2 * r2 * (x - tau2))) * \n       (delta2 * exp(-r2 * (x - tau2)) + 1)^(-1/delta2 - 2) - \n       r2^2 * \n       exp(-r2 * (x - tau2)) * \n       (delta2 * exp(-r2 * (x - tau2)) + 1)^(-1/delta2 - 1))\n}\n\n\n\n7.3.4 Help functions\nOnce again, we will rely on some help functions. These were defined previously:\n\nget_dates(): returns the dates relative to the sample of a specific country\nget_values_country(): returns the sample data for a single country\nloss_function(): the loss function that is minimized to find the model estimates\nget_criteria(): returns goodness of fit criteria for a model estimated with nls.lm.\n\n\n\n7.3.5 Example for the UK\nLet us consider an example, as was done for the first wave: that of the UK. The next section will show how the code can be used to estimate all the three double sigmoid models to each country.\nFirst, we need to decide the time horizon for the fitted values that will be returned. If the horizon is greater than the end of observed values, we will return out-of-sample predictions.\n\nend_date_sample <- lubridate::ymd(\"2020-09-28\")\nend_date_data <- max(confirmed_df$date)\nout_of_sample_horizon <- seq(end_date_sample, end_date_data, by = \"day\") |>\n  length()\n\nhorizon_pred <- 310\n\nSo, we want to focus on the UK:\n\ncountry_name <- \"United Kingdom\"\npop_country <- \n  population |> filter(country == !!country_name) |> \n  magrittr::extract2(\"pop\")\npop_country\n\n[1] 6.7e+07\n\n\nLet us show here how we can fit a Double Gompertz model, on the number of confirmed cases:\n\nmodel_name <- \"Double_Gompertz\"\ntype <- \"cases\"\n\nSetting the sample argument in our function get_values_country() allows us to extract the whole sample for a specific country. We can then store each element of the returned list in a single variable:\n\ndata_country <- get_values_country(\n  country_name, \n  sample = \"all\", \n  type = type\n)\n\nThe training sample:\n\ndf_country <- data_country$df_country\ndf_country\n\n# A tibble: 275 × 8\n   country  country_code date       value stringency_index days_since_2020_01_22\n   <chr>    <chr>        <date>     <int>            <dbl>                 <dbl>\n 1 United … GBR          2020-01-31     2             8.33                     9\n 2 United … GBR          2020-02-01     2             8.33                    10\n 3 United … GBR          2020-02-02     2            11.1                     11\n 4 United … GBR          2020-02-03     8            11.1                     12\n 5 United … GBR          2020-02-04     8            11.1                     13\n 6 United … GBR          2020-02-05     9            11.1                     14\n 7 United … GBR          2020-02-06     9            11.1                     15\n 8 United … GBR          2020-02-07     9            11.1                     16\n 9 United … GBR          2020-02-08    13            11.1                     17\n10 United … GBR          2020-02-09    14            11.1                     18\n# ℹ 265 more rows\n# ℹ 2 more variables: t <dbl>, y <int>\n\n\nAnd the dates of interest:\n\ndates_country <- data_country$dates_country\ndates_country\n\n# A tibble: 1 × 7\n  country        start_first_wave start_high_stringency start_reduce_restrict\n  <chr>          <date>           <date>                <date>               \n1 United Kingdom 2020-01-31       2020-03-23            2020-05-11           \n# ℹ 3 more variables: start_date_sample_second_wave <date>,\n#   length_high_stringency <dbl>, max_severity <dbl>\n\n\nWe previoulsy defined the Double Gompertz function and its first and second order derivatives. Let us use these:\n\nmodel_function <- double_gompertz_f\nmodel_function_2 <- double_logistic_f_2\nmodel_function_d <- double_gompertz_f_first_d\nmodel_function_dd <- double_gompertz_f_second_d\n\nThe training sample is the whole sample here:\n\ndf_country_training <- df_country\n\nLet us set a bounding value for \\(\\tau_2\\):\n\nnf_tau <- 280\n\nSome starting values for the optimization algorithm:\n\nstart <- list(\n  K1     = last(df_country$y)/2,\n  tau1   = 70,\n  r1     = .12,\n  K2     = last(df_country$y),\n  tau2   = 200,\n  r2     = 0.05\n)\n\nHere are the constraints that we set on the parameters of the model:\n\nlower_c <- c(K1 = 0, tau1 = 50, r1 = 0,\n             K2 = 0, tau2 = 0,  r2 = 0)\n\nupper_c <- c(K1 = pop_country, tau1 = Inf,    r1 = Inf,\n             K2 = pop_country, tau2 = nf_tau, r2 = Inf)\n\nWe can then use the nls.lm() function to minimize the loss function (loss_function()) for the Double Gompertz model whose function is stored in model_function():\n\nout <- nls.lm(\n  par = start,\n  fn = loss_function,\n  y = df_country$y,\n  t = df_country$t,\n  fun = model_function,\n  lower = lower_c,\n  upper = upper_c,\n  nls.lm.control(maxiter = 250),\n  jac=NULL\n)\n\nThe estimates can be stored in a table:\n\nparams <- tibble(\n  model_type = model_name,\n  country = country_name,\n  coef_estimate_name = names(coef(out)),\n  coef_estimate = coef(out)\n)\nparams\n\n# A tibble: 6 × 4\n  model_type      country        coef_estimate_name coef_estimate\n  <chr>           <chr>          <chr>                      <dbl>\n1 Double_Gompertz United Kingdom K1                   315516.    \n2 Double_Gompertz United Kingdom tau1                     77.6   \n3 Double_Gompertz United Kingdom r1                        0.0371\n4 Double_Gompertz United Kingdom K2                  2510392.    \n5 Double_Gompertz United Kingdom tau2                    280     \n6 Double_Gompertz United Kingdom r2                        0.0266\n\n\nThe goodness of fit can be obtained using the get_criteria() function that was previously defined:\n\ncrit <- get_criteria(\n  f = model_function,\n  data = df_country_training,\n  theta = params$coef_estimate\n)\n\ncriteria <- tibble(\n  model_type = model_name,\n  country = country_name,\n  bind_rows(crit)\n)\ncriteria\n\n# A tibble: 1 × 5\n  model_type      country          AIC   BIC  RMSE\n  <chr>           <chr>          <dbl> <dbl> <dbl>\n1 Double_Gompertz United Kingdom 5817. 5842. 9239.\n\n\nNow, let us get the fitted values, up to the desired horizon as set in horizon_pred:\n\n# Observed values\nobs <- df_country_training$value\ntype_obs <- rep(\"obs\", length(obs))\nif (length(obs) < horizon_pred) {\n  obs <- c(obs, rep(NA, horizon_pred-length(obs)))\n  type_obs <- c(\n    type_obs, \n    rep(\"out_of_sample\", horizon_pred-length(type_obs))\n  )\n}\nhead(obs)\n\n[1] 2 2 2 8 8 9\n\ntail(obs)\n\n[1] NA NA NA NA NA NA\n\nlength(obs)\n\n[1] 310\n\nhead(type_obs)\n\n[1] \"obs\" \"obs\" \"obs\" \"obs\" \"obs\" \"obs\"\n\ntail(type_obs)\n\n[1] \"out_of_sample\" \"out_of_sample\" \"out_of_sample\" \"out_of_sample\"\n[5] \"out_of_sample\" \"out_of_sample\"\n\nlength(type_obs)\n\n[1] 310\n\n\nThe corresponding dates:\n\ndates <- df_country_training$date\nif (length(dates) < horizon_pred) {\n  dates <- dates[1] + lubridate::ddays(seq_len(horizon_pred) - 1)\n}\nhead(dates)\n\n[1] \"2020-01-31\" \"2020-02-01\" \"2020-02-02\" \"2020-02-03\" \"2020-02-04\"\n[6] \"2020-02-05\"\n\ntail(dates)\n\n[1] \"2020-11-30\" \"2020-12-01\" \"2020-12-02\" \"2020-12-03\" \"2020-12-04\"\n[6] \"2020-12-05\"\n\nlength(dates)\n\n[1] 310\n\n\nThe fitted values can be stored in a table:\n\nfitted_val <- tibble(\n  country  = !!country_name,\n  index    = seq_len(horizon_pred) - 1,\n  value    = obs,\n  type_obs = type_obs,\n  date     = dates\n) |> \n  mutate(\n    model_type   = model_name,\n    fitted_value = model_function(theta = params$coef_estimate, x = index)\n  )\n\nfitted_val\n\n# A tibble: 310 × 7\n   country        index value type_obs date       model_type      fitted_value\n   <chr>          <dbl> <int> <chr>    <date>     <chr>                  <dbl>\n 1 United Kingdom     0     2 obs      2020-01-31 Double_Gompertz      0.00630\n 2 United Kingdom     1     2 obs      2020-02-01 Double_Gompertz      0.0120 \n 3 United Kingdom     2     2 obs      2020-02-02 Double_Gompertz      0.0224 \n 4 United Kingdom     3     8 obs      2020-02-03 Double_Gompertz      0.0407 \n 5 United Kingdom     4     8 obs      2020-02-04 Double_Gompertz      0.0725 \n 6 United Kingdom     5     9 obs      2020-02-05 Double_Gompertz      0.126  \n 7 United Kingdom     6     9 obs      2020-02-06 Double_Gompertz      0.216  \n 8 United Kingdom     7     9 obs      2020-02-07 Double_Gompertz      0.362  \n 9 United Kingdom     8    13 obs      2020-02-08 Double_Gompertz      0.596  \n10 United Kingdom     9    14 obs      2020-02-09 Double_Gompertz      0.963  \n# ℹ 300 more rows\n\n\nWe can also add the two components of the sigmoid, separately:\n\nfitted_val <- \n  fitted_val |> \n  mutate(\n    f_1 = model_function_2(x = index, theta = params$coef_estimate)[[\"f_1\"]],\n    f_2 = model_function_2(x = index, theta = params$coef_estimate)[[\"f_2\"]]\n  )\n\nThe key moments can be computed, using the second derivative of the model function.\nFor the first wave, the acceleration point corresponds to the moment where the second derivative reaches its maximum value. We therefore look at the time between the outbreak and the midpoint of the second wave where the second derivative reaches its maximum.\nLet us first get the midpoints of the sigmoids:\n\n# Estimated midpoints of the sigmoids\ntau_1 <- out$par$tau1\ntau_2 <- out$par$tau2\nprint(tau_1)\n\n[1] 77.56061\n\nprint(tau_2)\n\n[1] 280\n\n\nThen we can compute the acceleration point:\n\nacceleration_wave_1 <- model_function_dd(\n  theta = out$par,\n  x = seq(1, tau_1)\n) |> \n  which.max()\nacceleration_wave_1\n\n[1] 52\n\n\nThe deceleration point corresponds to the moment at which we observe the minimum acceleration:\n\ndeceleration_wave_1 <- \n  tau_1 + model_function_dd(\n    theta = out$par,\n    x = seq(tau_1, tau_2)\n  ) |> \n  which.min()\ndeceleration_wave_1\n\n[1] 104.5606\n\n\nThen we can compute the peak of the first wave, where the acceleration is null:\n\nabs_second_d_vals <- abs(\n  model_function_dd(\n    theta = out$par,\n    seq(acceleration_wave_1, deceleration_wave_1)\n  )\n)\nabs_second_d_vals <- abs_second_d_vals / min(abs_second_d_vals)\npeak_1 <- mean(which(abs_second_d_vals == 1)) + acceleration_wave_1\npeak_1\n\n[1] 79\n\n\nFor the second wave, the acceleration is the moment where we observe the maximum acceleration. We look at the second derivative of the model after the deleceration of the first wave:\n\nacceleration_wave_2 <- \n  deceleration_wave_1 +\n  model_function_dd(\n    theta = out$par, \n    seq(deceleration_wave_1, last(df_country_training$t))\n  ) |> \n  which.max()\n\nThe deceleration:\n\ndeceleration_wave_2 <- \n  acceleration_wave_2 +\n  model_function_dd(\n    theta = out$par, \n    seq(acceleration_wave_2, horizon_pred)\n  ) |> \n  which.min()\ndeceleration_wave_2\n\n[1] 310.5606\n\n\nAnd lastly, the peak of the second wave:\n\npeak_2 <- \n  model_function_dd(\n    theta = out$par,\n    seq(acceleration_wave_2, deceleration_wave_2)\n  ) |> \n  abs()\npeak_2 <- which(peak_2 < 10) |> \n  mean()\npeak_2 <- peak_2 + acceleration_wave_2\npeak_2\n\n[1] 281.0606\n\n\nWe can also add the relative speed at each peak. For the first wave:\n\nrelative_speed_1 <- model_function_d(theta = out$par, x = peak_1) /\n  model_function(theta = out$par, x = peak_1)\nrelative_speed_1\n\n[1] 0.03514423\n\n\nand for the second:\n\nrelative_speed_2 <- model_function_d(theta = out$par, x = peak_2) /\n  model_function(theta = out$par, x = peak_2)\nrelative_speed_2\n\n[1] 0.01873886\n\n\nThese moments can be stored in a table\n\nkey_moments <- tibble(\n  country  = country_name,\n  model_type = model_name,\n  t = c(\n    acceleration_wave_1, peak_1, deceleration_wave_1,\n    acceleration_wave_2, peak_2, deceleration_wave_2\n  ),\n  value = model_function(theta = out$par, t),\n  moment = c(\n    \"acceleration 1\", \"peak 1\", \"deceleration 1\",\n    \"acceleration 2\", \"peak 2\", \"deceleration 2\"\n  )\n) |> \n  bind_rows(\n    tibble(\n      country = country_name,\n      model_type = model_name,\n      t = c(peak_1, peak_2),\n      value = c(relative_speed_1, relative_speed_2),\n      moment = c(\"relative speed 1\", \"relative speed 2\")\n    )\n  ) |> \n  mutate(date = first(df_country_training$date) + lubridate::ddays(t) - 1)\nkey_moments\n\n# A tibble: 8 × 6\n  country        model_type          t        value moment   date               \n  <chr>          <chr>           <dbl>        <dbl> <chr>    <dttm>             \n1 United Kingdom Double_Gompertz   52    23923.     acceler… 2020-03-22 23:59:59\n2 United Kingdom Double_Gompertz   79   122263.     peak 1   2020-04-18 23:59:59\n3 United Kingdom Double_Gompertz  105.  218473.     deceler… 2020-05-14 13:27:15\n4 United Kingdom Double_Gompertz  245.  483560.     acceler… 2020-10-01 13:27:15\n5 United Kingdom Double_Gompertz  281. 1145566.     peak 2   2020-11-07 01:27:15\n6 United Kingdom Double_Gompertz  311. 1723791.     deceler… 2020-12-06 13:27:15\n7 United Kingdom Double_Gompertz   79        0.0351 relativ… 2020-04-18 23:59:59\n8 United Kingdom Double_Gompertz  281.       0.0187 relativ… 2020-11-07 01:27:15\n\n\nLet us prepare the data to make a plot.\n\nthe_dates <- map_df(\"United Kingdom\", get_dates, type = \"cases\")\ndf_plot <-\n  fitted_val |> \n  pivot_longer(cols = c(value, fitted_value)) |> \n  filter(! (type_obs == \"out_of_sample\" & name == \"value\") ) |>\n  mutate(linetype = str_c(type_obs, \"_\", name)) |>\n  mutate(\n    linetype = factor(\n      linetype,\n      levels = c(\n        \"obs_value\",\n        \"obs_fitted_value\", \n        \"out_of_sample_fitted_value\"\n      ),\n      labels = c(\n        \"Obs.\", \n        \"Double Gompertz\",\n        \"Double Gompertz\"\n      )\n    )\n  )\ndf_plot <-\n  df_plot |> \n  left_join(\n    colour_table,\n     by = c(\"country\")\n  )\n\ndate_end_obs <- \n  df_plot |> \n  filter(linetype == \"Obs.\") |> \n  arrange(desc(date)) |> \n  slice(1) |> \n  magrittr::extract2(\"date\")\n\ndate_end_sample <- max(df_plot$date)\n\nAnd the plot. The shaded area corresponds to out-of-sample predictions, starting with the end of the observed data (i.e., September 28).\n\nggplot(data = df_plot) +\n  geom_line(\n    mapping = aes(\n      x = date, y = value,\n      colour = country_code, linetype = linetype\n    )\n  ) +\n    scale_colour_manual(NULL, values = colour_countries) +\n    scale_y_continuous(labels = comma) +\n    labs(x = NULL, y = \"Cases\") +\n    scale_x_date(\n      labels = date_format(\"%b %d %Y\"),\n      breaks = lubridate::ymd(lubridate::pretty_dates(df_plot$date, n = 5))\n    ) +\n    # scale_linetype_manual(\n    #   NULL,\n    #   values = c(\n    #     \"Obs.\" = \"solid\",\n    #     \"Double Richards\" = \"dashed\",\n    #     \"Double Gompertz\" = \"dotted\"\n    #   )\n    # ) +\n    annotate(\n      geom = \"rect\", \n      xmin = date_end_obs, xmax = date_end_sample,\n      ymin = -Inf, ymax = Inf,\n      alpha = .2, fill = \"grey\"\n    ) +\n  theme_paper()\n\n\n\n\nFigure 7.2: Predictions of the number of cases for the UK, with a double Gompertz model."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Adam, David. 2020. “Special Report: The Simulations Driving the\nWorld’s Response to COVID-19.” Nature 580:\n316–18.\n\n\nBock, R. Darrell, Howard Wainer, Anne Petersen, David Thissen, James\nMurray, and Alex Roche. 1973. “A Parameterization for Individual\nHuman Growth Curves.” Human Biology 45 (1): 63–80. http://www.jstor.org/stable/41459847.\n\n\nCori, Anne, Neil M. Ferguson, Christophe Fraser, and Simon Cauchemez.\n2013. “A New Framework and Software to Estimate Time-Varying\nReproduction Numbers During Epidemics.” American Journal of\nEpidemiology 178 (9): 1505–12. https://doi.org/10.1093/aje/kwt133.\n\n\nGompertz, Benjamin. 1825. “On the Nature of the Function\nExpressive of the Law of Human Mortality, and on a New Mode of\nDetermining the Value of Life Contingencies.” Philosophical\nTransactions of the Royal Society of London 115 (December): 513–83.\nhttps://doi.org/10.1098/rstl.1825.0026.\n\n\nKermack, William Ogilvy, and A. G. McKendrick. 1927. “A\nContribution to the Mathematical Theory of Epidemics.”\nProceedings of the Royal Society A 115 (772): 700–721.\n\n\nLee, Se Yoon, Bowen Lei, and Bani Mallick. 2020. “Estimation of\nCOVID-19 Spread Curves Integrating Global Data and\nBorrowing Information.” PLOS ONE 15 (7): e0236860. https://doi.org/10.1371/journal.pone.0236860.\n\n\nLi, Qun, Xuhua Guan, Peng Wu, Xiaoye Wang, Lei Zhou, Yeqing Tong, Ruiqi\nRen, et al. 2020. “Early Transmission Dynamics in\nWuhan, China, of Novel Coronavirus-Infected\nPneumonia.” New England Journal of Medicine 382 (13):\n1199–1207. https://doi.org/10.1056/NEJMoa2001316.\n\n\nLipovetsky, Stan. 2010. “Double Logistic Curve in Regression\nModeling.” Journal of Applied Statistics 37 (11):\n1785–93. https://doi.org/10.1080/02664760903093633.\n\n\nMa, Junling. 2020. “Estimating Epidemic Exponential Growth Rate\nand Basic Reproduction Number.” Infectious Disease\nModelling 5 (January): 129–41. https://doi.org/10.1016/j.idm.2019.12.009.\n\n\nMoll, Benjamin. 2020. “Lockdowns in SIR\nModels.” LSE.\n\n\nOswald, Stephen A., Ian C. T. Nisbet, Andre Chiaradia, and Jennifer M.\nArnold. 2012. “FlexParamCurve: R Package\nfor Flexible Fitting of Nonlinear Parametric Curves.” Methods\nin Ecology and Evolution 3 (6): 1073–77. https://doi.org/10.1111/j.2041-210x.2012.00231.x.\n\n\nPark, M., A. R. Cook, J. T. Lim, Y. Sun, and B. L. Dickens. 2020.\n“A Systematic Review of\nCOVID-19 Epidemiology Based on\nCurrent Evidence.” Journal of\nClinical Medicine 9 (4). https://doi.org/10.3390/jcm9040967.\n\n\nRichards, F. J. 1959. “A Flexible Growth Function for Empirical\nUse.” Journal of Experimental Botany 10 (2): 290–301. https://doi.org/10.1093/jxb/10.2.290.\n\n\nThissen, David, R. Darrell Bock, Howard Wainer, and Alex F. Roche. 1976.\n“Individual Growth in Stature: A Comparison of Four\nGrowth Studies in the U.S.A.” Annals of Human\nBiology 3 (6): 529–42. https://doi.org/10.1080/03014467600001791.\n\n\nTjørve, Kathleen M. C., and Even Tjørve. 2017. “The Use of\nGompertz Models in Growth Analyses, and New\nGompertz-Model Approach: An Addition to the\nUnified-Richards Family.” PLOS\nONE 12 (6): e0178691. https://doi.org/10.1371/journal.pone.0178691.\n\n\nToda, Alexis Akira. 2020. “Susceptible-Infected-Recovered\n(SIR) Dynamics of COVID-19 and Economic\nImpact.” arXiv:2003.11221v2.\n\n\nTsoularis, A., and J. Wallace. 2002. “Analysis of Logistic Growth\nModels.” Mathematical Biosciences 179 (1): 21–55. https://doi.org/10.1016/S0025-5564(02)00096-2.\n\n\nVerhulst, P. F. 1845. “Recherches Mathématiques Sur\nLa Loi d’accroissement de La Population.” Nouveaux\nmémoires de l’Académie Royale Des Sciences Et\nBelles-Lettres de Bruxelles 18: 14–54. https://eudml.org/doc/182533.\n\n\nWang, Huwen, Zezhou Wang, Yinqiao Dong, Ruijie Chang, Chen Xu, Xiaoyue\nYu, Shuxian Zhang, et al. 2020. “Phase-Adjusted Estimation of the\nNumber of Coronavirus Disease 2019 Cases in Wuhan,\nChina.” Cell Discovery 6 (1): 1–8. https://doi.org/10.1038/s41421-020-0148-0.\n\n\nWang, Xiang-Sheng, Jianhong Wu, and Yong Yang. 2012. “Richards\nModel Revisited: Validation by and Application to Infection\nDynamics.” Journal of Theoretical Biology 313: 12–19. https://doi.org/10.1016/j.jtbi.2012.07.024."
  }
]